Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.2371811866760254 seconds
Warning: Permanently added '[192.168.0.137]:40720' (ECDSA) to the list of known hosts.
[2022-10-28 22:56:12,091] [INFO] [runner.py:415:main] Using IP address of 192.168.0.137 for node worker-0
[2022-10-28 22:56:12,092] [INFO] [runner.py:504:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ3b3JrZXItMCI6IFswLCAxLCAyLCAzLCA0LCA1LCA2LCA3XX0= --master_addr=192.168.0.137 --master_port=29500 /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py --override-lr-scheduler --adam-beta1 0.9 --adam-beta2 0.95 --tensor-model-parallel-size 1 --init-method-std 0.02 --lr-decay-tokens 260000000000 --lr-warmup-tokens 375000000 --micro-batch-size 4 --exit-duration-in-mins 30000000 --global-batch-size 256 --num-layers 12 --hidden-size 768 --num-attention-heads 12 --seq-length 2048 --max-position-embeddings 2048 --train-tokens 300000000000 --train-samples 439453125 --lr 3.0e-4 --min-lr 3.0e-5 --lr-decay-style cosine --split 98,2,0 --log-interval 10 --eval-interval 100 --eval-iters 10 --save-interval 10000 --weight-decay 0.1 --clip-grad 1.0 --hysteresis 2 --num-workers 0 --fp16 --increse-length-token-interval 1.75 --load /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4 --save /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4 --tensorboard-queue-size 1 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir tensorboard/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4_azwus2f200000C6_2022.10.28-22.56.05 --log-optimizer-states-to-tensorboard --vocab-file /blob//data/the_pile_public_merged_nopreprocessing/gpt2-vocab.json --merge-file /blob//data/the_pile_public_merged_nopreprocessing/gpt2-merges.txt --data-path /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document --data-impl mmap --deepspeed --deepspeed_config ds_config_gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4.json --zero-stage 0 --pipeline-model-parallel-size 1 --no-pipeline-parallel
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.23049497604370117 seconds
[2022-10-28 22:56:14,598] [INFO] [launch.py:129:main] 0 NCCL_IB_PCI_RELAXED_ORDERING=1
[2022-10-28 22:56:14,598] [INFO] [launch.py:129:main] 0 NCCL_VERSION=2.9.8
[2022-10-28 22:56:14,603] [INFO] [launch.py:129:main] 0 NCCL_SOCKET_IFNAME=eth0
[2022-10-28 22:56:14,603] [INFO] [launch.py:129:main] 0 NCCL_NET_GDR_LEVEL=5
[2022-10-28 22:56:14,603] [INFO] [launch.py:129:main] 0 NCCL_DEBUG=INFO
[2022-10-28 22:56:14,603] [INFO] [launch.py:129:main] 0 NCCL_TREE_THRESHOLD=0
[2022-10-28 22:56:14,603] [INFO] [launch.py:129:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2022-10-28 22:56:14,603] [INFO] [launch.py:129:main] 0 NCCL_IB_TIMEOUT=20
[2022-10-28 22:56:14,607] [INFO] [launch.py:129:main] 0 NCCL_TOPO_FILE=/opt/msft/topo.xml
[2022-10-28 22:56:14,607] [INFO] [launch.py:136:main] WORLD INFO DICT: {'worker-0': [0, 1, 2, 3, 4, 5, 6, 7]}
[2022-10-28 22:56:14,607] [INFO] [launch.py:142:main] nnodes=1, num_local_procs=8, node_rank=0
[2022-10-28 22:56:14,607] [INFO] [launch.py:155:main] global_rank_mapping=defaultdict(<class 'list'>, {'worker-0': [0, 1, 2, 3, 4, 5, 6, 7]})
[2022-10-28 22:56:14,607] [INFO] [launch.py:156:main] dist_world_size=8
[2022-10-28 22:56:14,607] [INFO] [launch.py:158:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.41286492347717285 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+7287051a, 7287051a, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
**** Git info for Megatron: git_hash=7188d07 git_branch=xiaoxia/token-drop-dynamic-train ****
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.5189995765686035 seconds
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
ninja: no work to do.
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+7287051a, 7287051a, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
Loading extension module token_dropping...
Time to load token_dropping op: 0.47481799125671387 seconds
Loading extension module token_dropping...
Time to load token_dropping op: 0.4416956901550293 seconds
Loading extension module token_dropping...
Loading extension module token_dropping...
Time to load token_dropping op: 0.43094491958618164 seconds
Time to load token_dropping op: 0.4207437038421631 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
Loading extension module token_dropping...
Time to load token_dropping op: 0.6685404777526855 seconds
**** Git info for Megatron: git_hash=7188d07 git_branch=xiaoxia/token-drop-dynamic-train ****
Loading extension module token_dropping...
----------------------------------------------------------------------------------------------------
DeepSpeed C++/CUDA extension op report

DeepSpeed C++/CUDA extension op report--------------------------------------------------

--------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.

NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.--------------------------------------------------

--------------------------------------------------JIT compiled ops requires ninja

JIT compiled ops requires ninja
ninja ninja..................  ..................[92m[OKAY][0m 
[92m[OKAY][0m--------------------------------------------------

--------------------------------------------------op name
 op name................  ................installed  installed..  ..compatible 
compatible--------------------------------------------------

--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adamcpu_adagrad  ...........................  [93m[NO][0m[93m[NO][0m  ..............  [92m[OKAY][0m[92m[OKAY][0m

fused_adam ............. [93m[NO][0m cpu_adagrad.......  ............[92m[OKAY][0m 
[93m[NO][0m .......fused_lamb  [92m[OKAY][0m.............
 [93m[NO][0m fused_adam.......  .............[92m[OKAY][0m 
[93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
Time to load token_dropping op: 0.6568713188171387 seconds--------------------------------------------------

DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
sparse_attnsparse_attn  ........................  [93m[NO][0m[93m[NO][0m  ....... .......[92m[OKAY][0m 
[92m[OKAY][0m
transformer ............transformer  [93m[NO][0m............  .......[93m[NO][0m  [92m[OKAY][0m.......
 [92m[OKAY][0m
stochastic_transformer . stochastic_transformer[93m[NO][0m  ........  [93m[NO][0m[92m[OKAY][0m 
....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ...............async_io [93m[NO][0m .......  [92m[OKAY][0m............... 
[93m[NO][0m ....... [92m[OKAY][0mutils
 .................. [93m[NO][0m .......utils  [92m[OKAY][0m..................
 [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m .......quantizer  [92m[OKAY][0m..............
 [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0mtransformer_inference 
.. [93m[NO][0m ....... [92m[OKAY][0mtoken_dropping
 ......... [93m[NO][0m ....... [92m[OKAY][0m
token_dropping --------------------------------------------------.........
 [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
DeepSpeed general environment info:torch install path ...............
 torch install path['/opt/conda/lib/python3.8/site-packages/torch'] 
...............torch version  .................... 1.11.0+cu113['/opt/conda/lib/python3.8/site-packages/torch']

torch cuda versiontorch version  ...................................  11.31.11.0+cu113

torch hip versiontorch cuda version ...............  11.3................
 torch hip versionNone 
................nvcc version  None.....................
 nvcc version11.3 
.....................deepspeed install path  11.3...........
 deepspeed install path ...........['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed'] 
deepspeed info ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']...................
 deepspeed info0.7.3+7287051a, 7287051a, xiaoxia/token-drop-dynamic-train 
...................deepspeed wheel compiled w.  0.7.3+7287051a, 7287051a, xiaoxia/token-drop-dynamic-train......
 deepspeed wheel compiled w.torch 1.11, cuda 11.3 
...... torch 1.11, cuda 11.3
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+7287051a, 7287051a, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+7287051a, 7287051a, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+7287051a, 7287051a, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+7287051a, 7287051a, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
**** Git info for Megatron: git_hash=7188d07 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=7188d07 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=7188d07 git_branch=xiaoxia/token-drop-dynamic-train ****
using world size: 8, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_token_layers ........................... 0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning ............................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 8
  data_path ....................................... ['/blob/data/the_pile_public_merged_nopreprocessing/pile_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ ds_config_gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 2048
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 3072
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 768
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  increse_length_token_interval ................... 1.75
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  initial_sequence_length ......................... 128
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 64
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4
  load_teacher .................................... None
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. True
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0003
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 260000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 375000000
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /blob//data/the_pile_public_merged_nopreprocessing/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 12
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 12
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... True
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4
  save_interval ................................... 10000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 2048
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. tensorboard/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4_azwus2f200000C6_2022.10.28-22.56.05
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_iters ..................................... None
  train_samples ................................... 439453125
  train_tokens .................................... 300000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /blob//data/the_pile_public_merged_nopreprocessing/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 8
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
**** Git info for Megatron: git_hash=7188d07 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=7188d07 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=7188d07 git_branch=xiaoxia/token-drop-dynamic-train ****
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
[2022-10-28 22:56:18,897] [INFO] [comm.py:633:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
> setting tensorboard ...
2022-10-28 22:56:19.189502: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> compiling dataset index builder ...
make: Entering directory '/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/data'
>>> done with dataset index builder. Compilation time: 0.139 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
azwus2f200000C6:58799:58799 [0] NCCL INFO Bootstrap : Using eth0:192.168.0.137<0>
azwus2f200000C6:58799:58799 [0] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000C6:58799:58799 [0] NCCL INFO P2P plugin IBext
azwus2f200000C6:58799:58799 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000C6:58799:58799 [0] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.137<0>
azwus2f200000C6:58799:58799 [0] NCCL INFO Using network IBext
NCCL version 2.10.3+cuda11.3
azwus2f200000C6:58809:58809 [6] NCCL INFO Bootstrap : Using eth0:192.168.0.137<0>
azwus2f200000C6:58800:58800 [1] NCCL INFO Bootstrap : Using eth0:192.168.0.137<0>
azwus2f200000C6:58801:58801 [2] NCCL INFO Bootstrap : Using eth0:192.168.0.137<0>
azwus2f200000C6:58805:58805 [4] NCCL INFO Bootstrap : Using eth0:192.168.0.137<0>
azwus2f200000C6:58803:58803 [3] NCCL INFO Bootstrap : Using eth0:192.168.0.137<0>
azwus2f200000C6:58811:58811 [7] NCCL INFO Bootstrap : Using eth0:192.168.0.137<0>
azwus2f200000C6:58807:58807 [5] NCCL INFO Bootstrap : Using eth0:192.168.0.137<0>
azwus2f200000C6:58800:58800 [1] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000C6:58809:58809 [6] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000C6:58807:58807 [5] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000C6:58801:58801 [2] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000C6:58811:58811 [7] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000C6:58800:58800 [1] NCCL INFO P2P plugin IBext
azwus2f200000C6:58811:58811 [7] NCCL INFO P2P plugin IBext
azwus2f200000C6:58803:58803 [3] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000C6:58800:58800 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000C6:58811:58811 [7] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000C6:58809:58809 [6] NCCL INFO P2P plugin IBext
azwus2f200000C6:58803:58803 [3] NCCL INFO P2P plugin IBext
azwus2f200000C6:58807:58807 [5] NCCL INFO P2P plugin IBext
azwus2f200000C6:58803:58803 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000C6:58801:58801 [2] NCCL INFO P2P plugin IBext
azwus2f200000C6:58809:58809 [6] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000C6:58805:58805 [4] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000C6:58807:58807 [5] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000C6:58801:58801 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000C6:58805:58805 [4] NCCL INFO P2P plugin IBext
azwus2f200000C6:58805:58805 [4] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000C6:58803:58803 [3] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.137<0>
azwus2f200000C6:58803:58803 [3] NCCL INFO Using network IBext
azwus2f200000C6:58805:58805 [4] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.137<0>
azwus2f200000C6:58809:58809 [6] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.137<0>
azwus2f200000C6:58807:58807 [5] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.137<0>
azwus2f200000C6:58811:58811 [7] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.137<0>
azwus2f200000C6:58800:58800 [1] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.137<0>
azwus2f200000C6:58801:58801 [2] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.137<0>
azwus2f200000C6:58805:58805 [4] NCCL INFO Using network IBext
azwus2f200000C6:58809:58809 [6] NCCL INFO Using network IBext
azwus2f200000C6:58807:58807 [5] NCCL INFO Using network IBext
azwus2f200000C6:58811:58811 [7] NCCL INFO Using network IBext
azwus2f200000C6:58800:58800 [1] NCCL INFO Using network IBext
azwus2f200000C6:58801:58801 [2] NCCL INFO Using network IBext
azwus2f200000C6:58803:60015 [3] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000C6:58811:60021 [7] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000C6:58807:60017 [5] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000C6:58801:60020 [2] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000C6:58809:60019 [6] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000C6:58805:60016 [4] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000C6:58799:59958 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000C6:58800:60018 [1] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000C6:58799:59958 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000C6:58807:60017 [5] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000C6:58801:60020 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000C6:58805:60016 [4] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000C6:58800:60018 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000C6:58803:60015 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000C6:58811:60021 [7] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000C6:58809:60019 [6] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000C6:58811:60021 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
azwus2f200000C6:58809:60019 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
azwus2f200000C6:58811:60021 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000C6:58809:60019 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58800:60018 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58801:60020 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58803:60015 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58801:60020 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58805:60016 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58807:60017 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58803:60015 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000C6:58807:60017 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58805:60016 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58800:60018 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:59958 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
azwus2f200000C6:58799:59958 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 00 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 00 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 01 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 00 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 00 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 01 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 02 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 01 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 00 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 01 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 00 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 02 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 01 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 03 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 02 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 02 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 01 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 03 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 02 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 04 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 03 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 03 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 02 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 00 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 04 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 03 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 05 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 04 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 04 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 03 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 01 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 00 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 05 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 04 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 06 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 05 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 05 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 04 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 02 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 01 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 06 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 05 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 07 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 06 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 06 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 03 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 05 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 02 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 07 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 06 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 08 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 07 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 07 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 04 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 06 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 03 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 08 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 07 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 09 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 08 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 08 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 05 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 07 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 04 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 09 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 08 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 10 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 09 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 09 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 06 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 08 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 05 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 09 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 10 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 11 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 10 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 07 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 10 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 06 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 09 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 10 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 11 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 12 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 11 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 08 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 11 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 07 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 10 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 11 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 12 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 13 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 12 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 09 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 12 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 08 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 11 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 12 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 13 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 14 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 13 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 10 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 13 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 09 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 12 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 13 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 14 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 15 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 14 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 11 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 14 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 13 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 10 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 14 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 15 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 16 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 12 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 15 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 15 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 14 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 11 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 15 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 16 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 17 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 13 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 16 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 16 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 15 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 12 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 16 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 17 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 18 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 14 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 17 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 17 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 16 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 13 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 17 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 18 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 19 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 15 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 18 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 18 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 17 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 14 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 18 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 19 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 20 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 16 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 19 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 19 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 18 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 15 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 19 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 20 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 17 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 21 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 20 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 20 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 19 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 16 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 20 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 21 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 18 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 22 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 21 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 21 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 17 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 20 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 21 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 22 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 19 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 23 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 22 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 22 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 18 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 21 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 22 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 23 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 20 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 23 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 23 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 19 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 22 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 23 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 21 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 20 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 23 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 22 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 21 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58799:59958 [0] NCCL INFO Channel 23 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 22 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 23 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Connected all rings
azwus2f200000C6:58809:60019 [6] NCCL INFO Connected all rings
azwus2f200000C6:58811:60021 [7] NCCL INFO Connected all rings
azwus2f200000C6:58799:59958 [0] NCCL INFO Connected all rings
azwus2f200000C6:58800:60018 [1] NCCL INFO Connected all rings
azwus2f200000C6:58803:60015 [3] NCCL INFO Connected all rings
azwus2f200000C6:58807:60017 [5] NCCL INFO Connected all rings
azwus2f200000C6:58805:60016 [4] NCCL INFO Connected all rings
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 00 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 01 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 02 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 03 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 04 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 05 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 06 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 07 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 08 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 09 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 10 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 11 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 12 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 13 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 14 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 15 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 00 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 16 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 00 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 01 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 17 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 01 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 02 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 18 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 02 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 03 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 19 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 00 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 00 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 00 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 00 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 03 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 04 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 20 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 01 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 01 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 01 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 01 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 04 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 05 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 21 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 02 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 02 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 02 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 02 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 05 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 06 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 22 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 03 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 03 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 03 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 03 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 06 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 07 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Channel 23 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 04 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 04 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 04 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 04 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 07 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 08 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 05 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 05 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 05 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 05 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 08 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 06 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 09 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 06 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 06 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 06 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 07 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 09 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 10 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 07 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 07 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 07 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 08 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 10 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 11 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 08 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 08 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 08 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 09 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 11 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 12 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 09 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 09 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 09 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 10 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 12 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 13 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 10 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 10 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 10 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 11 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 13 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 14 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 11 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 11 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 11 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 12 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 15 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 14 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 12 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 12 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 12 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 13 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 16 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 15 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 13 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 13 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 13 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 14 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 17 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 16 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 14 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 14 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 14 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 15 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 18 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 17 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 15 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 15 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 15 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 16 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 18 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 19 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 16 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 16 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 16 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 17 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 19 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 20 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 17 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 17 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 17 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 18 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 20 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 21 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 18 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 18 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 18 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 19 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 21 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 22 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 19 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 19 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 19 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 20 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 22 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60020 [2] NCCL INFO Channel 23 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 20 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 20 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 20 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 21 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60019 [6] NCCL INFO Channel 23 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 21 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 21 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 21 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 22 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 22 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 22 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 22 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60018 [1] NCCL INFO Channel 23 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60015 [3] NCCL INFO Channel 23 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60016 [4] NCCL INFO Channel 23 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60017 [5] NCCL INFO Channel 23 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58811:60021 [7] NCCL INFO Connected all trees
azwus2f200000C6:58811:60021 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58811:60021 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58799:59958 [0] NCCL INFO Connected all trees
azwus2f200000C6:58799:59958 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58799:59958 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58809:60019 [6] NCCL INFO Connected all trees
azwus2f200000C6:58809:60019 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58800:60018 [1] NCCL INFO Connected all trees
azwus2f200000C6:58803:60015 [3] NCCL INFO Connected all trees
azwus2f200000C6:58805:60016 [4] NCCL INFO Connected all trees
azwus2f200000C6:58801:60020 [2] NCCL INFO Connected all trees
azwus2f200000C6:58807:60017 [5] NCCL INFO Connected all trees
azwus2f200000C6:58800:60018 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58803:60015 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58805:60016 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58801:60020 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58807:60017 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58809:60019 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58807:60017 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58801:60020 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58803:60015 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58805:60016 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58800:60018 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58799:59958 [0] NCCL INFO comm 0x7fa324002fb0 rank 0 nranks 8 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000C6:58801:60020 [2] NCCL INFO comm 0x7ff7f4002fb0 rank 2 nranks 8 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000C6:58800:60018 [1] NCCL INFO comm 0x7f6a38002fb0 rank 1 nranks 8 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000C6:58803:60015 [3] NCCL INFO comm 0x7fc098002fb0 rank 3 nranks 8 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000C6:58811:60021 [7] NCCL INFO comm 0x7f0a84002fb0 rank 7 nranks 8 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000C6:58805:60016 [4] NCCL INFO comm 0x7f2b50002fb0 rank 4 nranks 8 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000C6:58807:60017 [5] NCCL INFO comm 0x7f2da0002fb0 rank 5 nranks 8 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000C6:58809:60019 [6] NCCL INFO comm 0x7f7b9c002fb0 rank 6 nranks 8 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000C6:58799:58799 [0] NCCL INFO Launch mode Parallel
>>> done with compiling and loading fused kernels. Compilation time: 14.062 seconds
time to initialize megatron (seconds): -45.152
[after megatron is initialized] datetime: 2022-10-28 22:56:34 
{'random_ltd': {'enabled': False}}
{'random_ltd': {'enabled': False}}{'random_ltd': {'enabled': False}}

{'random_ltd': {'enabled': False}}{'random_ltd': {'enabled': False}}
building GPT model ...

{'random_ltd': {'enabled': False}}{'random_ltd': {'enabled': False}}

[2022-10-28 22:56:34,922] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2022-10-28 22:56:34,923] [INFO] [utils.py:828:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2022-10-28 22:56:34,927] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 57.28 GB, percent = 3.2%
{'random_ltd': {'enabled': False}}
{'random_ltd': {'enabled': False}}
{'random_ltd': {'enabled': False}}
{'random_ltd': {'enabled': False}}
{'random_ltd': {'enabled': False}}
{'random_ltd': {'enabled': False}}
{'random_ltd': {'enabled': False}}
{'random_ltd': {'enabled': False}}
[2022-10-28 22:56:45,014] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2022-10-28 22:56:45,015] [INFO] [utils.py:828:see_memory_usage] MA 0.24 GB         Max_MA 0.24 GB         CA 0.25 GB         Max_CA 0 GB 
[2022-10-28 22:56:45,018] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 57.28 GB, percent = 3.2%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 125262336
setting training iterations to 1716613
> learning rate decay style: cosine
DeepSpeed is enabled.
[2022-10-28 22:56:45,025] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.3+7287051a, git-hash=7287051a, git-branch=xiaoxia/token-drop-dynamic-train
{'random_ltd': {'enabled': False}}
azwus2f200000C6:58803:60881 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
azwus2f200000C6:58800:60877 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
azwus2f200000C6:58805:60876 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
azwus2f200000C6:58803:60881 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000C6:58800:60877 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000C6:58807:60878 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
azwus2f200000C6:58805:60876 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000C6:58809:60882 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
azwus2f200000C6:58811:60879 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
azwus2f200000C6:58809:60882 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000C6:58811:60879 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000C6:58801:60880 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58801:60880 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58807:60878 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
azwus2f200000C6:58799:60875 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
azwus2f200000C6:58799:60875 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 00 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 00 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 00 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 00 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 00 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 00 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 00 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 00 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 01 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 01 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 01 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 01 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 01 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 01 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 01 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 01 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 02 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 02 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 02 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 02 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 02 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 02 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 02 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 02 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 03 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 03 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 03 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 03 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 03 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 03 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 03 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 03 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 04 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 04 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 04 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 04 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 04 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 04 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 04 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 05 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 04 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 05 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 05 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 05 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 05 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 05 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 05 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 06 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 05 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 06 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 06 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 06 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 06 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 06 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 06 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 07 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 06 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 07 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 07 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 07 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 07 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 07 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 07 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 08 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 07 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 08 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 08 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 08 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 08 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 09 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 08 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 08 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 09 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 08 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 09 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 09 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 09 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 10 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 09 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 10 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 09 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 10 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 09 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 10 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 10 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 11 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 10 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 11 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 10 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 11 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 10 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 11 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 12 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 11 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 11 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 12 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 11 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 12 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 11 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 12 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 13 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 12 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 13 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 12 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 12 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 13 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 12 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 13 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 13 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 14 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 14 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 13 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 13 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 13 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 14 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 14 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 15 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 14 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 15 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 14 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 14 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 15 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 14 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 16 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 15 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 15 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 16 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 15 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 15 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 16 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 15 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 17 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 16 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 16 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 17 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 16 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 16 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 17 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 16 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 18 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 17 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 17 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 18 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 17 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 17 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 18 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 17 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 18 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 18 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 19 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 19 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 18 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 18 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 19 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 18 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 19 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 20 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 19 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 20 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 19 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 19 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 20 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 20 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 19 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 21 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 20 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 21 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 20 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 20 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 21 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 21 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 20 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 22 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 21 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 22 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 21 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 21 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 22 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 22 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 21 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 23 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 22 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 23 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 22 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 22 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 23 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 23 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Channel 23 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 23 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 22 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 23 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 23 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Connected all rings
azwus2f200000C6:58800:60877 [1] NCCL INFO Connected all rings
azwus2f200000C6:58801:60880 [2] NCCL INFO Connected all rings
azwus2f200000C6:58803:60881 [3] NCCL INFO Connected all rings
azwus2f200000C6:58799:60875 [0] NCCL INFO Connected all rings
azwus2f200000C6:58807:60878 [5] NCCL INFO Connected all rings
azwus2f200000C6:58809:60882 [6] NCCL INFO Connected all rings
azwus2f200000C6:58811:60879 [7] NCCL INFO Connected all rings
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 00 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 01 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 02 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 03 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 04 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 05 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 06 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 07 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 08 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 09 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 10 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 11 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 12 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 13 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 14 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 15 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 16 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 17 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 18 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 19 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 20 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 21 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 22 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 00 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 00 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 00 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58811:60879 [7] NCCL INFO Channel 23 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 00 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 01 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 00 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 00 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 01 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 01 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 01 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 02 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 01 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 01 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 02 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 02 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 02 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 02 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 02 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 03 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 03 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 03 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 03 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 04 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 03 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 03 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 04 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 04 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 04 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 05 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 04 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 04 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 05 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 05 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 05 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 06 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 05 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 05 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 06 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 06 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 06 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 07 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 06 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 06 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 07 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 07 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 07 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 07 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 08 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 07 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 08 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 08 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 08 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 09 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 08 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 08 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 09 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 09 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 09 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 10 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 09 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 09 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 10 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 10 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 10 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 11 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 10 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 10 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 11 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 11 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 11 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 12 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 11 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 11 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 12 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 12 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 12 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 12 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 13 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 12 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 13 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 13 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 13 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 14 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 13 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 13 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 14 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 14 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 14 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 15 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 14 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 14 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 15 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 15 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 15 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 16 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 15 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 15 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 16 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 16 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 16 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 17 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 16 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 16 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 17 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 17 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 17 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 18 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 17 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 17 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 18 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 18 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 18 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 19 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 18 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 18 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 19 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 19 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 19 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 20 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 19 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 19 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 20 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 20 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 20 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 21 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 20 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 20 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 21 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 21 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 21 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 22 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 21 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 21 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 22 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 22 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 22 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58805:60876 [4] NCCL INFO Channel 23 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 22 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 22 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58800:60877 [1] NCCL INFO Channel 23 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000C6:58801:60880 [2] NCCL INFO Channel 23 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000C6:58803:60881 [3] NCCL INFO Channel 23 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000C6:58809:60882 [6] NCCL INFO Channel 23 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000C6:58807:60878 [5] NCCL INFO Channel 23 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000C6:58799:60875 [0] NCCL INFO Connected all trees
azwus2f200000C6:58799:60875 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58811:60879 [7] NCCL INFO Connected all trees
azwus2f200000C6:58811:60879 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58799:60875 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58811:60879 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58801:60880 [2] NCCL INFO Connected all trees
azwus2f200000C6:58801:60880 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58803:60881 [3] NCCL INFO Connected all trees
azwus2f200000C6:58800:60877 [1] NCCL INFO Connected all trees
azwus2f200000C6:58805:60876 [4] NCCL INFO Connected all trees
azwus2f200000C6:58807:60878 [5] NCCL INFO Connected all trees
azwus2f200000C6:58809:60882 [6] NCCL INFO Connected all trees
azwus2f200000C6:58803:60881 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58800:60877 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58805:60876 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58807:60878 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58809:60882 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000C6:58801:60880 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58805:60876 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58800:60877 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58809:60882 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58807:60878 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58803:60881 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58800:60877 [1] NCCL INFO comm 0x7f6550002fb0 rank 1 nranks 8 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000C6:58809:60882 [6] NCCL INFO comm 0x7f76a0002fb0 rank 6 nranks 8 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000C6:58799:60875 [0] NCCL INFO comm 0x7f9dd0002fb0 rank 0 nranks 8 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000C6:58801:60880 [2] NCCL INFO comm 0x7ff2dc002fb0 rank 2 nranks 8 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000C6:58807:60878 [5] NCCL INFO comm 0x7f2888002fb0 rank 5 nranks 8 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000C6:58811:60879 [7] NCCL INFO comm 0x7f057c002fb0 rank 7 nranks 8 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000C6:58803:60881 [3] NCCL INFO comm 0x7fbbac002fb0 rank 3 nranks 8 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000C6:58805:60876 [4] NCCL INFO comm 0x7f25dc002fb0 rank 4 nranks 8 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000C6:58799:58799 [0] NCCL INFO Launch mode Parallel
[2022-10-28 22:57:01,423] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-10-28 22:57:01,424] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-10-28 22:57:01,427] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-10-28 22:57:01,430] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = {basic_optimizer.__class__.__name__}
[2022-10-28 22:57:01,430] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...[2022-10-28 22:57:01,561] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...

Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
[2022-10-28 22:57:01,561] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2022-10-28 22:57:01,561] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7fa5a32c2d60>
[2022-10-28 22:57:01,564] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 22:57:01,565] [INFO] [config.py:978:print] DeepSpeedEngine configuration:
[2022-10-28 22:57:01,568] [INFO] [config.py:982:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-10-28 22:57:01,571] [INFO] [config.py:982:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-10-28 22:57:01,571] [INFO] [config.py:982:print]   amp_enabled .................. False
[2022-10-28 22:57:01,571] [INFO] [config.py:982:print]   amp_params ................... False
[2022-10-28 22:57:01,571] [INFO] [config.py:982:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-10-28 22:57:01,575] [INFO] [config.py:982:print]   bfloat16_enabled ............. False
[2022-10-28 22:57:01,575] [INFO] [config.py:982:print]   checkpoint_tag_validation_enabled  True
[2022-10-28 22:57:01,575] [INFO] [config.py:982:print]   checkpoint_tag_validation_fail  False
[2022-10-28 22:57:01,575] [INFO] [config.py:982:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa5a32c2f70>
[2022-10-28 22:57:01,575] [INFO] [config.py:982:print]   communication_data_type ...... None
[2022-10-28 22:57:01,575] [INFO] [config.py:982:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-10-28 22:57:01,578] [INFO] [config.py:982:print]   curriculum_enabled_legacy .... False
[2022-10-28 22:57:01,578] [INFO] [config.py:982:print]   curriculum_params_legacy ..... {'curriculum_type': 'seqlen', 'min_difficulty': 72, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 221108, 'difficulty_step': 8}}
[2022-10-28 22:57:01,578] [INFO] [config.py:982:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}}
[2022-10-28 22:57:01,578] [INFO] [config.py:982:print]   data_efficiency_enabled ...... False
[2022-10-28 22:57:01,578] [INFO] [config.py:982:print]   dataloader_drop_last ......... False
[2022-10-28 22:57:01,582] [INFO] [config.py:982:print]   disable_allgather ............ False
[2022-10-28 22:57:01,583] [INFO] [config.py:982:print]   dump_state ................... False
[2022-10-28 22:57:01,583] [INFO] [config.py:982:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2022-10-28 22:57:01,583] [INFO] [config.py:982:print]   dynamic_train_config ......... {'random_ltd': {'enabled': False}}
[2022-10-28 22:57:01,583] [INFO] [config.py:982:print]   eigenvalue_enabled ........... False
[2022-10-28 22:57:01,587] [INFO] [config.py:982:print]   eigenvalue_gas_boundary_resolution  1
[2022-10-28 22:57:01,587] [INFO] [config.py:982:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-10-28 22:57:01,587] [INFO] [config.py:982:print]   eigenvalue_layer_num ......... 0
[2022-10-28 22:57:01,587] [INFO] [config.py:982:print]   eigenvalue_max_iter .......... 100
[2022-10-28 22:57:01,587] [INFO] [config.py:982:print]   eigenvalue_stability ......... 1e-06
[2022-10-28 22:57:01,587] [INFO] [config.py:982:print]   eigenvalue_tol ............... 0.01
[2022-10-28 22:57:01,587] [INFO] [config.py:982:print]   eigenvalue_verbose ........... False
[2022-10-28 22:57:01,587] [INFO] [config.py:982:print]   elasticity_enabled ........... False
[2022-10-28 22:57:01,590] [INFO] [config.py:982:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-10-28 22:57:01,593] [INFO] [config.py:982:print]   fp16_auto_cast ............... False
[2022-10-28 22:57:01,593] [INFO] [config.py:982:print]   fp16_enabled ................. True
[2022-10-28 22:57:01,593] [INFO] [config.py:982:print]   fp16_master_weights_and_gradients  False
[2022-10-28 22:57:01,593] [INFO] [config.py:982:print]   global_rank .................. 0
[2022-10-28 22:57:01,593] [INFO] [config.py:982:print]   gradient_accumulation_steps .. 8
[2022-10-28 22:57:01,593] [INFO] [config.py:982:print]   gradient_clipping ............ 1.0
[2022-10-28 22:57:01,593] [INFO] [config.py:982:print]   gradient_predivide_factor .... 1.0
[2022-10-28 22:57:01,593] [INFO] [config.py:982:print]   initial_dynamic_scale ........ 2048
[2022-10-28 22:57:01,597] [INFO] [config.py:982:print]   load_universal_checkpoint .... False
[2022-10-28 22:57:01,597] [INFO] [config.py:982:print]   loss_scale ................... 0
[2022-10-28 22:57:01,597] [INFO] [config.py:982:print]   memory_breakdown ............. False
[2022-10-28 22:57:01,597] [INFO] [config.py:982:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7fa5a5d15490>
[2022-10-28 22:57:01,597] [INFO] [config.py:982:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-10-28 22:57:01,600] [INFO] [config.py:982:print]   optimizer_legacy_fusion ...... False
[2022-10-28 22:57:01,600] [INFO] [config.py:982:print]   optimizer_name ............... None
[2022-10-28 22:57:01,600] [INFO] [config.py:982:print]   optimizer_params ............. None
[2022-10-28 22:57:01,601] [INFO] [config.py:982:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-10-28 22:57:01,601] [INFO] [config.py:982:print]   pld_enabled .................. False
[2022-10-28 22:57:01,601] [INFO] [config.py:982:print]   pld_params ................... False
[2022-10-28 22:57:01,601] [INFO] [config.py:982:print]   prescale_gradients ........... True
[2022-10-28 22:57:01,604] [INFO] [config.py:982:print]   scheduler_name ............... None
[2022-10-28 22:57:01,604] [INFO] [config.py:982:print]   scheduler_params ............. None
[2022-10-28 22:57:01,604] [INFO] [config.py:982:print]   sparse_attention ............. None
[2022-10-28 22:57:01,604] [INFO] [config.py:982:print]   sparse_gradients_enabled ..... False
[2022-10-28 22:57:01,604] [INFO] [config.py:982:print]   steps_per_print .............. 10
[2022-10-28 22:57:01,604] [INFO] [config.py:982:print]   train_batch_size ............. 256
[2022-10-28 22:57:01,604] [INFO] [config.py:982:print]   train_micro_batch_size_per_gpu  4
[2022-10-28 22:57:01,604] [INFO] [config.py:982:print]   wall_clock_breakdown ......... False
[2022-10-28 22:57:01,607] [INFO] [config.py:982:print]   world_size ................... 8
[2022-10-28 22:57:01,607] [INFO] [config.py:982:print]   zero_allow_untested_optimizer  False
[2022-10-28 22:57:01,607] [INFO] [config.py:982:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-10-28 22:57:01,607] [INFO] [config.py:982:print]   zero_enabled ................. False
[2022-10-28 22:57:01,607] [INFO] [config.py:982:print]   zero_optimization_stage ...... 0
[2022-10-28 22:57:01,611] [INFO] [config.py:967:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 4, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 72, 
        "max_difficulty": 2.048000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 2.211080e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.4834558963775635 seconds
Loading extension module utils...Loading extension module utils...Loading extension module utils...


Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.51181960105896 seconds
Time to load utils op: 0.5100786685943604 seconds
Time to load utils op: 0.5111141204833984 seconds
Time to load utils op: 0.511803150177002 seconds
Time to load utils op: 0.5112428665161133 seconds
Time to load utils op: 0.5104665756225586 seconds
Loading extension module utils...
Time to load utils op: 0.5067603588104248 seconds
[2022-10-28 22:57:02,339] [WARNING] [engine.py:2641:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-28 22:57:02,552] [WARNING] [engine.py:2641:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-28 22:57:02,747] [WARNING] [engine.py:2641:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-28 22:57:03,052] [WARNING] [engine.py:2641:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-28 22:57:03,265] [WARNING] [engine.py:2641:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
WARNING: could not find the metadata file /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4 
    will not load any checkpoints and will start from random
[2022-10-28 22:57:03,462] [WARNING] [engine.py:2641:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-28 22:57:03,661] [WARNING] [engine.py:2641:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-28 22:57:03,859] [WARNING] [engine.py:2641:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
time (ms) | load-checkpoint: 1738.10
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-10-28 22:57:03 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      439453125
    validation: 43947520
    test:       2560
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 3057.539135 seconds
    number of documents: 210604984
 > dataset split:
    train:
     document indices in [0, 206392884) total of 206392884 documents
    validation:
     document indices in [206392884, 210604984) total of 4212100 documents
    test:
     document indices in [210604984, 210604984) total of 0 documents
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58803:63650 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58801:63654 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58801:63654 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58811:63661 [7] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58800:63653 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58811:63661 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58800:63653 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000C6:58811:63661 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58805:63664 [4] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58809:63667 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58805:63664 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000C6:58807:63658 [5] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58807:63658 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58807:63658 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58799:63670 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58799:63670 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000C6:58801:63654 [2] NCCL INFO Connected all rings
azwus2f200000C6:58801:63654 [2] NCCL INFO Connected all trees
azwus2f200000C6:58801:63654 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58803:63650 [3] NCCL INFO Connected all rings
azwus2f200000C6:58811:63661 [7] NCCL INFO Connected all rings
azwus2f200000C6:58803:63650 [3] NCCL INFO Connected all trees
azwus2f200000C6:58803:63650 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58800:63653 [1] NCCL INFO Connected all rings
azwus2f200000C6:58811:63661 [7] NCCL INFO Connected all trees
azwus2f200000C6:58800:63653 [1] NCCL INFO Connected all trees
azwus2f200000C6:58811:63661 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58800:63653 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58803:63650 [3] NCCL INFO comm 0x7f0b0c002fb0 rank 0 nranks 1 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000C6:58801:63654 [2] NCCL INFO comm 0x7f4240002fb0 rank 0 nranks 1 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000C6:58811:63661 [7] NCCL INFO comm 0x7e5518002fb0 rank 0 nranks 1 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000C6:58800:63653 [1] NCCL INFO comm 0x7eb4c0002fb0 rank 0 nranks 1 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000C6:58809:63667 [6] NCCL INFO Connected all rings
azwus2f200000C6:58807:63658 [5] NCCL INFO Connected all rings
azwus2f200000C6:58809:63667 [6] NCCL INFO Connected all trees
azwus2f200000C6:58805:63664 [4] NCCL INFO Connected all rings
azwus2f200000C6:58807:63658 [5] NCCL INFO Connected all trees
azwus2f200000C6:58809:63667 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58807:63658 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58805:63664 [4] NCCL INFO Connected all trees
azwus2f200000C6:58805:63664 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58805:63664 [4] NCCL INFO comm 0x7e757c002fb0 rank 0 nranks 1 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000C6:58809:63667 [6] NCCL INFO comm 0x7ec600002fb0 rank 0 nranks 1 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000C6:58807:63658 [5] NCCL INFO comm 0x7e77ec002fb0 rank 0 nranks 1 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000C6:58799:63670 [0] NCCL INFO Connected all rings
azwus2f200000C6:58799:63670 [0] NCCL INFO Connected all trees
azwus2f200000C6:58799:63670 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58799:63670 [0] NCCL INFO comm 0x7eed8c002fb0 rank 0 nranks 1 cudaDev 0 busId 100000 - Init COMPLETE
 > loading doc-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_train_indexmap_439453125ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_train_indexmap_439453125ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_train_indexmap_439453125ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 36.855 seconds
    total number of samples: 537390992
    total number of epochs: 3
 > loading doc-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_valid_indexmap_43947520ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_valid_indexmap_43947520ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_valid_indexmap_43947520ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 4.089 seconds
    total number of samples: 47470692
    total number of epochs: 13
> finished creating GPT datasets ...
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58800:63719 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58800:63719 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58811:63709 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58811:63709 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58809:63714 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58809:63714 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58801:63712 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58801:63712 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58805:63717 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58805:63717 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58803:63711 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58803:63711 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58807:63716 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58807:63716 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58799:63728 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58799:63728 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000C6:58800:63719 [1] NCCL INFO Connected all rings
azwus2f200000C6:58800:63719 [1] NCCL INFO Connected all trees
azwus2f200000C6:58811:63709 [7] NCCL INFO Connected all rings
azwus2f200000C6:58800:63719 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58811:63709 [7] NCCL INFO Connected all trees
azwus2f200000C6:58811:63709 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58800:63719 [1] NCCL INFO comm 0x7eb288002fb0 rank 0 nranks 1 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000C6:58811:63709 [7] NCCL INFO comm 0x7e52e0002fb0 rank 0 nranks 1 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000C6:58809:63714 [6] NCCL INFO Connected all rings
azwus2f200000C6:58809:63714 [6] NCCL INFO Connected all trees
azwus2f200000C6:58809:63714 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58801:63712 [2] NCCL INFO Connected all rings
azwus2f200000C6:58809:63714 [6] NCCL INFO comm 0x7ec3c8002fb0 rank 0 nranks 1 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000C6:58801:63712 [2] NCCL INFO Connected all trees
azwus2f200000C6:58805:63717 [4] NCCL INFO Connected all rings
azwus2f200000C6:58801:63712 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58803:63711 [3] NCCL INFO Connected all rings
azwus2f200000C6:58805:63717 [4] NCCL INFO Connected all trees
azwus2f200000C6:58805:63717 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58803:63711 [3] NCCL INFO Connected all trees
azwus2f200000C6:58803:63711 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58801:63712 [2] NCCL INFO comm 0x7f4008002fb0 rank 0 nranks 1 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000C6:58803:63711 [3] NCCL INFO comm 0x7f08d4002fb0 rank 0 nranks 1 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000C6:58805:63717 [4] NCCL INFO comm 0x7e7344002fb0 rank 0 nranks 1 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000C6:58807:63716 [5] NCCL INFO Connected all rings
azwus2f200000C6:58807:63716 [5] NCCL INFO Connected all trees
azwus2f200000C6:58807:63716 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58799:63728 [0] NCCL INFO Connected all rings
azwus2f200000C6:58807:63716 [5] NCCL INFO comm 0x7e75b4002fb0 rank 0 nranks 1 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000C6:58799:63728 [0] NCCL INFO Connected all trees
azwus2f200000C6:58799:63728 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58799:63728 [0] NCCL INFO comm 0x7eeb54002fb0 rank 0 nranks 1 cudaDev 0 busId 100000 - Init COMPLETE
[after dataloaders are built] datetime: 2022-10-28 23:48:50 
done with setup ...
time (ms) | model-and-optimizer-setup: 29012.03 | train/valid/test-data-iterators-setup: 3106242.77training ...

[before the start of training step] datetime: 2022-10-28 23:48:50 
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58800:63823 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58800:63823 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58807:63818 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58807:63818 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58801:63816 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58801:63816 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58799:63819 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58799:63819 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58805:63817 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58805:63817 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58809:63822 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58809:63822 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58811:63813 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58811:63813 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 00/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 01/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 02/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 03/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 04/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 05/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 06/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 07/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 08/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 09/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 10/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 11/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 12/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 13/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 14/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 15/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 16/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 17/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 18/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 19/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 20/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 21/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 22/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 23/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 24/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 25/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 26/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 27/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 28/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 29/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 30/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Channel 31/32 :    0
azwus2f200000C6:58803:63820 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000C6:58803:63820 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000C6:58800:63823 [1] NCCL INFO Connected all rings
azwus2f200000C6:58800:63823 [1] NCCL INFO Connected all trees
azwus2f200000C6:58800:63823 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58807:63818 [5] NCCL INFO Connected all rings
azwus2f200000C6:58800:63823 [1] NCCL INFO comm 0x7eb120002fb0 rank 0 nranks 1 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000C6:58807:63818 [5] NCCL INFO Connected all trees
azwus2f200000C6:58807:63818 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58799:63819 [0] NCCL INFO Connected all rings
azwus2f200000C6:58799:63819 [0] NCCL INFO Connected all trees
azwus2f200000C6:58799:63819 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58801:63816 [2] NCCL INFO Connected all rings
azwus2f200000C6:58807:63818 [5] NCCL INFO comm 0x7e744c002fb0 rank 0 nranks 1 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000C6:58799:63819 [0] NCCL INFO comm 0x7ee9ec002fb0 rank 0 nranks 1 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000C6:58801:63816 [2] NCCL INFO Connected all trees
azwus2f200000C6:58801:63816 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58805:63817 [4] NCCL INFO Connected all rings
azwus2f200000C6:58809:63822 [6] NCCL INFO Connected all rings
azwus2f200000C6:58805:63817 [4] NCCL INFO Connected all trees
azwus2f200000C6:58801:63816 [2] NCCL INFO comm 0x7f3ea0002fb0 rank 0 nranks 1 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000C6:58809:63822 [6] NCCL INFO Connected all trees
azwus2f200000C6:58805:63817 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58809:63822 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58809:63822 [6] NCCL INFO comm 0x7ec260002fb0 rank 0 nranks 1 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000C6:58811:63813 [7] NCCL INFO Connected all rings
azwus2f200000C6:58805:63817 [4] NCCL INFO comm 0x7e71dc002fb0 rank 0 nranks 1 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000C6:58811:63813 [7] NCCL INFO Connected all trees
azwus2f200000C6:58803:63820 [3] NCCL INFO Connected all rings
azwus2f200000C6:58811:63813 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58803:63820 [3] NCCL INFO Connected all trees
azwus2f200000C6:58803:63820 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000C6:58811:63813 [7] NCCL INFO comm 0x7e5180002fb0 rank 0 nranks 1 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000C6:58803:63820 [3] NCCL INFO comm 0x7f076c002fb0 rank 0 nranks 1 cudaDev 3 busId 400000 - Init COMPLETE
[2022-10-28 23:48:58,641] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[5.308416e-07, 5.308416e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:48:58,643] [INFO] [timer.py:198:stop] 0/10, RunningAvgSamplesPerSec=446.08298723123164, CurrSamplesPerSec=440.9095860530664, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
[Rank 0] (after 10 iterations) memory (MB) | allocated: 1674.31005859375 | max allocated: 5799.13525390625 | reserved: 6100.0 | max reserved: 6100.0
 iteration       10/ 1716613 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 849.6 | learning rate: 5.308E-07 | global batch size:   256 | lm loss: 1.069056E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 340.72 | backward-compute: 176.25 | backward-embedding-all-reduce: 0.01 | optimizer: 313.71 | batch-generator: 42.49
[2022-10-28 23:49:03,373] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[1.1206655999999999e-06, 1.1206655999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:49:03,376] [INFO] [timer.py:198:stop] 0/20, RunningAvgSamplesPerSec=451.13104992877084, CurrSamplesPerSec=442.9540273129905, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration       20/ 1716613 | consumed samples:         5120 | consumed tokens:     10485760 | elapsed time per iteration (ms): 473.2 | learning rate: 1.121E-06 | global batch size:   256 | lm loss: 1.062032E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 281.56 | backward-compute: 156.97 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 37.60
[2022-10-28 23:49:09,157] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[1.7104895999999997e-06, 1.7104895999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:49:09,159] [INFO] [timer.py:198:stop] 0/30, RunningAvgSamplesPerSec=412.82501545582494, CurrSamplesPerSec=337.401716449052, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration       30/ 1716613 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 578.4 | learning rate: 1.710E-06 | global batch size:   256 | lm loss: 1.048839E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 387.99 | backward-compute: 156.00 | backward-embedding-all-reduce: 0.01 | optimizer: 14.01 | batch-generator: 35.90
[2022-10-28 23:49:14,596] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[2.3003135999999996e-06, 2.3003135999999996e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:49:14,599] [INFO] [timer.py:198:stop] 0/40, RunningAvgSamplesPerSec=415.6833360595807, CurrSamplesPerSec=469.9961060608183, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration       40/ 1716613 | consumed samples:        10240 | consumed tokens:     20971520 | elapsed time per iteration (ms): 543.7 | learning rate: 2.300E-06 | global batch size:   256 | lm loss: 1.025902E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 354.50 | backward-compute: 155.86 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 32.57
[2022-10-28 23:49:20,315] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[2.8901375999999997e-06, 2.8901375999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:49:20,318] [INFO] [timer.py:198:stop] 0/50, RunningAvgSamplesPerSec=408.9251761200351, CurrSamplesPerSec=391.5119290354385, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration       50/ 1716613 | consumed samples:        12800 | consumed tokens:     26214400 | elapsed time per iteration (ms): 572.2 | learning rate: 2.890E-06 | global batch size:   256 | lm loss: 1.009488E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 379.97 | backward-compute: 157.44 | backward-embedding-all-reduce: 0.01 | optimizer: 14.08 | batch-generator: 30.39
[2022-10-28 23:49:25,724] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[3.4799615999999993e-06, 3.4799615999999993e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:49:25,727] [INFO] [timer.py:198:stop] 0/60, RunningAvgSamplesPerSec=411.8029267282173, CurrSamplesPerSec=456.8725316994298, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration       60/ 1716613 | consumed samples:        15360 | consumed tokens:     31457280 | elapsed time per iteration (ms): 540.8 | learning rate: 3.480E-06 | global batch size:   256 | lm loss: 9.833009E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 349.40 | backward-compute: 157.25 | backward-embedding-all-reduce: 0.01 | optimizer: 14.04 | batch-generator: 29.60
[2022-10-28 23:49:31,271] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[4.0697856e-06, 4.0697856e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:49:31,273] [INFO] [timer.py:198:stop] 0/70, RunningAvgSamplesPerSec=411.8487490577862, CurrSamplesPerSec=449.9873537399001, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration       70/ 1716613 | consumed samples:        17920 | consumed tokens:     36700160 | elapsed time per iteration (ms): 554.6 | learning rate: 4.070E-06 | global batch size:   256 | lm loss: 9.580311E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 361.17 | backward-compute: 159.03 | backward-embedding-all-reduce: 0.01 | optimizer: 14.05 | batch-generator: 26.59
[2022-10-28 23:49:36,924] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[4.6596095999999995e-06, 4.6596095999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:49:36,926] [INFO] [timer.py:198:stop] 0/80, RunningAvgSamplesPerSec=407.05040866271884, CurrSamplesPerSec=328.2770860084871, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration       80/ 1716613 | consumed samples:        20480 | consumed tokens:     41943040 | elapsed time per iteration (ms): 565.3 | learning rate: 4.660E-06 | global batch size:   256 | lm loss: 9.481867E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 373.43 | backward-compute: 157.34 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 26.33
[2022-10-28 23:49:42,614] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[5.2494336e-06, 5.2494336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:49:42,616] [INFO] [timer.py:198:stop] 0/90, RunningAvgSamplesPerSec=406.80205647345866, CurrSamplesPerSec=392.44605456677283, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration       90/ 1716613 | consumed samples:        23040 | consumed tokens:     47185920 | elapsed time per iteration (ms): 569.1 | learning rate: 5.249E-06 | global batch size:   256 | lm loss: 9.284767E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 375.19 | backward-compute: 159.03 | backward-embedding-all-reduce: 0.01 | optimizer: 14.07 | batch-generator: 25.26
[2022-10-28 23:49:48,034] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[5.8392576e-06, 5.8392576e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:49:48,037] [INFO] [timer.py:198:stop] 0/100, RunningAvgSamplesPerSec=408.2009050450606, CurrSamplesPerSec=392.83429089898, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      100/ 1716613 | consumed samples:        25600 | consumed tokens:     52428800 | elapsed time per iteration (ms): 541.8 | learning rate: 5.839E-06 | global batch size:   256 | lm loss: 9.235551E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 348.16 | backward-compute: 159.56 | backward-embedding-all-reduce: 0.01 | optimizer: 14.04 | batch-generator: 25.55
-----------------------------------------------------------------------------------------------
 validation loss at iteration 100 | lm loss value: 9.082616E+00 | lm loss PPL: 8.800958E+03 | 
-----------------------------------------------------------------------------------------------
[2022-10-28 23:49:58,193] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[6.429081599999999e-06, 6.429081599999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:49:58,196] [INFO] [timer.py:198:stop] 0/110, RunningAvgSamplesPerSec=410.5193737874603, CurrSamplesPerSec=470.0175025126156, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      110/ 1716613 | consumed samples:        28160 | consumed tokens:     57671680 | elapsed time per iteration (ms): 1016.1 | learning rate: 6.429E-06 | global batch size:   256 | lm loss: 9.088983E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 822.11 | backward-compute: 158.08 | backward-embedding-all-reduce: 0.01 | optimizer: 14.29 | batch-generator: 52.08
[2022-10-28 23:50:03,643] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[7.018905599999999e-06, 7.018905599999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:50:03,646] [INFO] [timer.py:198:stop] 0/120, RunningAvgSamplesPerSec=411.87582724273784, CurrSamplesPerSec=456.28698088063317, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      120/ 1716613 | consumed samples:        30720 | consumed tokens:     62914560 | elapsed time per iteration (ms): 545.0 | learning rate: 7.019E-06 | global batch size:   256 | lm loss: 8.957894E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 352.71 | backward-compute: 158.31 | backward-embedding-all-reduce: 0.01 | optimizer: 14.01 | batch-generator: 24.52
[2022-10-28 23:50:08,953] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[7.6087295999999995e-06, 7.6087295999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:50:08,956] [INFO] [timer.py:198:stop] 0/130, RunningAvgSamplesPerSec=413.68213722822287, CurrSamplesPerSec=463.49581631137835, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      130/ 1716613 | consumed samples:        33280 | consumed tokens:     68157440 | elapsed time per iteration (ms): 531.1 | learning rate: 7.609E-06 | global batch size:   256 | lm loss: 8.870831E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 339.43 | backward-compute: 157.34 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 24.87
[2022-10-28 23:50:14,195] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[8.198553599999999e-06, 8.198553599999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:50:14,198] [INFO] [timer.py:198:stop] 0/140, RunningAvgSamplesPerSec=412.69424123640226, CurrSamplesPerSec=376.6362797067003, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      140/ 1716613 | consumed samples:        35840 | consumed tokens:     73400320 | elapsed time per iteration (ms): 523.9 | learning rate: 8.199E-06 | global batch size:   256 | lm loss: 8.760947E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 333.51 | backward-compute: 156.84 | backward-embedding-all-reduce: 0.01 | optimizer: 14.46 | batch-generator: 25.11
[2022-10-28 23:50:19,487] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[8.788377599999999e-06, 8.788377599999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:50:19,489] [INFO] [timer.py:198:stop] 0/150, RunningAvgSamplesPerSec=412.8396366145976, CurrSamplesPerSec=472.6991642571116, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      150/ 1716613 | consumed samples:        38400 | consumed tokens:     78643200 | elapsed time per iteration (ms): 529.2 | learning rate: 8.788E-06 | global batch size:   256 | lm loss: 8.664790E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 340.58 | backward-compute: 156.53 | backward-embedding-all-reduce: 0.01 | optimizer: 14.22 | batch-generator: 23.42
[2022-10-28 23:50:24,672] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[9.378201599999998e-06, 9.378201599999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:50:24,675] [INFO] [timer.py:198:stop] 0/160, RunningAvgSamplesPerSec=413.0630928794906, CurrSamplesPerSec=349.62002208931585, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      160/ 1716613 | consumed samples:        40960 | consumed tokens:     83886080 | elapsed time per iteration (ms): 518.5 | learning rate: 9.378E-06 | global batch size:   256 | lm loss: 8.663758E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 329.44 | backward-compute: 156.80 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 24.20
[2022-10-28 23:50:30,188] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[9.968025599999998e-06, 9.968025599999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:50:30,191] [INFO] [timer.py:198:stop] 0/170, RunningAvgSamplesPerSec=412.1647165232922, CurrSamplesPerSec=471.96944911350386, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      170/ 1716613 | consumed samples:        43520 | consumed tokens:     89128960 | elapsed time per iteration (ms): 551.8 | learning rate: 9.968E-06 | global batch size:   256 | lm loss: 8.550940E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 361.72 | backward-compute: 157.92 | backward-embedding-all-reduce: 0.01 | optimizer: 14.15 | batch-generator: 22.81
[2022-10-28 23:50:35,566] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[1.05578496e-05, 1.05578496e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:50:35,569] [INFO] [timer.py:198:stop] 0/180, RunningAvgSamplesPerSec=412.6208805192731, CurrSamplesPerSec=398.24146505017416, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      180/ 1716613 | consumed samples:        46080 | consumed tokens:     94371840 | elapsed time per iteration (ms): 537.8 | learning rate: 1.056E-05 | global batch size:   256 | lm loss: 8.450900E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 347.40 | backward-compute: 158.07 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 22.29
[2022-10-28 23:50:40,953] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[1.1147673599999999e-05, 1.1147673599999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:50:40,956] [INFO] [timer.py:198:stop] 0/190, RunningAvgSamplesPerSec=412.82806734417903, CurrSamplesPerSec=390.8677399063439, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      190/ 1716613 | consumed samples:        48640 | consumed tokens:     99614720 | elapsed time per iteration (ms): 538.7 | learning rate: 1.115E-05 | global batch size:   256 | lm loss: 8.342679E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 348.86 | backward-compute: 156.96 | backward-embedding-all-reduce: 0.01 | optimizer: 14.16 | batch-generator: 24.44
[2022-10-28 23:50:46,345] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[1.1737497599999999e-05, 1.1737497599999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:50:46,347] [INFO] [timer.py:198:stop] 0/200, RunningAvgSamplesPerSec=413.0517492711823, CurrSamplesPerSec=385.2690728929254, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      200/ 1716613 | consumed samples:        51200 | consumed tokens:    104857600 | elapsed time per iteration (ms): 539.1 | learning rate: 1.174E-05 | global batch size:   256 | lm loss: 8.202409E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 348.62 | backward-compute: 157.67 | backward-embedding-all-reduce: 0.01 | optimizer: 14.05 | batch-generator: 23.73
-----------------------------------------------------------------------------------------------
 validation loss at iteration 200 | lm loss value: 8.075084E+00 | lm loss PPL: 3.213396E+03 | 
-----------------------------------------------------------------------------------------------
[2022-10-28 23:50:55,958] [INFO] [logging.py:68:log_dist] [Rank 0] step=210, skipped=0, lr=[1.2327321599999998e-05, 1.2327321599999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:50:55,961] [INFO] [timer.py:198:stop] 0/210, RunningAvgSamplesPerSec=413.34571631325946, CurrSamplesPerSec=469.03036063740564, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      210/ 1716613 | consumed samples:        53760 | consumed tokens:    110100480 | elapsed time per iteration (ms): 961.3 | learning rate: 1.233E-05 | global batch size:   256 | lm loss: 8.199614E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 770.63 | backward-compute: 157.52 | backward-embedding-all-reduce: 0.01 | optimizer: 14.02 | batch-generator: 45.75
[2022-10-28 23:51:01,435] [INFO] [logging.py:68:log_dist] [Rank 0] step=220, skipped=0, lr=[1.2917145599999998e-05, 1.2917145599999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:51:01,438] [INFO] [timer.py:198:stop] 0/220, RunningAvgSamplesPerSec=412.1030587479573, CurrSamplesPerSec=386.56734369419706, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      220/ 1716613 | consumed samples:        56320 | consumed tokens:    115343360 | elapsed time per iteration (ms): 547.7 | learning rate: 1.292E-05 | global batch size:   256 | lm loss: 8.007769E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 355.16 | backward-compute: 158.77 | backward-embedding-all-reduce: 0.01 | optimizer: 14.36 | batch-generator: 23.96
[2022-10-28 23:51:06,822] [INFO] [logging.py:68:log_dist] [Rank 0] step=230, skipped=0, lr=[1.3506969599999998e-05, 1.3506969599999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:51:06,825] [INFO] [timer.py:198:stop] 0/230, RunningAvgSamplesPerSec=412.7400289517548, CurrSamplesPerSec=468.25636964341163, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      230/ 1716613 | consumed samples:        58880 | consumed tokens:    120586240 | elapsed time per iteration (ms): 538.7 | learning rate: 1.351E-05 | global batch size:   256 | lm loss: 7.985692E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 347.00 | backward-compute: 158.21 | backward-embedding-all-reduce: 0.01 | optimizer: 14.53 | batch-generator: 23.47
[2022-10-28 23:51:12,252] [INFO] [logging.py:68:log_dist] [Rank 0] step=240, skipped=0, lr=[1.4096793599999997e-05, 1.4096793599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:51:12,254] [INFO] [timer.py:198:stop] 0/240, RunningAvgSamplesPerSec=413.27367496224514, CurrSamplesPerSec=353.0651795343943, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      240/ 1716613 | consumed samples:        61440 | consumed tokens:    125829120 | elapsed time per iteration (ms): 542.9 | learning rate: 1.410E-05 | global batch size:   256 | lm loss: 7.832829E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 351.67 | backward-compute: 158.12 | backward-embedding-all-reduce: 0.01 | optimizer: 13.96 | batch-generator: 23.45
[2022-10-28 23:51:17,668] [INFO] [logging.py:68:log_dist] [Rank 0] step=250, skipped=0, lr=[1.4686617599999997e-05, 1.4686617599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:51:17,671] [INFO] [timer.py:198:stop] 0/250, RunningAvgSamplesPerSec=413.470295505087, CurrSamplesPerSec=445.6085072758723, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      250/ 1716613 | consumed samples:        64000 | consumed tokens:    131072000 | elapsed time per iteration (ms): 541.7 | learning rate: 1.469E-05 | global batch size:   256 | lm loss: 7.744413E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 350.73 | backward-compute: 157.89 | backward-embedding-all-reduce: 0.01 | optimizer: 14.10 | batch-generator: 25.96
[2022-10-28 23:51:23,066] [INFO] [logging.py:68:log_dist] [Rank 0] step=260, skipped=0, lr=[1.5276441599999997e-05, 1.5276441599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:51:23,069] [INFO] [timer.py:198:stop] 0/260, RunningAvgSamplesPerSec=414.1706831540568, CurrSamplesPerSec=410.1306866795414, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      260/ 1716613 | consumed samples:        66560 | consumed tokens:    136314880 | elapsed time per iteration (ms): 539.8 | learning rate: 1.528E-05 | global batch size:   256 | lm loss: 7.683002E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 349.65 | backward-compute: 157.40 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 22.86
[2022-10-28 23:51:28,386] [INFO] [logging.py:68:log_dist] [Rank 0] step=270, skipped=0, lr=[1.5866265599999998e-05, 1.5866265599999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:51:28,389] [INFO] [timer.py:198:stop] 0/270, RunningAvgSamplesPerSec=413.52507769441866, CurrSamplesPerSec=403.6599118200793, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      270/ 1716613 | consumed samples:        69120 | consumed tokens:    141557760 | elapsed time per iteration (ms): 532.0 | learning rate: 1.587E-05 | global batch size:   256 | lm loss: 7.583304E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 339.60 | backward-compute: 158.30 | backward-embedding-all-reduce: 0.01 | optimizer: 14.34 | batch-generator: 22.73
[2022-10-28 23:51:33,789] [INFO] [logging.py:68:log_dist] [Rank 0] step=280, skipped=0, lr=[1.64560896e-05, 1.64560896e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:51:33,792] [INFO] [timer.py:198:stop] 0/280, RunningAvgSamplesPerSec=413.4138437119694, CurrSamplesPerSec=469.83651708795844, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      280/ 1716613 | consumed samples:        71680 | consumed tokens:    146800640 | elapsed time per iteration (ms): 540.3 | learning rate: 1.646E-05 | global batch size:   256 | lm loss: 7.490342E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 350.08 | backward-compute: 157.87 | backward-embedding-all-reduce: 0.01 | optimizer: 14.06 | batch-generator: 23.08
[2022-10-28 23:51:39,044] [INFO] [logging.py:68:log_dist] [Rank 0] step=290, skipped=0, lr=[1.70459136e-05, 1.70459136e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:51:39,047] [INFO] [timer.py:198:stop] 0/290, RunningAvgSamplesPerSec=413.61328106040577, CurrSamplesPerSec=391.9830143659446, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      290/ 1716613 | consumed samples:        74240 | consumed tokens:    152043520 | elapsed time per iteration (ms): 525.5 | learning rate: 1.705E-05 | global batch size:   256 | lm loss: 7.394756E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 334.49 | backward-compute: 158.00 | backward-embedding-all-reduce: 0.01 | optimizer: 14.09 | batch-generator: 22.31
[2022-10-28 23:51:44,473] [INFO] [logging.py:68:log_dist] [Rank 0] step=300, skipped=0, lr=[1.76357376e-05, 1.76357376e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:51:44,476] [INFO] [timer.py:198:stop] 0/300, RunningAvgSamplesPerSec=413.98619072499093, CurrSamplesPerSec=468.87306466938685, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      300/ 1716613 | consumed samples:        76800 | consumed tokens:    157286400 | elapsed time per iteration (ms): 542.9 | learning rate: 1.764E-05 | global batch size:   256 | lm loss: 7.376567E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 350.47 | backward-compute: 159.18 | backward-embedding-all-reduce: 0.01 | optimizer: 14.24 | batch-generator: 24.07
-----------------------------------------------------------------------------------------------
 validation loss at iteration 300 | lm loss value: 7.308299E+00 | lm loss PPL: 1.492635E+03 | 
-----------------------------------------------------------------------------------------------
[2022-10-28 23:51:53,679] [INFO] [logging.py:68:log_dist] [Rank 0] step=310, skipped=0, lr=[1.82255616e-05, 1.82255616e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:51:53,682] [INFO] [timer.py:198:stop] 0/310, RunningAvgSamplesPerSec=415.29114895992313, CurrSamplesPerSec=468.3527745015249, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      310/ 1716613 | consumed samples:        79360 | consumed tokens:    162529280 | elapsed time per iteration (ms): 920.6 | learning rate: 1.823E-05 | global batch size:   256 | lm loss: 7.271868E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 727.40 | backward-compute: 160.05 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 43.78
[2022-10-28 23:51:58,911] [INFO] [logging.py:68:log_dist] [Rank 0] step=320, skipped=0, lr=[1.8815385599999998e-05, 1.8815385599999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:51:58,914] [INFO] [timer.py:198:stop] 0/320, RunningAvgSamplesPerSec=414.73077809186583, CurrSamplesPerSec=408.3935639346657, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      320/ 1716613 | consumed samples:        81920 | consumed tokens:    167772160 | elapsed time per iteration (ms): 523.1 | learning rate: 1.882E-05 | global batch size:   256 | lm loss: 7.166348E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 331.25 | backward-compute: 158.88 | backward-embedding-all-reduce: 0.01 | optimizer: 14.06 | batch-generator: 23.61
[2022-10-28 23:52:03,968] [INFO] [logging.py:68:log_dist] [Rank 0] step=330, skipped=0, lr=[1.94052096e-05, 1.94052096e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:52:03,970] [INFO] [timer.py:198:stop] 0/330, RunningAvgSamplesPerSec=415.4115426666135, CurrSamplesPerSec=468.5571532803396, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      330/ 1716613 | consumed samples:        84480 | consumed tokens:    173015040 | elapsed time per iteration (ms): 505.7 | learning rate: 1.941E-05 | global batch size:   256 | lm loss: 7.175598E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 314.20 | backward-compute: 158.56 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 22.85
[2022-10-28 23:52:09,012] [INFO] [logging.py:68:log_dist] [Rank 0] step=340, skipped=0, lr=[1.9995033599999997e-05, 1.9995033599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:52:09,014] [INFO] [timer.py:198:stop] 0/340, RunningAvgSamplesPerSec=415.4228713289663, CurrSamplesPerSec=473.1757746260396, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      340/ 1716613 | consumed samples:        87040 | consumed tokens:    178257920 | elapsed time per iteration (ms): 504.4 | learning rate: 2.000E-05 | global batch size:   256 | lm loss: 7.056921E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 313.89 | backward-compute: 157.48 | backward-embedding-all-reduce: 0.01 | optimizer: 14.06 | batch-generator: 23.53
[2022-10-28 23:52:14,343] [INFO] [logging.py:68:log_dist] [Rank 0] step=350, skipped=0, lr=[2.05848576e-05, 2.05848576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:52:14,346] [INFO] [timer.py:198:stop] 0/350, RunningAvgSamplesPerSec=415.4379170159463, CurrSamplesPerSec=424.36496890403726, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      350/ 1716613 | consumed samples:        89600 | consumed tokens:    183500800 | elapsed time per iteration (ms): 533.1 | learning rate: 2.058E-05 | global batch size:   256 | lm loss: 7.037567E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 341.11 | backward-compute: 159.49 | backward-embedding-all-reduce: 0.01 | optimizer: 14.06 | batch-generator: 22.44
[2022-10-28 23:52:19,588] [INFO] [logging.py:68:log_dist] [Rank 0] step=360, skipped=0, lr=[2.1174681599999997e-05, 2.1174681599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:52:19,590] [INFO] [timer.py:198:stop] 0/360, RunningAvgSamplesPerSec=416.32056965540403, CurrSamplesPerSec=472.05244647027376, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      360/ 1716613 | consumed samples:        92160 | consumed tokens:    188743680 | elapsed time per iteration (ms): 524.5 | learning rate: 2.117E-05 | global batch size:   256 | lm loss: 6.940005E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 331.09 | backward-compute: 159.98 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 22.63
[2022-10-28 23:52:24,849] [INFO] [logging.py:68:log_dist] [Rank 0] step=370, skipped=0, lr=[2.1764505599999998e-05, 2.1764505599999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:52:24,851] [INFO] [timer.py:198:stop] 0/370, RunningAvgSamplesPerSec=417.0996326598766, CurrSamplesPerSec=464.66720443695107, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      370/ 1716613 | consumed samples:        94720 | consumed tokens:    193986560 | elapsed time per iteration (ms): 526.0 | learning rate: 2.176E-05 | global batch size:   256 | lm loss: 6.887592E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 334.70 | backward-compute: 158.53 | backward-embedding-all-reduce: 0.01 | optimizer: 14.02 | batch-generator: 22.03
[2022-10-28 23:52:30,064] [INFO] [logging.py:68:log_dist] [Rank 0] step=380, skipped=0, lr=[2.2354329599999996e-05, 2.2354329599999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:52:30,067] [INFO] [timer.py:198:stop] 0/380, RunningAvgSamplesPerSec=417.6892546071838, CurrSamplesPerSec=458.0544063996287, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      380/ 1716613 | consumed samples:        97280 | consumed tokens:    199229440 | elapsed time per iteration (ms): 521.6 | learning rate: 2.235E-05 | global batch size:   256 | lm loss: 6.824432E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 330.83 | backward-compute: 157.79 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 23.68
[2022-10-28 23:52:35,501] [INFO] [logging.py:68:log_dist] [Rank 0] step=390, skipped=0, lr=[2.2944153599999997e-05, 2.2944153599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:52:35,504] [INFO] [timer.py:198:stop] 0/390, RunningAvgSamplesPerSec=417.3719689146175, CurrSamplesPerSec=356.2353603155266, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      390/ 1716613 | consumed samples:        99840 | consumed tokens:    204472320 | elapsed time per iteration (ms): 543.8 | learning rate: 2.294E-05 | global batch size:   256 | lm loss: 6.754166E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 353.67 | backward-compute: 157.24 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 22.62
[2022-10-28 23:52:40,968] [INFO] [logging.py:68:log_dist] [Rank 0] step=400, skipped=0, lr=[2.3533977599999995e-05, 2.3533977599999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:52:40,970] [INFO] [timer.py:198:stop] 0/400, RunningAvgSamplesPerSec=417.2151630708771, CurrSamplesPerSec=461.27368956463164, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      400/ 1716613 | consumed samples:       102400 | consumed tokens:    209715200 | elapsed time per iteration (ms): 546.6 | learning rate: 2.353E-05 | global batch size:   256 | lm loss: 6.706354E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 356.11 | backward-compute: 156.58 | backward-embedding-all-reduce: 0.01 | optimizer: 14.33 | batch-generator: 24.17
-----------------------------------------------------------------------------------------------
 validation loss at iteration 400 | lm loss value: 6.690281E+00 | lm loss PPL: 8.045482E+02 | 
-----------------------------------------------------------------------------------------------
[2022-10-28 23:52:50,812] [INFO] [logging.py:68:log_dist] [Rank 0] step=410, skipped=0, lr=[2.4123801599999997e-05, 2.4123801599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:52:50,815] [INFO] [timer.py:198:stop] 0/410, RunningAvgSamplesPerSec=416.685335351756, CurrSamplesPerSec=405.15746227431754, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      410/ 1716613 | consumed samples:       104960 | consumed tokens:    214958080 | elapsed time per iteration (ms): 984.3 | learning rate: 2.412E-05 | global batch size:   256 | lm loss: 6.680529E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 794.27 | backward-compute: 156.73 | backward-embedding-all-reduce: 0.01 | optimizer: 14.05 | batch-generator: 41.83
[2022-10-28 23:52:56,082] [INFO] [logging.py:68:log_dist] [Rank 0] step=420, skipped=0, lr=[2.4713625599999998e-05, 2.4713625599999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:52:56,085] [INFO] [timer.py:198:stop] 0/420, RunningAvgSamplesPerSec=417.48249273204584, CurrSamplesPerSec=477.68565886644717, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      420/ 1716613 | consumed samples:       107520 | consumed tokens:    220200960 | elapsed time per iteration (ms): 527.0 | learning rate: 2.471E-05 | global batch size:   256 | lm loss: 6.667885E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 339.55 | backward-compute: 155.99 | backward-embedding-all-reduce: 0.01 | optimizer: 13.96 | batch-generator: 23.45
[2022-10-28 23:53:01,560] [INFO] [logging.py:68:log_dist] [Rank 0] step=430, skipped=0, lr=[2.53034496e-05, 2.53034496e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:53:01,563] [INFO] [timer.py:198:stop] 0/430, RunningAvgSamplesPerSec=417.87604512316204, CurrSamplesPerSec=471.4455801667755, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      430/ 1716613 | consumed samples:       110080 | consumed tokens:    225443840 | elapsed time per iteration (ms): 547.8 | learning rate: 2.530E-05 | global batch size:   256 | lm loss: 6.657738E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 358.20 | backward-compute: 157.46 | backward-embedding-all-reduce: 0.01 | optimizer: 14.12 | batch-generator: 21.87
[2022-10-28 23:53:07,066] [INFO] [logging.py:68:log_dist] [Rank 0] step=440, skipped=0, lr=[2.5893273599999997e-05, 2.5893273599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:53:07,068] [INFO] [timer.py:198:stop] 0/440, RunningAvgSamplesPerSec=417.59694798884215, CurrSamplesPerSec=461.9977144115987, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      440/ 1716613 | consumed samples:       112640 | consumed tokens:    230686720 | elapsed time per iteration (ms): 550.5 | learning rate: 2.589E-05 | global batch size:   256 | lm loss: 6.601845E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 361.79 | backward-compute: 157.00 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 21.56
[2022-10-28 23:53:12,567] [INFO] [logging.py:68:log_dist] [Rank 0] step=450, skipped=0, lr=[2.64830976e-05, 2.64830976e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:53:12,570] [INFO] [timer.py:198:stop] 0/450, RunningAvgSamplesPerSec=416.982865320305, CurrSamplesPerSec=392.4839254905401, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      450/ 1716613 | consumed samples:       115200 | consumed tokens:    235929600 | elapsed time per iteration (ms): 550.1 | learning rate: 2.648E-05 | global batch size:   256 | lm loss: 6.546951E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 362.70 | backward-compute: 155.49 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 23.37
[2022-10-28 23:53:18,222] [INFO] [logging.py:68:log_dist] [Rank 0] step=460, skipped=0, lr=[2.7072921599999997e-05, 2.7072921599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:53:18,224] [INFO] [timer.py:198:stop] 0/460, RunningAvgSamplesPerSec=415.8702431291965, CurrSamplesPerSec=367.3142475568071, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      460/ 1716613 | consumed samples:       117760 | consumed tokens:    241172480 | elapsed time per iteration (ms): 565.1 | learning rate: 2.707E-05 | global batch size:   256 | lm loss: 6.484863E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 378.03 | backward-compute: 154.98 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 22.83
[2022-10-28 23:53:24,101] [INFO] [logging.py:68:log_dist] [Rank 0] step=470, skipped=0, lr=[2.7662745599999998e-05, 2.7662745599999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:53:24,104] [INFO] [timer.py:198:stop] 0/470, RunningAvgSamplesPerSec=415.32506866340225, CurrSamplesPerSec=376.86682521810184, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      470/ 1716613 | consumed samples:       120320 | consumed tokens:    246415360 | elapsed time per iteration (ms): 588.2 | learning rate: 2.766E-05 | global batch size:   256 | lm loss: 6.467755E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 398.76 | backward-compute: 157.71 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 21.53
[2022-10-28 23:53:29,971] [INFO] [logging.py:68:log_dist] [Rank 0] step=480, skipped=0, lr=[2.82525696e-05, 2.82525696e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:53:29,973] [INFO] [timer.py:198:stop] 0/480, RunningAvgSamplesPerSec=414.4594739744254, CurrSamplesPerSec=466.36087811590073, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      480/ 1716613 | consumed samples:       122880 | consumed tokens:    251658240 | elapsed time per iteration (ms): 587.0 | learning rate: 2.825E-05 | global batch size:   256 | lm loss: 6.405563E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 397.77 | backward-compute: 157.59 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 22.26
[2022-10-28 23:53:35,427] [INFO] [logging.py:68:log_dist] [Rank 0] step=490, skipped=0, lr=[2.8842393599999997e-05, 2.8842393599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:53:35,429] [INFO] [timer.py:198:stop] 0/490, RunningAvgSamplesPerSec=414.9009337331928, CurrSamplesPerSec=457.62179928672253, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      490/ 1716613 | consumed samples:       125440 | consumed tokens:    256901120 | elapsed time per iteration (ms): 545.6 | learning rate: 2.884E-05 | global batch size:   256 | lm loss: 6.367046E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 358.80 | backward-compute: 155.45 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 23.82
[2022-10-28 23:53:40,843] [INFO] [logging.py:68:log_dist] [Rank 0] step=500, skipped=0, lr=[2.9432217600000002e-05, 2.9432217600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:53:40,846] [INFO] [timer.py:198:stop] 0/500, RunningAvgSamplesPerSec=414.5680495569137, CurrSamplesPerSec=374.20847517641516, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      500/ 1716613 | consumed samples:       128000 | consumed tokens:    262144000 | elapsed time per iteration (ms): 541.6 | learning rate: 2.943E-05 | global batch size:   256 | lm loss: 6.348278E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 352.37 | backward-compute: 157.60 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 22.60
-----------------------------------------------------------------------------------------------
 validation loss at iteration 500 | lm loss value: 6.333366E+00 | lm loss PPL: 5.630489E+02 | 
-----------------------------------------------------------------------------------------------
[2022-10-28 23:53:45,962] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:53:45,962] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:53:45,962] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-28 23:53:45,962] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-28 23:53:45,962] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:53:45,962] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:53:45,969] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-28 23:53:45,962] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:53:45,969] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-28 23:53:45,963] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:53:45,969] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-28 23:53:45,963] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:53:45,969] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-28 23:53:45,963] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:53:45,969] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-28 23:53:45,974] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-28 23:53:51,167] [INFO] [logging.py:68:log_dist] [Rank 0] step=510, skipped=0, lr=[3.0022041599999997e-05, 3.0022041599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:53:51,169] [INFO] [timer.py:198:stop] 0/510, RunningAvgSamplesPerSec=414.12139479817205, CurrSamplesPerSec=470.2431066981522, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      510/ 1716613 | consumed samples:       130560 | consumed tokens:    267386880 | elapsed time per iteration (ms): 1032.5 | learning rate: 3.002E-05 | global batch size:   256 | lm loss: 6.297860E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 840.66 | backward-compute: 157.74 | backward-embedding-all-reduce: 0.01 | optimizer: 15.21 | batch-generator: 41.10
[2022-10-28 23:53:56,696] [INFO] [logging.py:68:log_dist] [Rank 0] step=520, skipped=0, lr=[3.06118656e-05, 3.06118656e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:53:56,698] [INFO] [timer.py:198:stop] 0/520, RunningAvgSamplesPerSec=413.8310777757953, CurrSamplesPerSec=408.6459772564661, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      520/ 1716613 | consumed samples:       133120 | consumed tokens:    272629760 | elapsed time per iteration (ms): 552.9 | learning rate: 3.061E-05 | global batch size:   256 | lm loss: 6.268655E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 363.39 | backward-compute: 156.66 | backward-embedding-all-reduce: 0.01 | optimizer: 14.08 | batch-generator: 22.68
[2022-10-28 23:54:02,453] [INFO] [logging.py:68:log_dist] [Rank 0] step=530, skipped=0, lr=[3.1201689599999996e-05, 3.1201689599999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:54:02,455] [INFO] [timer.py:198:stop] 0/530, RunningAvgSamplesPerSec=414.2094342311069, CurrSamplesPerSec=470.6718566989992, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      530/ 1716613 | consumed samples:       135680 | consumed tokens:    277872640 | elapsed time per iteration (ms): 575.7 | learning rate: 3.120E-05 | global batch size:   256 | lm loss: 6.308060E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 386.54 | backward-compute: 156.12 | backward-embedding-all-reduce: 0.01 | optimizer: 13.96 | batch-generator: 22.96
[2022-10-28 23:54:08,029] [INFO] [logging.py:68:log_dist] [Rank 0] step=540, skipped=0, lr=[3.17915136e-05, 3.17915136e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:54:08,031] [INFO] [timer.py:198:stop] 0/540, RunningAvgSamplesPerSec=414.7311796752025, CurrSamplesPerSec=392.84118960718143, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      540/ 1716613 | consumed samples:       138240 | consumed tokens:    283115520 | elapsed time per iteration (ms): 557.6 | learning rate: 3.179E-05 | global batch size:   256 | lm loss: 6.187885E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 366.39 | backward-compute: 157.55 | backward-embedding-all-reduce: 0.01 | optimizer: 14.12 | batch-generator: 23.06
[2022-10-28 23:54:13,900] [INFO] [logging.py:68:log_dist] [Rank 0] step=550, skipped=0, lr=[3.23813376e-05, 3.23813376e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:54:13,905] [INFO] [timer.py:198:stop] 0/550, RunningAvgSamplesPerSec=414.8020175158258, CurrSamplesPerSec=460.31816061678603, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      550/ 1716613 | consumed samples:       140800 | consumed tokens:    288358400 | elapsed time per iteration (ms): 587.2 | learning rate: 3.238E-05 | global batch size:   256 | lm loss: 6.154863E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 397.24 | backward-compute: 156.55 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 21.81
[2022-10-28 23:54:19,474] [INFO] [logging.py:68:log_dist] [Rank 0] step=560, skipped=0, lr=[3.2971161599999997e-05, 3.2971161599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:54:19,476] [INFO] [timer.py:198:stop] 0/560, RunningAvgSamplesPerSec=414.6488113745539, CurrSamplesPerSec=397.75168992505354, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      560/ 1716613 | consumed samples:       143360 | consumed tokens:    293601280 | elapsed time per iteration (ms): 557.3 | learning rate: 3.297E-05 | global batch size:   256 | lm loss: 6.155941E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 370.14 | backward-compute: 154.40 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 22.71
[2022-10-28 23:54:25,386] [INFO] [logging.py:68:log_dist] [Rank 0] step=570, skipped=0, lr=[3.3560985599999995e-05, 3.3560985599999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:54:25,389] [INFO] [timer.py:198:stop] 0/570, RunningAvgSamplesPerSec=413.2560105196759, CurrSamplesPerSec=254.1809862889175, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      570/ 1716613 | consumed samples:       145920 | consumed tokens:    298844160 | elapsed time per iteration (ms): 591.3 | learning rate: 3.356E-05 | global batch size:   256 | lm loss: 6.125657E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 401.67 | backward-compute: 155.59 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 22.39
[2022-10-28 23:54:30,807] [INFO] [logging.py:68:log_dist] [Rank 0] step=580, skipped=0, lr=[3.41508096e-05, 3.41508096e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:54:30,810] [INFO] [timer.py:198:stop] 0/580, RunningAvgSamplesPerSec=413.562058143215, CurrSamplesPerSec=364.59923286718606, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      580/ 1716613 | consumed samples:       148480 | consumed tokens:    304087040 | elapsed time per iteration (ms): 541.7 | learning rate: 3.415E-05 | global batch size:   256 | lm loss: 6.066160E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 351.56 | backward-compute: 157.24 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 21.87
[2022-10-28 23:54:36,552] [INFO] [logging.py:68:log_dist] [Rank 0] step=590, skipped=0, lr=[3.47406336e-05, 3.47406336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:54:36,555] [INFO] [timer.py:198:stop] 0/590, RunningAvgSamplesPerSec=413.1209609429323, CurrSamplesPerSec=343.1028763663507, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      590/ 1716613 | consumed samples:       151040 | consumed tokens:    309329920 | elapsed time per iteration (ms): 574.9 | learning rate: 3.474E-05 | global batch size:   256 | lm loss: 6.072766E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 382.89 | backward-compute: 159.05 | backward-embedding-all-reduce: 0.01 | optimizer: 13.93 | batch-generator: 21.88
[2022-10-28 23:54:42,350] [INFO] [logging.py:68:log_dist] [Rank 0] step=600, skipped=0, lr=[3.5330457599999995e-05, 3.5330457599999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:54:42,352] [INFO] [timer.py:198:stop] 0/600, RunningAvgSamplesPerSec=412.74459328668814, CurrSamplesPerSec=305.8354631131852, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      600/ 1716613 | consumed samples:       153600 | consumed tokens:    314572800 | elapsed time per iteration (ms): 579.7 | learning rate: 3.533E-05 | global batch size:   256 | lm loss: 6.086641E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 387.38 | backward-compute: 159.59 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 20.49
-----------------------------------------------------------------------------------------------
 validation loss at iteration 600 | lm loss value: 5.977872E+00 | lm loss PPL: 3.945999E+02 | 
-----------------------------------------------------------------------------------------------
[2022-10-28 23:54:53,184] [INFO] [logging.py:68:log_dist] [Rank 0] step=610, skipped=0, lr=[3.592028159999999e-05, 3.592028159999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:54:53,187] [INFO] [timer.py:198:stop] 0/610, RunningAvgSamplesPerSec=412.55210509569, CurrSamplesPerSec=468.01471506131855, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      610/ 1716613 | consumed samples:       156160 | consumed tokens:    319815680 | elapsed time per iteration (ms): 1083.4 | learning rate: 3.592E-05 | global batch size:   256 | lm loss: 5.998275E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 890.19 | backward-compute: 159.86 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 39.84
[2022-10-28 23:54:58,911] [INFO] [logging.py:68:log_dist] [Rank 0] step=620, skipped=0, lr=[3.65101056e-05, 3.65101056e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:54:58,914] [INFO] [timer.py:198:stop] 0/620, RunningAvgSamplesPerSec=411.32646781200543, CurrSamplesPerSec=473.41444040774576, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      620/ 1716613 | consumed samples:       158720 | consumed tokens:    325058560 | elapsed time per iteration (ms): 572.8 | learning rate: 3.651E-05 | global batch size:   256 | lm loss: 5.967393E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 385.33 | backward-compute: 154.94 | backward-embedding-all-reduce: 0.01 | optimizer: 14.10 | batch-generator: 22.74
[2022-10-28 23:55:04,585] [INFO] [logging.py:68:log_dist] [Rank 0] step=630, skipped=0, lr=[3.70999296e-05, 3.70999296e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:55:04,588] [INFO] [timer.py:198:stop] 0/630, RunningAvgSamplesPerSec=411.4482426697971, CurrSamplesPerSec=469.7641619387287, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      630/ 1716613 | consumed samples:       161280 | consumed tokens:    330301440 | elapsed time per iteration (ms): 567.3 | learning rate: 3.710E-05 | global batch size:   256 | lm loss: 5.968924E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 377.38 | backward-compute: 156.41 | backward-embedding-all-reduce: 0.01 | optimizer: 14.09 | batch-generator: 22.72
[2022-10-28 23:55:10,433] [INFO] [logging.py:68:log_dist] [Rank 0] step=640, skipped=0, lr=[3.7689753599999994e-05, 3.7689753599999994e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:55:10,436] [INFO] [timer.py:198:stop] 0/640, RunningAvgSamplesPerSec=411.08020448120766, CurrSamplesPerSec=381.8601358810074, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      640/ 1716613 | consumed samples:       163840 | consumed tokens:    335544320 | elapsed time per iteration (ms): 584.9 | learning rate: 3.769E-05 | global batch size:   256 | lm loss: 5.927871E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 393.02 | backward-compute: 157.88 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 22.17
[2022-10-28 23:55:16,450] [INFO] [logging.py:68:log_dist] [Rank 0] step=650, skipped=0, lr=[3.82795776e-05, 3.82795776e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:55:16,452] [INFO] [timer.py:198:stop] 0/650, RunningAvgSamplesPerSec=411.1834764744244, CurrSamplesPerSec=474.21899522663756, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      650/ 1716613 | consumed samples:       166400 | consumed tokens:    340787200 | elapsed time per iteration (ms): 601.6 | learning rate: 3.828E-05 | global batch size:   256 | lm loss: 5.916811E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 411.27 | backward-compute: 157.39 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 21.16
[2022-10-28 23:55:22,224] [INFO] [logging.py:68:log_dist] [Rank 0] step=660, skipped=0, lr=[3.8869401599999997e-05, 3.8869401599999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:55:22,227] [INFO] [timer.py:198:stop] 0/660, RunningAvgSamplesPerSec=411.3103702879017, CurrSamplesPerSec=470.8485309852485, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      660/ 1716613 | consumed samples:       168960 | consumed tokens:    346030080 | elapsed time per iteration (ms): 577.4 | learning rate: 3.887E-05 | global batch size:   256 | lm loss: 5.895373E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 387.11 | backward-compute: 157.84 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 20.69
[2022-10-28 23:55:28,248] [INFO] [logging.py:68:log_dist] [Rank 0] step=670, skipped=0, lr=[3.94592256e-05, 3.94592256e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:55:28,250] [INFO] [timer.py:198:stop] 0/670, RunningAvgSamplesPerSec=411.2719579963297, CurrSamplesPerSec=381.342614664693, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      670/ 1716613 | consumed samples:       171520 | consumed tokens:    351272960 | elapsed time per iteration (ms): 602.4 | learning rate: 3.946E-05 | global batch size:   256 | lm loss: 5.877367E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 412.75 | backward-compute: 156.04 | backward-embedding-all-reduce: 0.01 | optimizer: 14.04 | batch-generator: 23.00
[2022-10-28 23:55:34,120] [INFO] [logging.py:68:log_dist] [Rank 0] step=680, skipped=0, lr=[4.004904959999999e-05, 4.004904959999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:55:34,123] [INFO] [timer.py:198:stop] 0/680, RunningAvgSamplesPerSec=411.1014831925877, CurrSamplesPerSec=338.7446097001421, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      680/ 1716613 | consumed samples:       174080 | consumed tokens:    356515840 | elapsed time per iteration (ms): 587.2 | learning rate: 4.005E-05 | global batch size:   256 | lm loss: 5.887423E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 397.20 | backward-compute: 156.81 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 22.46
[2022-10-28 23:55:40,120] [INFO] [logging.py:68:log_dist] [Rank 0] step=690, skipped=0, lr=[4.06388736e-05, 4.06388736e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:55:40,123] [INFO] [timer.py:198:stop] 0/690, RunningAvgSamplesPerSec=410.8876544838993, CurrSamplesPerSec=473.22415592474545, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      690/ 1716613 | consumed samples:       176640 | consumed tokens:    361758720 | elapsed time per iteration (ms): 600.1 | learning rate: 4.064E-05 | global batch size:   256 | lm loss: 5.817918E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 410.82 | backward-compute: 155.70 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 22.52
[2022-10-28 23:55:46,130] [INFO] [logging.py:68:log_dist] [Rank 0] step=700, skipped=0, lr=[4.1228697599999995e-05, 4.1228697599999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:55:46,132] [INFO] [timer.py:198:stop] 0/700, RunningAvgSamplesPerSec=410.3080790416223, CurrSamplesPerSec=473.81386728609704, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      700/ 1716613 | consumed samples:       179200 | consumed tokens:    367001600 | elapsed time per iteration (ms): 600.9 | learning rate: 4.123E-05 | global batch size:   256 | lm loss: 5.814724E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 412.95 | backward-compute: 154.71 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 23.16
-----------------------------------------------------------------------------------------------
 validation loss at iteration 700 | lm loss value: 5.782743E+00 | lm loss PPL: 3.246485E+02 | 
-----------------------------------------------------------------------------------------------
[2022-10-28 23:55:56,947] [INFO] [logging.py:68:log_dist] [Rank 0] step=710, skipped=0, lr=[4.18185216e-05, 4.18185216e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:55:56,949] [INFO] [timer.py:198:stop] 0/710, RunningAvgSamplesPerSec=410.1789350588061, CurrSamplesPerSec=324.3588065559191, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      710/ 1716613 | consumed samples:       181760 | consumed tokens:    372244480 | elapsed time per iteration (ms): 1081.7 | learning rate: 4.182E-05 | global batch size:   256 | lm loss: 5.754875E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 892.58 | backward-compute: 154.86 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 40.56
[2022-10-28 23:56:02,938] [INFO] [logging.py:68:log_dist] [Rank 0] step=720, skipped=0, lr=[4.240834559999999e-05, 4.240834559999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:56:02,941] [INFO] [timer.py:198:stop] 0/720, RunningAvgSamplesPerSec=409.96892181731977, CurrSamplesPerSec=389.8074686772112, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      720/ 1716613 | consumed samples:       184320 | consumed tokens:    377487360 | elapsed time per iteration (ms): 599.5 | learning rate: 4.241E-05 | global batch size:   256 | lm loss: 5.781979E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 411.07 | backward-compute: 154.98 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 21.85
[2022-10-28 23:56:08,977] [INFO] [logging.py:68:log_dist] [Rank 0] step=730, skipped=0, lr=[4.2998169599999996e-05, 4.2998169599999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:56:08,980] [INFO] [timer.py:198:stop] 0/730, RunningAvgSamplesPerSec=409.8993738703075, CurrSamplesPerSec=346.9216143423576, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      730/ 1716613 | consumed samples:       186880 | consumed tokens:    382730240 | elapsed time per iteration (ms): 603.4 | learning rate: 4.300E-05 | global batch size:   256 | lm loss: 5.760532E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 414.52 | backward-compute: 154.76 | backward-embedding-all-reduce: 0.01 | optimizer: 14.26 | batch-generator: 23.99
[2022-10-28 23:56:14,877] [INFO] [logging.py:68:log_dist] [Rank 0] step=740, skipped=0, lr=[4.3587993599999994e-05, 4.3587993599999994e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:56:14,880] [INFO] [timer.py:198:stop] 0/740, RunningAvgSamplesPerSec=409.8199452776728, CurrSamplesPerSec=404.96187163016356, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      740/ 1716613 | consumed samples:       189440 | consumed tokens:    387973120 | elapsed time per iteration (ms): 590.2 | learning rate: 4.359E-05 | global batch size:   256 | lm loss: 5.708175E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 401.90 | backward-compute: 154.67 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 22.53
[2022-10-28 23:56:20,608] [INFO] [logging.py:68:log_dist] [Rank 0] step=750, skipped=0, lr=[4.41778176e-05, 4.41778176e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:56:20,610] [INFO] [timer.py:198:stop] 0/750, RunningAvgSamplesPerSec=409.8731616620177, CurrSamplesPerSec=475.19783604653634, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      750/ 1716613 | consumed samples:       192000 | consumed tokens:    393216000 | elapsed time per iteration (ms): 573.0 | learning rate: 4.418E-05 | global batch size:   256 | lm loss: 5.723958E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 384.70 | backward-compute: 154.91 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 23.64
[2022-10-28 23:56:26,602] [INFO] [logging.py:68:log_dist] [Rank 0] step=760, skipped=0, lr=[4.476764159999999e-05, 4.476764159999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:56:26,604] [INFO] [timer.py:198:stop] 0/760, RunningAvgSamplesPerSec=409.5986543579828, CurrSamplesPerSec=423.46790177599553, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      760/ 1716613 | consumed samples:       194560 | consumed tokens:    398458880 | elapsed time per iteration (ms): 599.4 | learning rate: 4.477E-05 | global batch size:   256 | lm loss: 5.701545E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 410.52 | backward-compute: 155.21 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 22.70
[2022-10-28 23:56:32,906] [INFO] [logging.py:68:log_dist] [Rank 0] step=770, skipped=0, lr=[4.5357465599999994e-05, 4.5357465599999994e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:56:32,909] [INFO] [timer.py:198:stop] 0/770, RunningAvgSamplesPerSec=409.5122030679213, CurrSamplesPerSec=314.3235512464725, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      770/ 1716613 | consumed samples:       197120 | consumed tokens:    403701760 | elapsed time per iteration (ms): 630.5 | learning rate: 4.536E-05 | global batch size:   256 | lm loss: 5.667970E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 440.23 | backward-compute: 157.75 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 24.86
[2022-10-28 23:56:38,838] [INFO] [logging.py:68:log_dist] [Rank 0] step=780, skipped=0, lr=[4.59472896e-05, 4.59472896e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:56:38,841] [INFO] [timer.py:198:stop] 0/780, RunningAvgSamplesPerSec=409.3142852351703, CurrSamplesPerSec=464.16262220700577, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      780/ 1716613 | consumed samples:       199680 | consumed tokens:    408944640 | elapsed time per iteration (ms): 593.2 | learning rate: 4.595E-05 | global batch size:   256 | lm loss: 5.663579E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 403.66 | backward-compute: 156.85 | backward-embedding-all-reduce: 0.01 | optimizer: 14.24 | batch-generator: 21.52
[2022-10-28 23:56:44,793] [INFO] [logging.py:68:log_dist] [Rank 0] step=790, skipped=0, lr=[4.65371136e-05, 4.65371136e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:56:44,795] [INFO] [timer.py:198:stop] 0/790, RunningAvgSamplesPerSec=409.0764789930499, CurrSamplesPerSec=379.27683551958586, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      790/ 1716613 | consumed samples:       202240 | consumed tokens:    414187520 | elapsed time per iteration (ms): 595.5 | learning rate: 4.654E-05 | global batch size:   256 | lm loss: 5.677182E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 405.94 | backward-compute: 156.78 | backward-embedding-all-reduce: 0.01 | optimizer: 14.02 | batch-generator: 21.67
[2022-10-28 23:56:51,095] [INFO] [logging.py:68:log_dist] [Rank 0] step=800, skipped=0, lr=[4.7126937599999995e-05, 4.7126937599999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:56:51,098] [INFO] [timer.py:198:stop] 0/800, RunningAvgSamplesPerSec=408.6210857402189, CurrSamplesPerSec=427.3946318427447, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      800/ 1716613 | consumed samples:       204800 | consumed tokens:    419430400 | elapsed time per iteration (ms): 630.2 | learning rate: 4.713E-05 | global batch size:   256 | lm loss: 5.602820E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 439.37 | backward-compute: 157.98 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 21.38
-----------------------------------------------------------------------------------------------
 validation loss at iteration 800 | lm loss value: 5.657986E+00 | lm loss PPL: 2.865708E+02 | 
-----------------------------------------------------------------------------------------------
[2022-10-28 23:57:01,590] [INFO] [logging.py:68:log_dist] [Rank 0] step=810, skipped=0, lr=[4.771676159999999e-05, 4.771676159999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:57:01,593] [INFO] [timer.py:198:stop] 0/810, RunningAvgSamplesPerSec=408.2187331213662, CurrSamplesPerSec=329.5813690799215, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      810/ 1716613 | consumed samples:       207360 | consumed tokens:    424673280 | elapsed time per iteration (ms): 1049.6 | learning rate: 4.772E-05 | global batch size:   256 | lm loss: 5.560336E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 858.61 | backward-compute: 157.57 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 39.08
[2022-10-28 23:57:07,807] [INFO] [logging.py:68:log_dist] [Rank 0] step=820, skipped=0, lr=[4.83065856e-05, 4.83065856e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:57:07,810] [INFO] [timer.py:198:stop] 0/820, RunningAvgSamplesPerSec=408.3297021528975, CurrSamplesPerSec=393.51382540496957, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      820/ 1716613 | consumed samples:       209920 | consumed tokens:    429916160 | elapsed time per iteration (ms): 621.7 | learning rate: 4.831E-05 | global batch size:   256 | lm loss: 5.613160E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 431.89 | backward-compute: 157.47 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 20.91
[2022-10-28 23:57:13,912] [INFO] [logging.py:68:log_dist] [Rank 0] step=830, skipped=0, lr=[4.8896409599999996e-05, 4.8896409599999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:57:13,915] [INFO] [timer.py:198:stop] 0/830, RunningAvgSamplesPerSec=407.91272944652655, CurrSamplesPerSec=469.1238050632115, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      830/ 1716613 | consumed samples:       212480 | consumed tokens:    435159040 | elapsed time per iteration (ms): 610.5 | learning rate: 4.890E-05 | global batch size:   256 | lm loss: 5.597042E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 420.30 | backward-compute: 157.69 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 21.10
[2022-10-28 23:57:20,081] [INFO] [logging.py:68:log_dist] [Rank 0] step=840, skipped=0, lr=[4.9486233599999994e-05, 4.9486233599999994e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:57:20,084] [INFO] [timer.py:198:stop] 0/840, RunningAvgSamplesPerSec=407.7941248211734, CurrSamplesPerSec=460.21083238468816, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      840/ 1716613 | consumed samples:       215040 | consumed tokens:    440401920 | elapsed time per iteration (ms): 616.9 | learning rate: 4.949E-05 | global batch size:   256 | lm loss: 5.541208E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 426.85 | backward-compute: 157.82 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 21.92
[2022-10-28 23:57:26,335] [INFO] [logging.py:68:log_dist] [Rank 0] step=850, skipped=0, lr=[5.007605759999999e-05, 5.007605759999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:57:26,338] [INFO] [timer.py:198:stop] 0/850, RunningAvgSamplesPerSec=407.692795802911, CurrSamplesPerSec=344.49870252539125, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      850/ 1716613 | consumed samples:       217600 | consumed tokens:    445644800 | elapsed time per iteration (ms): 625.5 | learning rate: 5.008E-05 | global batch size:   256 | lm loss: 5.560807E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 435.15 | backward-compute: 157.63 | backward-embedding-all-reduce: 0.01 | optimizer: 13.96 | batch-generator: 21.29
[2022-10-28 23:57:32,362] [INFO] [logging.py:68:log_dist] [Rank 0] step=860, skipped=0, lr=[5.0665881599999996e-05, 5.0665881599999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:57:32,364] [INFO] [timer.py:198:stop] 0/860, RunningAvgSamplesPerSec=407.12056781305984, CurrSamplesPerSec=385.3376513011323, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      860/ 1716613 | consumed samples:       220160 | consumed tokens:    450887680 | elapsed time per iteration (ms): 602.6 | learning rate: 5.067E-05 | global batch size:   256 | lm loss: 5.543103E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 410.48 | backward-compute: 159.35 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 20.93
[2022-10-28 23:57:38,480] [INFO] [logging.py:68:log_dist] [Rank 0] step=870, skipped=0, lr=[5.1255705599999994e-05, 5.1255705599999994e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:57:38,483] [INFO] [timer.py:198:stop] 0/870, RunningAvgSamplesPerSec=407.182583609953, CurrSamplesPerSec=465.7895617228467, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      870/ 1716613 | consumed samples:       222720 | consumed tokens:    456130560 | elapsed time per iteration (ms): 611.9 | learning rate: 5.126E-05 | global batch size:   256 | lm loss: 5.518352E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 420.28 | backward-compute: 159.03 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 21.11
[2022-10-28 23:57:44,825] [INFO] [logging.py:68:log_dist] [Rank 0] step=880, skipped=0, lr=[5.18455296e-05, 5.18455296e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:57:44,828] [INFO] [timer.py:198:stop] 0/880, RunningAvgSamplesPerSec=407.3619062564994, CurrSamplesPerSec=469.4240626748741, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      880/ 1716613 | consumed samples:       225280 | consumed tokens:    461373440 | elapsed time per iteration (ms): 634.5 | learning rate: 5.185E-05 | global batch size:   256 | lm loss: 5.544656E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 443.70 | backward-compute: 158.62 | backward-embedding-all-reduce: 0.01 | optimizer: 13.92 | batch-generator: 20.21
[2022-10-28 23:57:50,794] [INFO] [logging.py:68:log_dist] [Rank 0] step=890, skipped=0, lr=[5.243535359999999e-05, 5.243535359999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:57:50,796] [INFO] [timer.py:198:stop] 0/890, RunningAvgSamplesPerSec=406.8931508449851, CurrSamplesPerSec=467.85320691578363, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      890/ 1716613 | consumed samples:       227840 | consumed tokens:    466616320 | elapsed time per iteration (ms): 596.9 | learning rate: 5.244E-05 | global batch size:   256 | lm loss: 5.486741E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 404.50 | backward-compute: 158.60 | backward-embedding-all-reduce: 0.01 | optimizer: 13.93 | batch-generator: 21.56
[2022-10-28 23:57:57,106] [INFO] [logging.py:68:log_dist] [Rank 0] step=900, skipped=0, lr=[5.3025177599999995e-05, 5.3025177599999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:57:57,109] [INFO] [timer.py:198:stop] 0/900, RunningAvgSamplesPerSec=405.9637820242942, CurrSamplesPerSec=468.3086520982132, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      900/ 1716613 | consumed samples:       230400 | consumed tokens:    471859200 | elapsed time per iteration (ms): 631.3 | learning rate: 5.303E-05 | global batch size:   256 | lm loss: 5.484399E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 439.34 | backward-compute: 159.31 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 20.37
-----------------------------------------------------------------------------------------------
 validation loss at iteration 900 | lm loss value: 5.488704E+00 | lm loss PPL: 2.419435E+02 | 
-----------------------------------------------------------------------------------------------
[2022-10-28 23:58:07,908] [INFO] [logging.py:68:log_dist] [Rank 0] step=910, skipped=0, lr=[5.36150016e-05, 5.36150016e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:58:07,910] [INFO] [timer.py:198:stop] 0/910, RunningAvgSamplesPerSec=405.7023422725524, CurrSamplesPerSec=467.43445811581233, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      910/ 1716613 | consumed samples:       232960 | consumed tokens:    477102080 | elapsed time per iteration (ms): 1080.1 | learning rate: 5.362E-05 | global batch size:   256 | lm loss: 5.441276E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 887.34 | backward-compute: 159.17 | backward-embedding-all-reduce: 0.01 | optimizer: 14.31 | batch-generator: 37.26
[2022-10-28 23:58:14,158] [INFO] [logging.py:68:log_dist] [Rank 0] step=920, skipped=0, lr=[5.42048256e-05, 5.42048256e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:58:14,160] [INFO] [timer.py:198:stop] 0/920, RunningAvgSamplesPerSec=405.4573482206784, CurrSamplesPerSec=464.59321617346217, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      920/ 1716613 | consumed samples:       235520 | consumed tokens:    482344960 | elapsed time per iteration (ms): 625.0 | learning rate: 5.420E-05 | global batch size:   256 | lm loss: 5.465002E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 435.53 | backward-compute: 157.33 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 22.25
[2022-10-28 23:58:20,263] [INFO] [logging.py:68:log_dist] [Rank 0] step=930, skipped=0, lr=[5.4794649599999996e-05, 5.4794649599999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:58:20,266] [INFO] [timer.py:198:stop] 0/930, RunningAvgSamplesPerSec=405.09699711413145, CurrSamplesPerSec=469.5998012686617, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      930/ 1716613 | consumed samples:       238080 | consumed tokens:    487587840 | elapsed time per iteration (ms): 610.6 | learning rate: 5.479E-05 | global batch size:   256 | lm loss: 5.437845E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 419.03 | backward-compute: 158.69 | backward-embedding-all-reduce: 0.01 | optimizer: 14.25 | batch-generator: 21.55
[2022-10-28 23:58:26,283] [INFO] [logging.py:68:log_dist] [Rank 0] step=940, skipped=0, lr=[5.5384473599999994e-05, 5.5384473599999994e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:58:26,286] [INFO] [timer.py:198:stop] 0/940, RunningAvgSamplesPerSec=405.0644289040473, CurrSamplesPerSec=438.72468922324606, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      940/ 1716613 | consumed samples:       240640 | consumed tokens:    492830720 | elapsed time per iteration (ms): 602.0 | learning rate: 5.538E-05 | global batch size:   256 | lm loss: 5.413171E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 410.91 | backward-compute: 158.36 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 21.94
[2022-10-28 23:58:32,255] [INFO] [logging.py:68:log_dist] [Rank 0] step=950, skipped=0, lr=[5.59742976e-05, 5.59742976e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:58:32,258] [INFO] [timer.py:198:stop] 0/950, RunningAvgSamplesPerSec=405.2853907709302, CurrSamplesPerSec=468.6667737497468, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      950/ 1716613 | consumed samples:       243200 | consumed tokens:    498073600 | elapsed time per iteration (ms): 597.2 | learning rate: 5.597E-05 | global batch size:   256 | lm loss: 5.462171E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 406.48 | backward-compute: 158.06 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 22.63
[2022-10-28 23:58:38,442] [INFO] [logging.py:68:log_dist] [Rank 0] step=960, skipped=0, lr=[5.6564121599999996e-05, 5.6564121599999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:58:38,445] [INFO] [timer.py:198:stop] 0/960, RunningAvgSamplesPerSec=404.8746071402975, CurrSamplesPerSec=469.01069287002224, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      960/ 1716613 | consumed samples:       245760 | consumed tokens:    503316480 | elapsed time per iteration (ms): 618.6 | learning rate: 5.656E-05 | global batch size:   256 | lm loss: 5.425368E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 427.93 | backward-compute: 157.99 | backward-embedding-all-reduce: 0.01 | optimizer: 14.16 | batch-generator: 21.64
[2022-10-28 23:58:44,552] [INFO] [logging.py:68:log_dist] [Rank 0] step=970, skipped=0, lr=[5.7153945599999994e-05, 5.7153945599999994e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:58:44,555] [INFO] [timer.py:198:stop] 0/970, RunningAvgSamplesPerSec=405.0146718341559, CurrSamplesPerSec=469.0992108151182, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      970/ 1716613 | consumed samples:       248320 | consumed tokens:    508559360 | elapsed time per iteration (ms): 611.0 | learning rate: 5.715E-05 | global batch size:   256 | lm loss: 5.432969E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 422.94 | backward-compute: 156.83 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 21.88
[2022-10-28 23:58:50,227] [INFO] [logging.py:68:log_dist] [Rank 0] step=980, skipped=0, lr=[5.774376959999999e-05, 5.774376959999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:58:50,230] [INFO] [timer.py:198:stop] 0/980, RunningAvgSamplesPerSec=404.98222559892855, CurrSamplesPerSec=464.3537202205908, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      980/ 1716613 | consumed samples:       250880 | consumed tokens:    513802240 | elapsed time per iteration (ms): 567.4 | learning rate: 5.774E-05 | global batch size:   256 | lm loss: 5.390070E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 378.39 | backward-compute: 156.74 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 22.26
[2022-10-28 23:58:56,694] [INFO] [logging.py:68:log_dist] [Rank 0] step=990, skipped=0, lr=[5.83335936e-05, 5.83335936e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:58:56,697] [INFO] [timer.py:198:stop] 0/990, RunningAvgSamplesPerSec=404.4591839085416, CurrSamplesPerSec=447.95984246712504, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration      990/ 1716613 | consumed samples:       253440 | consumed tokens:    519045120 | elapsed time per iteration (ms): 646.8 | learning rate: 5.833E-05 | global batch size:   256 | lm loss: 5.355200E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 456.64 | backward-compute: 157.18 | backward-embedding-all-reduce: 0.01 | optimizer: 14.25 | batch-generator: 21.18
[2022-10-28 23:59:03,132] [INFO] [logging.py:68:log_dist] [Rank 0] step=1000, skipped=0, lr=[5.8923417599999995e-05, 5.8923417599999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:59:03,135] [INFO] [timer.py:198:stop] 0/1000, RunningAvgSamplesPerSec=404.54899015234975, CurrSamplesPerSec=467.4800441644114, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1000/ 1716613 | consumed samples:       256000 | consumed tokens:    524288000 | elapsed time per iteration (ms): 644.1 | learning rate: 5.892E-05 | global batch size:   256 | lm loss: 5.373733E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 453.96 | backward-compute: 157.15 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 21.48
------------------------------------------------------------------------------------------------
 validation loss at iteration 1000 | lm loss value: 5.395442E+00 | lm loss PPL: 2.203996E+02 | 
------------------------------------------------------------------------------------------------
[2022-10-28 23:59:08,660] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:59:08,660] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2022-10-28 23:59:08,660] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:59:08,660] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:59:08,660] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:59:08,665] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2022-10-28 23:59:08,665] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2022-10-28 23:59:08,660] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:59:08,665] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2022-10-28 23:59:08,660] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:59:08,665] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2022-10-28 23:59:08,660] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:59:08,665] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2022-10-28 23:59:08,661] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-28 23:59:08,665] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2022-10-28 23:59:08,665] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2022-10-28 23:59:14,139] [INFO] [logging.py:68:log_dist] [Rank 0] step=1010, skipped=0, lr=[5.9513241599999986e-05, 5.9513241599999986e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:59:14,141] [INFO] [timer.py:198:stop] 0/1010, RunningAvgSamplesPerSec=403.9590827246782, CurrSamplesPerSec=411.3940738878961, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1010/ 1716613 | consumed samples:       258560 | consumed tokens:    529530880 | elapsed time per iteration (ms): 1100.3 | learning rate: 5.951E-05 | global batch size:   256 | lm loss: 5.369384E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 910.70 | backward-compute: 155.61 | backward-embedding-all-reduce: 0.01 | optimizer: 14.97 | batch-generator: 37.18
[2022-10-28 23:59:19,935] [INFO] [logging.py:68:log_dist] [Rank 0] step=1020, skipped=0, lr=[6.01030656e-05, 6.01030656e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:59:19,937] [INFO] [timer.py:198:stop] 0/1020, RunningAvgSamplesPerSec=403.8541438718423, CurrSamplesPerSec=466.9888348045134, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1020/ 1716613 | consumed samples:       261120 | consumed tokens:    534773760 | elapsed time per iteration (ms): 579.6 | learning rate: 6.010E-05 | global batch size:   256 | lm loss: 5.375521E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 388.47 | backward-compute: 158.20 | backward-embedding-all-reduce: 0.01 | optimizer: 14.02 | batch-generator: 21.84
[2022-10-28 23:59:25,302] [INFO] [logging.py:68:log_dist] [Rank 0] step=1030, skipped=0, lr=[6.0692889599999996e-05, 6.0692889599999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:59:25,305] [INFO] [timer.py:198:stop] 0/1030, RunningAvgSamplesPerSec=404.08009892466015, CurrSamplesPerSec=444.0164218062002, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1030/ 1716613 | consumed samples:       263680 | consumed tokens:    540016640 | elapsed time per iteration (ms): 536.7 | learning rate: 6.069E-05 | global batch size:   256 | lm loss: 5.331916E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 346.25 | backward-compute: 157.54 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 21.49
[2022-10-28 23:59:31,212] [INFO] [logging.py:68:log_dist] [Rank 0] step=1040, skipped=0, lr=[6.128271359999999e-05, 6.128271359999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:59:31,214] [INFO] [timer.py:198:stop] 0/1040, RunningAvgSamplesPerSec=404.1972748316675, CurrSamplesPerSec=468.49500326367337, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1040/ 1716613 | consumed samples:       266240 | consumed tokens:    545259520 | elapsed time per iteration (ms): 591.0 | learning rate: 6.128E-05 | global batch size:   256 | lm loss: 5.302897E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 400.78 | backward-compute: 157.12 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 21.94
[2022-10-28 23:59:37,045] [INFO] [logging.py:68:log_dist] [Rank 0] step=1050, skipped=0, lr=[6.18725376e-05, 6.18725376e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:59:37,048] [INFO] [timer.py:198:stop] 0/1050, RunningAvgSamplesPerSec=404.23813267372344, CurrSamplesPerSec=390.8563574203393, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1050/ 1716613 | consumed samples:       268800 | consumed tokens:    550502400 | elapsed time per iteration (ms): 583.4 | learning rate: 6.187E-05 | global batch size:   256 | lm loss: 5.297782E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 394.87 | backward-compute: 155.47 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 20.99
[2022-10-28 23:59:43,025] [INFO] [logging.py:68:log_dist] [Rank 0] step=1060, skipped=0, lr=[6.24623616e-05, 6.24623616e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:59:43,028] [INFO] [timer.py:198:stop] 0/1060, RunningAvgSamplesPerSec=403.21800515024086, CurrSamplesPerSec=465.4083852600845, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1060/ 1716613 | consumed samples:       271360 | consumed tokens:    555745280 | elapsed time per iteration (ms): 597.9 | learning rate: 6.246E-05 | global batch size:   256 | lm loss: 5.294712E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 410.10 | backward-compute: 154.66 | backward-embedding-all-reduce: 0.01 | optimizer: 14.05 | batch-generator: 22.66
[2022-10-28 23:59:49,287] [INFO] [logging.py:68:log_dist] [Rank 0] step=1070, skipped=0, lr=[6.305218559999999e-05, 6.305218559999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:59:49,290] [INFO] [timer.py:198:stop] 0/1070, RunningAvgSamplesPerSec=403.10990979245724, CurrSamplesPerSec=376.81921243395306, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1070/ 1716613 | consumed samples:       273920 | consumed tokens:    560988160 | elapsed time per iteration (ms): 626.2 | learning rate: 6.305E-05 | global batch size:   256 | lm loss: 5.281139E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 436.84 | backward-compute: 156.08 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 21.80
[2022-10-28 23:59:55,034] [INFO] [logging.py:68:log_dist] [Rank 0] step=1080, skipped=0, lr=[6.364200959999999e-05, 6.364200959999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-28 23:59:55,037] [INFO] [timer.py:198:stop] 0/1080, RunningAvgSamplesPerSec=403.3089804897403, CurrSamplesPerSec=468.0098192367775, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1080/ 1716613 | consumed samples:       276480 | consumed tokens:    566231040 | elapsed time per iteration (ms): 574.7 | learning rate: 6.364E-05 | global batch size:   256 | lm loss: 5.310715E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 385.92 | backward-compute: 155.48 | backward-embedding-all-reduce: 0.01 | optimizer: 14.28 | batch-generator: 21.88
[2022-10-29 00:00:00,989] [INFO] [logging.py:68:log_dist] [Rank 0] step=1090, skipped=0, lr=[6.42318336e-05, 6.42318336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:00:00,992] [INFO] [timer.py:198:stop] 0/1090, RunningAvgSamplesPerSec=403.4628055436206, CurrSamplesPerSec=472.06074781146793, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1090/ 1716613 | consumed samples:       279040 | consumed tokens:    571473920 | elapsed time per iteration (ms): 595.5 | learning rate: 6.423E-05 | global batch size:   256 | lm loss: 5.293048E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 406.14 | backward-compute: 155.77 | backward-embedding-all-reduce: 0.01 | optimizer: 14.01 | batch-generator: 20.53
[2022-10-29 00:00:06,928] [INFO] [logging.py:68:log_dist] [Rank 0] step=1100, skipped=0, lr=[6.48216576e-05, 6.48216576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:00:06,931] [INFO] [timer.py:198:stop] 0/1100, RunningAvgSamplesPerSec=403.21528373242063, CurrSamplesPerSec=467.77494005464786, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1100/ 1716613 | consumed samples:       281600 | consumed tokens:    576716800 | elapsed time per iteration (ms): 593.9 | learning rate: 6.482E-05 | global batch size:   256 | lm loss: 5.216929E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 404.15 | backward-compute: 156.88 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 20.70
------------------------------------------------------------------------------------------------
 validation loss at iteration 1100 | lm loss value: 5.242290E+00 | lm loss PPL: 1.891026E+02 | 
------------------------------------------------------------------------------------------------
[2022-10-29 00:00:17,807] [INFO] [logging.py:68:log_dist] [Rank 0] step=1110, skipped=0, lr=[6.541148159999999e-05, 6.541148159999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:00:17,810] [INFO] [timer.py:198:stop] 0/1110, RunningAvgSamplesPerSec=403.27103854264806, CurrSamplesPerSec=468.4737452006981, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1110/ 1716613 | consumed samples:       284160 | consumed tokens:    581959680 | elapsed time per iteration (ms): 1087.9 | learning rate: 6.541E-05 | global batch size:   256 | lm loss: 5.276583E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 898.71 | backward-compute: 155.32 | backward-embedding-all-reduce: 0.01 | optimizer: 13.96 | batch-generator: 36.75
[2022-10-29 00:00:23,937] [INFO] [logging.py:68:log_dist] [Rank 0] step=1120, skipped=0, lr=[6.600130559999998e-05, 6.600130559999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:00:23,940] [INFO] [timer.py:198:stop] 0/1120, RunningAvgSamplesPerSec=403.1285890739012, CurrSamplesPerSec=470.5744948653851, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1120/ 1716613 | consumed samples:       286720 | consumed tokens:    587202560 | elapsed time per iteration (ms): 613.0 | learning rate: 6.600E-05 | global batch size:   256 | lm loss: 5.254648E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 424.14 | backward-compute: 156.18 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 21.16
[2022-10-29 00:00:29,525] [INFO] [logging.py:68:log_dist] [Rank 0] step=1130, skipped=0, lr=[6.65911296e-05, 6.65911296e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:00:29,527] [INFO] [timer.py:198:stop] 0/1130, RunningAvgSamplesPerSec=403.19543682474114, CurrSamplesPerSec=297.11935890908285, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1130/ 1716613 | consumed samples:       289280 | consumed tokens:    592445440 | elapsed time per iteration (ms): 558.7 | learning rate: 6.659E-05 | global batch size:   256 | lm loss: 5.233228E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 368.05 | backward-compute: 157.28 | backward-embedding-all-reduce: 0.01 | optimizer: 14.05 | batch-generator: 21.08
[2022-10-29 00:00:35,435] [INFO] [logging.py:68:log_dist] [Rank 0] step=1140, skipped=0, lr=[6.71809536e-05, 6.71809536e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:00:35,437] [INFO] [timer.py:198:stop] 0/1140, RunningAvgSamplesPerSec=403.08883483263344, CurrSamplesPerSec=459.80249603464165, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1140/ 1716613 | consumed samples:       291840 | consumed tokens:    597688320 | elapsed time per iteration (ms): 591.0 | learning rate: 6.718E-05 | global batch size:   256 | lm loss: 5.225908E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 400.75 | backward-compute: 157.09 | backward-embedding-all-reduce: 0.01 | optimizer: 13.96 | batch-generator: 21.58
[2022-10-29 00:00:41,610] [INFO] [logging.py:68:log_dist] [Rank 0] step=1150, skipped=0, lr=[6.777077759999998e-05, 6.777077759999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:00:41,613] [INFO] [timer.py:198:stop] 0/1150, RunningAvgSamplesPerSec=402.6093314970211, CurrSamplesPerSec=395.66106171733134, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1150/ 1716613 | consumed samples:       294400 | consumed tokens:    602931200 | elapsed time per iteration (ms): 617.6 | learning rate: 6.777E-05 | global batch size:   256 | lm loss: 5.241798E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 427.97 | backward-compute: 156.53 | backward-embedding-all-reduce: 0.01 | optimizer: 14.06 | batch-generator: 20.58
[2022-10-29 00:00:47,538] [INFO] [logging.py:68:log_dist] [Rank 0] step=1160, skipped=0, lr=[6.836060159999999e-05, 6.836060159999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:00:47,541] [INFO] [timer.py:198:stop] 0/1160, RunningAvgSamplesPerSec=402.84324982853764, CurrSamplesPerSec=475.832962856606, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1160/ 1716613 | consumed samples:       296960 | consumed tokens:    608174080 | elapsed time per iteration (ms): 592.8 | learning rate: 6.836E-05 | global batch size:   256 | lm loss: 5.219453E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 402.76 | backward-compute: 157.06 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 20.72
[2022-10-29 00:00:53,689] [INFO] [logging.py:68:log_dist] [Rank 0] step=1170, skipped=0, lr=[6.89504256e-05, 6.89504256e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:00:53,692] [INFO] [timer.py:198:stop] 0/1170, RunningAvgSamplesPerSec=402.6983100002606, CurrSamplesPerSec=468.5080861075332, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1170/ 1716613 | consumed samples:       299520 | consumed tokens:    613416960 | elapsed time per iteration (ms): 615.0 | learning rate: 6.895E-05 | global batch size:   256 | lm loss: 5.159483E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 425.72 | backward-compute: 156.67 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 20.90
[2022-10-29 00:00:59,743] [INFO] [logging.py:68:log_dist] [Rank 0] step=1180, skipped=0, lr=[6.95402496e-05, 6.95402496e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:00:59,746] [INFO] [timer.py:198:stop] 0/1180, RunningAvgSamplesPerSec=402.33248812079734, CurrSamplesPerSec=398.4566386814074, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1180/ 1716613 | consumed samples:       302080 | consumed tokens:    618659840 | elapsed time per iteration (ms): 605.4 | learning rate: 6.954E-05 | global batch size:   256 | lm loss: 5.124444E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 414.00 | backward-compute: 158.14 | backward-embedding-all-reduce: 0.01 | optimizer: 14.01 | batch-generator: 22.02
[2022-10-29 00:01:05,788] [INFO] [logging.py:68:log_dist] [Rank 0] step=1190, skipped=0, lr=[7.013007359999999e-05, 7.013007359999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:01:05,790] [INFO] [timer.py:198:stop] 0/1190, RunningAvgSamplesPerSec=402.18671241347306, CurrSamplesPerSec=403.7194069537649, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1190/ 1716613 | consumed samples:       304640 | consumed tokens:    623902720 | elapsed time per iteration (ms): 604.3 | learning rate: 7.013E-05 | global batch size:   256 | lm loss: 5.134994E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 413.68 | backward-compute: 158.03 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 21.12
[2022-10-29 00:01:12,091] [INFO] [logging.py:68:log_dist] [Rank 0] step=1200, skipped=0, lr=[7.07198976e-05, 7.07198976e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:01:12,093] [INFO] [timer.py:198:stop] 0/1200, RunningAvgSamplesPerSec=402.0756865175507, CurrSamplesPerSec=325.8637234360243, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1200/ 1716613 | consumed samples:       307200 | consumed tokens:    629145600 | elapsed time per iteration (ms): 630.4 | learning rate: 7.072E-05 | global batch size:   256 | lm loss: 5.161723E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 439.86 | backward-compute: 157.95 | backward-embedding-all-reduce: 0.01 | optimizer: 13.92 | batch-generator: 20.80
------------------------------------------------------------------------------------------------
 validation loss at iteration 1200 | lm loss value: 5.155525E+00 | lm loss PPL: 1.733868E+02 | 
------------------------------------------------------------------------------------------------
[2022-10-29 00:01:23,025] [INFO] [logging.py:68:log_dist] [Rank 0] step=1210, skipped=0, lr=[7.13097216e-05, 7.13097216e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:01:23,028] [INFO] [timer.py:198:stop] 0/1210, RunningAvgSamplesPerSec=401.7731050023967, CurrSamplesPerSec=414.986157618257, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1210/ 1716613 | consumed samples:       309760 | consumed tokens:    634388480 | elapsed time per iteration (ms): 1093.5 | learning rate: 7.131E-05 | global batch size:   256 | lm loss: 5.142014E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 901.26 | backward-compute: 158.27 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 36.19
[2022-10-29 00:01:29,442] [INFO] [logging.py:68:log_dist] [Rank 0] step=1220, skipped=0, lr=[7.189954559999999e-05, 7.189954559999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:01:29,444] [INFO] [timer.py:198:stop] 0/1220, RunningAvgSamplesPerSec=401.9431790943491, CurrSamplesPerSec=470.68341077659517, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1220/ 1716613 | consumed samples:       312320 | consumed tokens:    639631360 | elapsed time per iteration (ms): 641.6 | learning rate: 7.190E-05 | global batch size:   256 | lm loss: 5.158358E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 450.11 | backward-compute: 158.28 | backward-embedding-all-reduce: 0.01 | optimizer: 14.33 | batch-generator: 21.70
[2022-10-29 00:01:35,527] [INFO] [logging.py:68:log_dist] [Rank 0] step=1230, skipped=0, lr=[7.24893696e-05, 7.24893696e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:01:35,530] [INFO] [timer.py:198:stop] 0/1230, RunningAvgSamplesPerSec=401.74411623602964, CurrSamplesPerSec=278.31335354398306, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1230/ 1716613 | consumed samples:       314880 | consumed tokens:    644874240 | elapsed time per iteration (ms): 608.6 | learning rate: 7.249E-05 | global batch size:   256 | lm loss: 5.155268E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 418.83 | backward-compute: 156.89 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 21.49
[2022-10-29 00:01:42,138] [INFO] [logging.py:68:log_dist] [Rank 0] step=1240, skipped=0, lr=[7.30791936e-05, 7.30791936e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:01:42,141] [INFO] [timer.py:198:stop] 0/1240, RunningAvgSamplesPerSec=401.29350759375444, CurrSamplesPerSec=271.85154248283936, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1240/ 1716613 | consumed samples:       317440 | consumed tokens:    650117120 | elapsed time per iteration (ms): 661.0 | learning rate: 7.308E-05 | global batch size:   256 | lm loss: 5.132719E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 471.78 | backward-compute: 156.82 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 21.30
[2022-10-29 00:01:48,738] [INFO] [logging.py:68:log_dist] [Rank 0] step=1250, skipped=0, lr=[7.366901759999999e-05, 7.366901759999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:01:48,741] [INFO] [timer.py:198:stop] 0/1250, RunningAvgSamplesPerSec=400.7573138611176, CurrSamplesPerSec=470.4788224860575, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1250/ 1716613 | consumed samples:       320000 | consumed tokens:    655360000 | elapsed time per iteration (ms): 660.1 | learning rate: 7.367E-05 | global batch size:   256 | lm loss: 5.087719E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 470.43 | backward-compute: 156.67 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 21.32
[2022-10-29 00:01:54,727] [INFO] [logging.py:68:log_dist] [Rank 0] step=1260, skipped=0, lr=[7.42588416e-05, 7.42588416e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:01:54,729] [INFO] [timer.py:198:stop] 0/1260, RunningAvgSamplesPerSec=401.0265317660315, CurrSamplesPerSec=450.83379127338685, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1260/ 1716613 | consumed samples:       322560 | consumed tokens:    660602880 | elapsed time per iteration (ms): 598.9 | learning rate: 7.426E-05 | global batch size:   256 | lm loss: 5.103029E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 408.69 | backward-compute: 156.89 | backward-embedding-all-reduce: 0.01 | optimizer: 13.96 | batch-generator: 22.87
[2022-10-29 00:02:00,966] [INFO] [logging.py:68:log_dist] [Rank 0] step=1270, skipped=0, lr=[7.484866559999999e-05, 7.484866559999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:02:00,968] [INFO] [timer.py:198:stop] 0/1270, RunningAvgSamplesPerSec=401.1220955833385, CurrSamplesPerSec=471.00715541534043, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1270/ 1716613 | consumed samples:       325120 | consumed tokens:    665845760 | elapsed time per iteration (ms): 623.9 | learning rate: 7.485E-05 | global batch size:   256 | lm loss: 5.120033E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 434.13 | backward-compute: 156.77 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 21.27
[2022-10-29 00:02:07,132] [INFO] [logging.py:68:log_dist] [Rank 0] step=1280, skipped=0, lr=[7.543848959999999e-05, 7.543848959999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:02:07,134] [INFO] [timer.py:198:stop] 0/1280, RunningAvgSamplesPerSec=401.3328629434388, CurrSamplesPerSec=467.4539939050936, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1280/ 1716613 | consumed samples:       327680 | consumed tokens:    671088640 | elapsed time per iteration (ms): 616.5 | learning rate: 7.544E-05 | global batch size:   256 | lm loss: 5.087354E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 426.95 | backward-compute: 156.64 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 22.33
[2022-10-29 00:02:13,026] [INFO] [logging.py:68:log_dist] [Rank 0] step=1290, skipped=0, lr=[7.60283136e-05, 7.60283136e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:02:13,028] [INFO] [timer.py:198:stop] 0/1290, RunningAvgSamplesPerSec=401.2040446470434, CurrSamplesPerSec=390.60724710648316, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1290/ 1716613 | consumed samples:       330240 | consumed tokens:    676331520 | elapsed time per iteration (ms): 589.3 | learning rate: 7.603E-05 | global batch size:   256 | lm loss: 5.106075E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 400.45 | backward-compute: 156.47 | backward-embedding-all-reduce: 0.01 | optimizer: 14.02 | batch-generator: 21.98
[2022-10-29 00:02:19,436] [INFO] [logging.py:68:log_dist] [Rank 0] step=1300, skipped=0, lr=[7.661813759999999e-05, 7.661813759999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:02:19,438] [INFO] [timer.py:198:stop] 0/1300, RunningAvgSamplesPerSec=400.8936245105835, CurrSamplesPerSec=400.1947874208736, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1300/ 1716613 | consumed samples:       332800 | consumed tokens:    681574400 | elapsed time per iteration (ms): 641.0 | learning rate: 7.662E-05 | global batch size:   256 | lm loss: 5.070874E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 451.69 | backward-compute: 157.35 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 22.01
------------------------------------------------------------------------------------------------
 validation loss at iteration 1300 | lm loss value: 5.053892E+00 | lm loss PPL: 1.566308E+02 | 
------------------------------------------------------------------------------------------------
[2022-10-29 00:02:29,719] [INFO] [logging.py:68:log_dist] [Rank 0] step=1310, skipped=0, lr=[7.720796159999999e-05, 7.720796159999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:02:29,722] [INFO] [timer.py:198:stop] 0/1310, RunningAvgSamplesPerSec=400.81054668277966, CurrSamplesPerSec=431.2590264858317, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1310/ 1716613 | consumed samples:       335360 | consumed tokens:    686817280 | elapsed time per iteration (ms): 1028.3 | learning rate: 7.721E-05 | global batch size:   256 | lm loss: 5.033627E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 837.80 | backward-compute: 158.30 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 37.50
[2022-10-29 00:02:35,916] [INFO] [logging.py:68:log_dist] [Rank 0] step=1320, skipped=0, lr=[7.77977856e-05, 7.77977856e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:02:35,919] [INFO] [timer.py:198:stop] 0/1320, RunningAvgSamplesPerSec=400.4180250642778, CurrSamplesPerSec=354.9137372081339, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1320/ 1716613 | consumed samples:       337920 | consumed tokens:    692060160 | elapsed time per iteration (ms): 619.7 | learning rate: 7.780E-05 | global batch size:   256 | lm loss: 5.091842E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 429.49 | backward-compute: 158.54 | backward-embedding-all-reduce: 0.01 | optimizer: 13.96 | batch-generator: 21.05
[2022-10-29 00:02:42,425] [INFO] [logging.py:68:log_dist] [Rank 0] step=1330, skipped=0, lr=[7.838760959999999e-05, 7.838760959999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:02:42,428] [INFO] [timer.py:198:stop] 0/1330, RunningAvgSamplesPerSec=400.0651483960935, CurrSamplesPerSec=468.3347977039971, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1330/ 1716613 | consumed samples:       340480 | consumed tokens:    697303040 | elapsed time per iteration (ms): 650.9 | learning rate: 7.839E-05 | global batch size:   256 | lm loss: 5.061866E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 460.54 | backward-compute: 158.37 | backward-embedding-all-reduce: 0.01 | optimizer: 14.01 | batch-generator: 20.57
[2022-10-29 00:02:48,618] [INFO] [logging.py:68:log_dist] [Rank 0] step=1340, skipped=0, lr=[7.89774336e-05, 7.89774336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:02:48,621] [INFO] [timer.py:198:stop] 0/1340, RunningAvgSamplesPerSec=400.2963021994236, CurrSamplesPerSec=389.4387483896426, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1340/ 1716613 | consumed samples:       343040 | consumed tokens:    702545920 | elapsed time per iteration (ms): 619.3 | learning rate: 7.898E-05 | global batch size:   256 | lm loss: 5.002848E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 430.09 | backward-compute: 157.84 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 20.93
[2022-10-29 00:02:54,893] [INFO] [logging.py:68:log_dist] [Rank 0] step=1350, skipped=0, lr=[7.95672576e-05, 7.95672576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:02:54,896] [INFO] [timer.py:198:stop] 0/1350, RunningAvgSamplesPerSec=399.83306823355923, CurrSamplesPerSec=471.24198344902163, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1350/ 1716613 | consumed samples:       345600 | consumed tokens:    707788800 | elapsed time per iteration (ms): 627.5 | learning rate: 7.957E-05 | global batch size:   256 | lm loss: 5.020114E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 436.64 | backward-compute: 158.31 | backward-embedding-all-reduce: 0.01 | optimizer: 14.28 | batch-generator: 21.08
[2022-10-29 00:03:00,837] [INFO] [logging.py:68:log_dist] [Rank 0] step=1360, skipped=0, lr=[8.015708159999999e-05, 8.015708159999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:03:00,840] [INFO] [timer.py:198:stop] 0/1360, RunningAvgSamplesPerSec=399.8064677390429, CurrSamplesPerSec=425.0342104179795, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1360/ 1716613 | consumed samples:       348160 | consumed tokens:    713031680 | elapsed time per iteration (ms): 594.4 | learning rate: 8.016E-05 | global batch size:   256 | lm loss: 5.006388E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 404.76 | backward-compute: 157.20 | backward-embedding-all-reduce: 0.01 | optimizer: 14.07 | batch-generator: 21.64
[2022-10-29 00:03:07,310] [INFO] [logging.py:68:log_dist] [Rank 0] step=1370, skipped=0, lr=[8.07469056e-05, 8.07469056e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:03:07,313] [INFO] [timer.py:198:stop] 0/1370, RunningAvgSamplesPerSec=399.32086104730035, CurrSamplesPerSec=244.2871797111895, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1370/ 1716613 | consumed samples:       350720 | consumed tokens:    718274560 | elapsed time per iteration (ms): 647.4 | learning rate: 8.075E-05 | global batch size:   256 | lm loss: 5.021085E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 455.49 | backward-compute: 159.00 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 20.24
[2022-10-29 00:03:13,926] [INFO] [logging.py:68:log_dist] [Rank 0] step=1380, skipped=0, lr=[8.133672959999999e-05, 8.133672959999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:03:13,929] [INFO] [timer.py:198:stop] 0/1380, RunningAvgSamplesPerSec=399.01137647374134, CurrSamplesPerSec=287.1108696256714, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1380/ 1716613 | consumed samples:       353280 | consumed tokens:    723517440 | elapsed time per iteration (ms): 661.6 | learning rate: 8.134E-05 | global batch size:   256 | lm loss: 4.967877E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 470.69 | backward-compute: 158.41 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 20.02
[2022-10-29 00:03:19,851] [INFO] [logging.py:68:log_dist] [Rank 0] step=1390, skipped=0, lr=[8.19265536e-05, 8.19265536e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:03:19,853] [INFO] [timer.py:198:stop] 0/1390, RunningAvgSamplesPerSec=399.211276411958, CurrSamplesPerSec=466.53595189266224, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1390/ 1716613 | consumed samples:       355840 | consumed tokens:    728760320 | elapsed time per iteration (ms): 592.4 | learning rate: 8.193E-05 | global batch size:   256 | lm loss: 5.002713E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 401.16 | backward-compute: 157.95 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 20.76
[2022-10-29 00:03:26,309] [INFO] [logging.py:68:log_dist] [Rank 0] step=1400, skipped=0, lr=[8.25163776e-05, 8.25163776e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:03:26,312] [INFO] [timer.py:198:stop] 0/1400, RunningAvgSamplesPerSec=399.48020786409234, CurrSamplesPerSec=471.62283589903967, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1400/ 1716613 | consumed samples:       358400 | consumed tokens:    734003200 | elapsed time per iteration (ms): 645.9 | learning rate: 8.252E-05 | global batch size:   256 | lm loss: 4.983418E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 454.05 | backward-compute: 159.04 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 20.20
------------------------------------------------------------------------------------------------
 validation loss at iteration 1400 | lm loss value: 4.973206E+00 | lm loss PPL: 1.444894E+02 | 
------------------------------------------------------------------------------------------------
[2022-10-29 00:03:38,046] [INFO] [logging.py:68:log_dist] [Rank 0] step=1410, skipped=0, lr=[8.310620159999999e-05, 8.310620159999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:03:38,048] [INFO] [timer.py:198:stop] 0/1410, RunningAvgSamplesPerSec=398.8243229236037, CurrSamplesPerSec=467.2131665697786, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1410/ 1716613 | consumed samples:       360960 | consumed tokens:    739246080 | elapsed time per iteration (ms): 1173.6 | learning rate: 8.311E-05 | global batch size:   256 | lm loss: 4.968483E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 982.77 | backward-compute: 157.36 | backward-embedding-all-reduce: 0.01 | optimizer: 13.96 | batch-generator: 34.58
[2022-10-29 00:03:44,587] [INFO] [logging.py:68:log_dist] [Rank 0] step=1420, skipped=0, lr=[8.369602559999999e-05, 8.369602559999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:03:44,590] [INFO] [timer.py:198:stop] 0/1420, RunningAvgSamplesPerSec=398.77501263505513, CurrSamplesPerSec=247.97549394554127, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1420/ 1716613 | consumed samples:       363520 | consumed tokens:    744488960 | elapsed time per iteration (ms): 654.2 | learning rate: 8.370E-05 | global batch size:   256 | lm loss: 4.987673E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 463.10 | backward-compute: 158.48 | backward-embedding-all-reduce: 0.01 | optimizer: 14.13 | batch-generator: 20.75
[2022-10-29 00:03:51,071] [INFO] [logging.py:68:log_dist] [Rank 0] step=1430, skipped=0, lr=[8.42858496e-05, 8.42858496e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:03:51,073] [INFO] [timer.py:198:stop] 0/1430, RunningAvgSamplesPerSec=398.5142849624305, CurrSamplesPerSec=468.42306068104324, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1430/ 1716613 | consumed samples:       366080 | consumed tokens:    749731840 | elapsed time per iteration (ms): 648.3 | learning rate: 8.429E-05 | global batch size:   256 | lm loss: 4.974889E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 456.31 | backward-compute: 158.78 | backward-embedding-all-reduce: 0.01 | optimizer: 14.11 | batch-generator: 20.54
[2022-10-29 00:03:57,263] [INFO] [logging.py:68:log_dist] [Rank 0] step=1440, skipped=0, lr=[8.487567359999999e-05, 8.487567359999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:03:57,266] [INFO] [timer.py:198:stop] 0/1440, RunningAvgSamplesPerSec=398.23210505059313, CurrSamplesPerSec=275.7488674628497, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1440/ 1716613 | consumed samples:       368640 | consumed tokens:    754974720 | elapsed time per iteration (ms): 619.2 | learning rate: 8.488E-05 | global batch size:   256 | lm loss: 4.945400E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 427.97 | backward-compute: 158.17 | backward-embedding-all-reduce: 0.01 | optimizer: 14.07 | batch-generator: 21.10
[2022-10-29 00:04:03,806] [INFO] [logging.py:68:log_dist] [Rank 0] step=1450, skipped=0, lr=[8.546549759999999e-05, 8.546549759999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:04:03,809] [INFO] [timer.py:198:stop] 0/1450, RunningAvgSamplesPerSec=397.8203000751853, CurrSamplesPerSec=377.88756655095034, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1450/ 1716613 | consumed samples:       371200 | consumed tokens:    760217600 | elapsed time per iteration (ms): 654.1 | learning rate: 8.547E-05 | global batch size:   256 | lm loss: 4.955269E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 463.64 | backward-compute: 157.86 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 20.20
[2022-10-29 00:04:10,542] [INFO] [logging.py:68:log_dist] [Rank 0] step=1460, skipped=0, lr=[8.60553216e-05, 8.60553216e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:04:10,545] [INFO] [timer.py:198:stop] 0/1460, RunningAvgSamplesPerSec=397.57343808280876, CurrSamplesPerSec=467.6624773081252, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1460/ 1716613 | consumed samples:       373760 | consumed tokens:    765460480 | elapsed time per iteration (ms): 673.8 | learning rate: 8.606E-05 | global batch size:   256 | lm loss: 4.899848E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 482.37 | backward-compute: 158.32 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 19.50
[2022-10-29 00:04:17,284] [INFO] [logging.py:68:log_dist] [Rank 0] step=1470, skipped=0, lr=[8.66451456e-05, 8.66451456e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:04:17,286] [INFO] [timer.py:198:stop] 0/1470, RunningAvgSamplesPerSec=396.97975726384004, CurrSamplesPerSec=392.5815054769879, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1470/ 1716613 | consumed samples:       376320 | consumed tokens:    770703360 | elapsed time per iteration (ms): 674.2 | learning rate: 8.665E-05 | global batch size:   256 | lm loss: 4.944721E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 482.43 | backward-compute: 158.14 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 19.47
[2022-10-29 00:04:23,902] [INFO] [logging.py:68:log_dist] [Rank 0] step=1480, skipped=0, lr=[8.723496959999999e-05, 8.723496959999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:04:23,904] [INFO] [timer.py:198:stop] 0/1480, RunningAvgSamplesPerSec=396.83943261270656, CurrSamplesPerSec=385.57454509936855, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1480/ 1716613 | consumed samples:       378880 | consumed tokens:    775946240 | elapsed time per iteration (ms): 661.7 | learning rate: 8.723E-05 | global batch size:   256 | lm loss: 4.899843E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 470.00 | backward-compute: 158.08 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 19.55
[2022-10-29 00:04:30,183] [INFO] [logging.py:68:log_dist] [Rank 0] step=1490, skipped=0, lr=[8.782479359999998e-05, 8.782479359999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:04:30,185] [INFO] [timer.py:198:stop] 0/1490, RunningAvgSamplesPerSec=396.77876400481705, CurrSamplesPerSec=472.89402512842554, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1490/ 1716613 | consumed samples:       381440 | consumed tokens:    781189120 | elapsed time per iteration (ms): 628.1 | learning rate: 8.782E-05 | global batch size:   256 | lm loss: 4.949457E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 437.54 | backward-compute: 157.10 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 21.43
[2022-10-29 00:04:36,531] [INFO] [logging.py:68:log_dist] [Rank 0] step=1500, skipped=0, lr=[8.84146176e-05, 8.84146176e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:04:36,533] [INFO] [timer.py:198:stop] 0/1500, RunningAvgSamplesPerSec=396.9645774614007, CurrSamplesPerSec=299.42070860977384, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1500/ 1716613 | consumed samples:       384000 | consumed tokens:    786432000 | elapsed time per iteration (ms): 634.8 | learning rate: 8.841E-05 | global batch size:   256 | lm loss: 4.904569E+00 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 445.55 | backward-compute: 156.00 | backward-embedding-all-reduce: 0.01 | optimizer: 14.06 | batch-generator: 20.35
------------------------------------------------------------------------------------------------
 validation loss at iteration 1500 | lm loss value: 4.890817E+00 | lm loss PPL: 1.330622E+02 | 
------------------------------------------------------------------------------------------------
[2022-10-29 00:04:42,066] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:04:42,066] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:04:42,066] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2022-10-29 00:04:42,066] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2022-10-29 00:04:42,066] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:04:42,066] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:04:42,071] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2022-10-29 00:04:42,066] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:04:42,071] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2022-10-29 00:04:42,066] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:04:42,071] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2022-10-29 00:04:42,066] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:04:42,071] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2022-10-29 00:04:42,066] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:04:42,071] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2022-10-29 00:04:42,071] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2022-10-29 00:04:48,264] [INFO] [logging.py:68:log_dist] [Rank 0] step=1510, skipped=0, lr=[8.900444159999999e-05, 8.900444159999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:04:48,275] [INFO] [timer.py:198:stop] 0/1510, RunningAvgSamplesPerSec=396.74716518247334, CurrSamplesPerSec=414.89251313755796, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1510/ 1716613 | consumed samples:       386560 | consumed tokens:    791674880 | elapsed time per iteration (ms): 1173.3 | learning rate: 8.900E-05 | global batch size:   256 | lm loss: 4.920242E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 982.59 | backward-compute: 155.85 | backward-embedding-all-reduce: 0.01 | optimizer: 14.87 | batch-generator: 34.49
[2022-10-29 00:04:54,867] [INFO] [logging.py:68:log_dist] [Rank 0] step=1520, skipped=0, lr=[8.959426559999998e-05, 8.959426559999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:04:54,870] [INFO] [timer.py:198:stop] 0/1520, RunningAvgSamplesPerSec=396.8907808739611, CurrSamplesPerSec=462.95544556163014, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1520/ 1716613 | consumed samples:       389120 | consumed tokens:    796917760 | elapsed time per iteration (ms): 660.3 | learning rate: 8.959E-05 | global batch size:   256 | lm loss: 4.896585E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 469.87 | backward-compute: 156.14 | backward-embedding-all-reduce: 0.01 | optimizer: 14.02 | batch-generator: 21.23
[2022-10-29 00:05:01,300] [INFO] [logging.py:68:log_dist] [Rank 0] step=1530, skipped=0, lr=[9.018408959999999e-05, 9.018408959999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:05:01,303] [INFO] [timer.py:198:stop] 0/1530, RunningAvgSamplesPerSec=396.83226651980107, CurrSamplesPerSec=327.78003052689417, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1530/ 1716613 | consumed samples:       391680 | consumed tokens:    802160640 | elapsed time per iteration (ms): 643.3 | learning rate: 9.018E-05 | global batch size:   256 | lm loss: 4.851504E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 455.23 | backward-compute: 155.35 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 21.28
[2022-10-29 00:05:07,899] [INFO] [logging.py:68:log_dist] [Rank 0] step=1540, skipped=0, lr=[9.07739136e-05, 9.07739136e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:05:07,901] [INFO] [timer.py:198:stop] 0/1540, RunningAvgSamplesPerSec=396.42284634896123, CurrSamplesPerSec=465.9820019997778, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1540/ 1716613 | consumed samples:       394240 | consumed tokens:    807403520 | elapsed time per iteration (ms): 660.2 | learning rate: 9.077E-05 | global batch size:   256 | lm loss: 4.887496E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 470.39 | backward-compute: 156.31 | backward-embedding-all-reduce: 0.01 | optimizer: 14.04 | batch-generator: 20.81
[2022-10-29 00:05:14,830] [INFO] [logging.py:68:log_dist] [Rank 0] step=1550, skipped=0, lr=[9.13637376e-05, 9.13637376e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:05:14,833] [INFO] [timer.py:198:stop] 0/1550, RunningAvgSamplesPerSec=396.6649841097971, CurrSamplesPerSec=465.4907052882748, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1550/ 1716613 | consumed samples:       396800 | consumed tokens:    812646400 | elapsed time per iteration (ms): 692.8 | learning rate: 9.136E-05 | global batch size:   256 | lm loss: 4.893434E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 502.17 | backward-compute: 157.80 | backward-embedding-all-reduce: 0.01 | optimizer: 14.05 | batch-generator: 21.12
[2022-10-29 00:05:20,981] [INFO] [logging.py:68:log_dist] [Rank 0] step=1560, skipped=0, lr=[9.195356159999999e-05, 9.195356159999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:05:20,983] [INFO] [timer.py:198:stop] 0/1560, RunningAvgSamplesPerSec=396.609888177698, CurrSamplesPerSec=402.5618024708391, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1560/ 1716613 | consumed samples:       399360 | consumed tokens:    817889280 | elapsed time per iteration (ms): 615.1 | learning rate: 9.195E-05 | global batch size:   256 | lm loss: 4.891385E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 424.85 | backward-compute: 157.19 | backward-embedding-all-reduce: 0.01 | optimizer: 14.04 | batch-generator: 20.37
[2022-10-29 00:05:26,772] [INFO] [logging.py:68:log_dist] [Rank 0] step=1570, skipped=0, lr=[9.254338559999998e-05, 9.254338559999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:05:26,775] [INFO] [timer.py:198:stop] 0/1570, RunningAvgSamplesPerSec=396.63775182483823, CurrSamplesPerSec=454.05953415991473, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1570/ 1716613 | consumed samples:       401920 | consumed tokens:    823132160 | elapsed time per iteration (ms): 579.2 | learning rate: 9.254E-05 | global batch size:   256 | lm loss: 4.876285E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 389.30 | backward-compute: 157.50 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 20.75
[2022-10-29 00:05:32,048] [INFO] [logging.py:68:log_dist] [Rank 0] step=1580, skipped=0, lr=[9.31332096e-05, 9.31332096e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:05:32,050] [INFO] [timer.py:198:stop] 0/1580, RunningAvgSamplesPerSec=396.9363445203733, CurrSamplesPerSec=472.91901891073866, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1580/ 1716613 | consumed samples:       404480 | consumed tokens:    828375040 | elapsed time per iteration (ms): 527.6 | learning rate: 9.313E-05 | global batch size:   256 | lm loss: 4.869447E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 337.60 | backward-compute: 156.89 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 21.28
[2022-10-29 00:05:37,503] [INFO] [logging.py:68:log_dist] [Rank 0] step=1590, skipped=0, lr=[9.372303359999999e-05, 9.372303359999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:05:37,505] [INFO] [timer.py:198:stop] 0/1590, RunningAvgSamplesPerSec=396.8707900840812, CurrSamplesPerSec=472.2268359703472, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1590/ 1716613 | consumed samples:       407040 | consumed tokens:    833617920 | elapsed time per iteration (ms): 545.5 | learning rate: 9.372E-05 | global batch size:   256 | lm loss: 4.862310E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 355.02 | backward-compute: 157.41 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 21.63
[2022-10-29 00:05:43,041] [INFO] [logging.py:68:log_dist] [Rank 0] step=1600, skipped=0, lr=[9.43128576e-05, 9.43128576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:05:43,044] [INFO] [timer.py:198:stop] 0/1600, RunningAvgSamplesPerSec=396.88858444798893, CurrSamplesPerSec=471.5830660093953, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1600/ 1716613 | consumed samples:       409600 | consumed tokens:    838860800 | elapsed time per iteration (ms): 553.8 | learning rate: 9.431E-05 | global batch size:   256 | lm loss: 4.825438E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 363.29 | backward-compute: 158.12 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 20.14
------------------------------------------------------------------------------------------------
 validation loss at iteration 1600 | lm loss value: 4.832019E+00 | lm loss PPL: 1.254641E+02 | 
------------------------------------------------------------------------------------------------
[2022-10-29 00:05:52,656] [INFO] [logging.py:68:log_dist] [Rank 0] step=1610, skipped=0, lr=[9.49026816e-05, 9.49026816e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:05:52,658] [INFO] [timer.py:198:stop] 0/1610, RunningAvgSamplesPerSec=397.0670864382674, CurrSamplesPerSec=401.55853543241125, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1610/ 1716613 | consumed samples:       412160 | consumed tokens:    844103680 | elapsed time per iteration (ms): 961.3 | learning rate: 9.490E-05 | global batch size:   256 | lm loss: 4.826755E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 770.72 | backward-compute: 157.44 | backward-embedding-all-reduce: 0.01 | optimizer: 14.10 | batch-generator: 34.21
[2022-10-29 00:05:58,278] [INFO] [logging.py:68:log_dist] [Rank 0] step=1620, skipped=0, lr=[9.549250559999999e-05, 9.549250559999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:05:58,280] [INFO] [timer.py:198:stop] 0/1620, RunningAvgSamplesPerSec=397.33205298184845, CurrSamplesPerSec=429.50772499775996, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1620/ 1716613 | consumed samples:       414720 | consumed tokens:    849346560 | elapsed time per iteration (ms): 562.2 | learning rate: 9.549E-05 | global batch size:   256 | lm loss: 4.803628E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 370.38 | backward-compute: 157.72 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 20.93
[2022-10-29 00:06:04,214] [INFO] [logging.py:68:log_dist] [Rank 0] step=1630, skipped=0, lr=[9.60823296e-05, 9.60823296e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:06:04,217] [INFO] [timer.py:198:stop] 0/1630, RunningAvgSamplesPerSec=397.5015837404677, CurrSamplesPerSec=459.10103335397076, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1630/ 1716613 | consumed samples:       417280 | consumed tokens:    854589440 | elapsed time per iteration (ms): 593.7 | learning rate: 9.608E-05 | global batch size:   256 | lm loss: 4.829176E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 403.44 | backward-compute: 158.13 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 20.65
[2022-10-29 00:06:10,073] [INFO] [logging.py:68:log_dist] [Rank 0] step=1640, skipped=0, lr=[9.667215359999999e-05, 9.667215359999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:06:10,075] [INFO] [timer.py:198:stop] 0/1640, RunningAvgSamplesPerSec=397.31957188850095, CurrSamplesPerSec=369.4862507054273, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1640/ 1716613 | consumed samples:       419840 | consumed tokens:    859832320 | elapsed time per iteration (ms): 586.0 | learning rate: 9.667E-05 | global batch size:   256 | lm loss: 4.793145E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 396.32 | backward-compute: 157.21 | backward-embedding-all-reduce: 0.01 | optimizer: 14.08 | batch-generator: 20.48
[2022-10-29 00:06:16,466] [INFO] [logging.py:68:log_dist] [Rank 0] step=1650, skipped=0, lr=[9.726197759999999e-05, 9.726197759999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:06:16,469] [INFO] [timer.py:198:stop] 0/1650, RunningAvgSamplesPerSec=397.2316547747506, CurrSamplesPerSec=434.3208177873274, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1650/ 1716613 | consumed samples:       422400 | consumed tokens:    865075200 | elapsed time per iteration (ms): 639.3 | learning rate: 9.726E-05 | global batch size:   256 | lm loss: 4.861969E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 449.85 | backward-compute: 156.54 | backward-embedding-all-reduce: 0.01 | optimizer: 13.93 | batch-generator: 20.41
[2022-10-29 00:06:23,142] [INFO] [logging.py:68:log_dist] [Rank 0] step=1660, skipped=0, lr=[9.78518016e-05, 9.78518016e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:06:23,145] [INFO] [timer.py:198:stop] 0/1660, RunningAvgSamplesPerSec=397.4166316628754, CurrSamplesPerSec=471.12123275650254, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1660/ 1716613 | consumed samples:       424960 | consumed tokens:    870318080 | elapsed time per iteration (ms): 667.6 | learning rate: 9.785E-05 | global batch size:   256 | lm loss: 4.814530E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 478.58 | backward-compute: 156.39 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 20.32
[2022-10-29 00:06:29,520] [INFO] [logging.py:68:log_dist] [Rank 0] step=1670, skipped=0, lr=[9.844162559999999e-05, 9.844162559999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:06:29,522] [INFO] [timer.py:198:stop] 0/1670, RunningAvgSamplesPerSec=397.16511994202756, CurrSamplesPerSec=463.83058251085123, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1670/ 1716613 | consumed samples:       427520 | consumed tokens:    875560960 | elapsed time per iteration (ms): 637.8 | learning rate: 9.844E-05 | global batch size:   256 | lm loss: 4.800351E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 447.89 | backward-compute: 157.07 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 22.71
[2022-10-29 00:06:36,053] [INFO] [logging.py:68:log_dist] [Rank 0] step=1680, skipped=0, lr=[9.903144959999999e-05, 9.903144959999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:06:36,055] [INFO] [timer.py:198:stop] 0/1680, RunningAvgSamplesPerSec=396.67646373960764, CurrSamplesPerSec=469.31901085375404, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1680/ 1716613 | consumed samples:       430080 | consumed tokens:    880803840 | elapsed time per iteration (ms): 653.2 | learning rate: 9.903E-05 | global batch size:   256 | lm loss: 4.790151E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 461.70 | backward-compute: 159.69 | backward-embedding-all-reduce: 0.01 | optimizer: 14.25 | batch-generator: 20.44
[2022-10-29 00:06:42,672] [INFO] [logging.py:68:log_dist] [Rank 0] step=1690, skipped=0, lr=[9.96212736e-05, 9.96212736e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:06:42,674] [INFO] [timer.py:198:stop] 0/1690, RunningAvgSamplesPerSec=396.37974933603107, CurrSamplesPerSec=333.27720183452067, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1690/ 1716613 | consumed samples:       432640 | consumed tokens:    886046720 | elapsed time per iteration (ms): 661.9 | learning rate: 9.962E-05 | global batch size:   256 | lm loss: 4.794605E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 471.80 | backward-compute: 158.69 | backward-embedding-all-reduce: 0.01 | optimizer: 14.15 | batch-generator: 20.03
[2022-10-29 00:06:49,316] [INFO] [logging.py:68:log_dist] [Rank 0] step=1700, skipped=0, lr=[0.00010021109759999999, 0.00010021109759999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:06:49,318] [INFO] [timer.py:198:stop] 0/1700, RunningAvgSamplesPerSec=396.3941394473868, CurrSamplesPerSec=471.47704575392993, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1700/ 1716613 | consumed samples:       435200 | consumed tokens:    891289600 | elapsed time per iteration (ms): 664.4 | learning rate: 1.002E-04 | global batch size:   256 | lm loss: 4.786678E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 473.99 | backward-compute: 159.14 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 20.85
------------------------------------------------------------------------------------------------
 validation loss at iteration 1700 | lm loss value: 4.754271E+00 | lm loss PPL: 1.160790E+02 | 
------------------------------------------------------------------------------------------------
[2022-10-29 00:07:00,580] [INFO] [logging.py:68:log_dist] [Rank 0] step=1710, skipped=0, lr=[0.00010080092159999999, 0.00010080092159999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:07:00,583] [INFO] [timer.py:198:stop] 0/1710, RunningAvgSamplesPerSec=396.1759181202087, CurrSamplesPerSec=469.5669429352104, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1710/ 1716613 | consumed samples:       437760 | consumed tokens:    896532480 | elapsed time per iteration (ms): 1126.5 | learning rate: 1.008E-04 | global batch size:   256 | lm loss: 4.771722E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 934.73 | backward-compute: 158.67 | backward-embedding-all-reduce: 0.01 | optimizer: 13.93 | batch-generator: 34.70
[2022-10-29 00:07:06,545] [INFO] [logging.py:68:log_dist] [Rank 0] step=1720, skipped=0, lr=[0.00010139074559999998, 0.00010139074559999998], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:07:06,547] [INFO] [timer.py:198:stop] 0/1720, RunningAvgSamplesPerSec=396.2982745088757, CurrSamplesPerSec=403.955131388465, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1720/ 1716613 | consumed samples:       440320 | consumed tokens:    901775360 | elapsed time per iteration (ms): 596.4 | learning rate: 1.014E-04 | global batch size:   256 | lm loss: 4.771881E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 406.89 | backward-compute: 156.90 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 21.11
[2022-10-29 00:07:12,730] [INFO] [logging.py:68:log_dist] [Rank 0] step=1730, skipped=0, lr=[0.00010198056959999999, 0.00010198056959999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:07:12,733] [INFO] [timer.py:198:stop] 0/1730, RunningAvgSamplesPerSec=396.1240717384329, CurrSamplesPerSec=393.21753600600005, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1730/ 1716613 | consumed samples:       442880 | consumed tokens:    907018240 | elapsed time per iteration (ms): 618.6 | learning rate: 1.020E-04 | global batch size:   256 | lm loss: 4.777816E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 428.14 | backward-compute: 157.76 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 20.29
[2022-10-29 00:07:19,022] [INFO] [logging.py:68:log_dist] [Rank 0] step=1740, skipped=0, lr=[0.0001025703936, 0.0001025703936], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:07:19,025] [INFO] [timer.py:198:stop] 0/1740, RunningAvgSamplesPerSec=396.14582912145033, CurrSamplesPerSec=372.27274946191227, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1740/ 1716613 | consumed samples:       445440 | consumed tokens:    912261120 | elapsed time per iteration (ms): 629.2 | learning rate: 1.026E-04 | global batch size:   256 | lm loss: 4.744328E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 437.97 | backward-compute: 158.09 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 21.02
[2022-10-29 00:07:25,378] [INFO] [logging.py:68:log_dist] [Rank 0] step=1750, skipped=0, lr=[0.00010316021759999998, 0.00010316021759999998], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:07:25,381] [INFO] [timer.py:198:stop] 0/1750, RunningAvgSamplesPerSec=396.1535362180696, CurrSamplesPerSec=347.765675242199, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1750/ 1716613 | consumed samples:       448000 | consumed tokens:    917504000 | elapsed time per iteration (ms): 635.6 | learning rate: 1.032E-04 | global batch size:   256 | lm loss: 4.739930E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 444.82 | backward-compute: 158.04 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 20.27
[2022-10-29 00:07:32,014] [INFO] [logging.py:68:log_dist] [Rank 0] step=1760, skipped=0, lr=[0.0001037500416, 0.0001037500416], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:07:32,016] [INFO] [timer.py:198:stop] 0/1760, RunningAvgSamplesPerSec=395.7768204184751, CurrSamplesPerSec=306.8745638704072, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1760/ 1716613 | consumed samples:       450560 | consumed tokens:    922746880 | elapsed time per iteration (ms): 663.6 | learning rate: 1.038E-04 | global batch size:   256 | lm loss: 4.744090E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 472.34 | backward-compute: 158.21 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 20.33
[2022-10-29 00:07:38,443] [INFO] [logging.py:68:log_dist] [Rank 0] step=1770, skipped=0, lr=[0.0001043398656, 0.0001043398656], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:07:38,446] [INFO] [timer.py:198:stop] 0/1770, RunningAvgSamplesPerSec=395.85934912451137, CurrSamplesPerSec=268.5272923339782, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1770/ 1716613 | consumed samples:       453120 | consumed tokens:    927989760 | elapsed time per iteration (ms): 642.9 | learning rate: 1.043E-04 | global batch size:   256 | lm loss: 4.746668E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 452.54 | backward-compute: 158.03 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 20.47
[2022-10-29 00:07:45,368] [INFO] [logging.py:68:log_dist] [Rank 0] step=1780, skipped=0, lr=[0.00010492968959999999, 0.00010492968959999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:07:45,371] [INFO] [timer.py:198:stop] 0/1780, RunningAvgSamplesPerSec=395.71771885862734, CurrSamplesPerSec=415.61711298833205, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1780/ 1716613 | consumed samples:       455680 | consumed tokens:    933232640 | elapsed time per iteration (ms): 692.6 | learning rate: 1.049E-04 | global batch size:   256 | lm loss: 4.740154E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 500.97 | backward-compute: 158.91 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 20.47
[2022-10-29 00:07:51,428] [INFO] [logging.py:68:log_dist] [Rank 0] step=1790, skipped=0, lr=[0.00010551951359999999, 0.00010551951359999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:07:51,430] [INFO] [timer.py:198:stop] 0/1790, RunningAvgSamplesPerSec=395.3862051349121, CurrSamplesPerSec=415.73941351934855, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1790/ 1716613 | consumed samples:       458240 | consumed tokens:    938475520 | elapsed time per iteration (ms): 605.9 | learning rate: 1.055E-04 | global batch size:   256 | lm loss: 4.732510E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 414.62 | backward-compute: 158.63 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 20.62
[2022-10-29 00:07:57,866] [INFO] [logging.py:68:log_dist] [Rank 0] step=1800, skipped=0, lr=[0.0001061093376, 0.0001061093376], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:07:57,869] [INFO] [timer.py:198:stop] 0/1800, RunningAvgSamplesPerSec=395.33406779247, CurrSamplesPerSec=468.6913226733527, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1800/ 1716613 | consumed samples:       460800 | consumed tokens:    943718400 | elapsed time per iteration (ms): 643.9 | learning rate: 1.061E-04 | global batch size:   256 | lm loss: 4.682151E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 453.34 | backward-compute: 157.95 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 20.02
------------------------------------------------------------------------------------------------
 validation loss at iteration 1800 | lm loss value: 4.700767E+00 | lm loss PPL: 1.100315E+02 | 
------------------------------------------------------------------------------------------------
[2022-10-29 00:08:08,457] [INFO] [logging.py:68:log_dist] [Rank 0] step=1810, skipped=0, lr=[0.00010669916159999999, 0.00010669916159999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:08:08,459] [INFO] [timer.py:198:stop] 0/1810, RunningAvgSamplesPerSec=395.41470255310605, CurrSamplesPerSec=439.1280370101359, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1810/ 1716613 | consumed samples:       463360 | consumed tokens:    948961280 | elapsed time per iteration (ms): 1059.0 | learning rate: 1.067E-04 | global batch size:   256 | lm loss: 4.785451E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 867.15 | backward-compute: 158.59 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 34.05
[2022-10-29 00:08:14,517] [INFO] [logging.py:68:log_dist] [Rank 0] step=1820, skipped=0, lr=[0.00010728898559999999, 0.00010728898559999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:08:14,519] [INFO] [timer.py:198:stop] 0/1820, RunningAvgSamplesPerSec=395.23513382207113, CurrSamplesPerSec=217.79753022312374, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1820/ 1716613 | consumed samples:       465920 | consumed tokens:    954204160 | elapsed time per iteration (ms): 606.0 | learning rate: 1.073E-04 | global batch size:   256 | lm loss: 4.728530E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 416.18 | backward-compute: 157.55 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 19.16
[2022-10-29 00:08:20,699] [INFO] [logging.py:68:log_dist] [Rank 0] step=1830, skipped=0, lr=[0.00010787880959999998, 0.00010787880959999998], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:08:20,701] [INFO] [timer.py:198:stop] 0/1830, RunningAvgSamplesPerSec=394.9724367748071, CurrSamplesPerSec=465.7620832292274, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1830/ 1716613 | consumed samples:       468480 | consumed tokens:    959447040 | elapsed time per iteration (ms): 618.2 | learning rate: 1.079E-04 | global batch size:   256 | lm loss: 4.698870E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 427.89 | backward-compute: 156.78 | backward-embedding-all-reduce: 0.01 | optimizer: 14.42 | batch-generator: 21.12
[2022-10-29 00:08:26,594] [INFO] [logging.py:68:log_dist] [Rank 0] step=1840, skipped=0, lr=[0.0001084686336, 0.0001084686336], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:08:26,596] [INFO] [timer.py:198:stop] 0/1840, RunningAvgSamplesPerSec=394.59206716821734, CurrSamplesPerSec=410.42540998896095, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1840/ 1716613 | consumed samples:       471040 | consumed tokens:    964689920 | elapsed time per iteration (ms): 589.5 | learning rate: 1.085E-04 | global batch size:   256 | lm loss: 4.728936E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 399.49 | backward-compute: 156.99 | backward-embedding-all-reduce: 0.01 | optimizer: 14.26 | batch-generator: 21.20
[2022-10-29 00:08:32,866] [INFO] [logging.py:68:log_dist] [Rank 0] step=1850, skipped=0, lr=[0.00010905845759999999, 0.00010905845759999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:08:32,869] [INFO] [timer.py:198:stop] 0/1850, RunningAvgSamplesPerSec=394.6640364297889, CurrSamplesPerSec=356.68219880678726, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1850/ 1716613 | consumed samples:       473600 | consumed tokens:    969932800 | elapsed time per iteration (ms): 627.3 | learning rate: 1.091E-04 | global batch size:   256 | lm loss: 4.681994E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 439.12 | backward-compute: 155.05 | backward-embedding-all-reduce: 0.01 | optimizer: 14.14 | batch-generator: 20.67
[2022-10-29 00:08:39,124] [INFO] [logging.py:68:log_dist] [Rank 0] step=1860, skipped=0, lr=[0.00010964828159999998, 0.00010964828159999998], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:08:39,126] [INFO] [timer.py:198:stop] 0/1860, RunningAvgSamplesPerSec=394.6120085908185, CurrSamplesPerSec=465.01655406575895, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1860/ 1716613 | consumed samples:       476160 | consumed tokens:    975175680 | elapsed time per iteration (ms): 625.6 | learning rate: 1.096E-04 | global batch size:   256 | lm loss: 4.665341E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 437.02 | backward-compute: 155.25 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 21.28
[2022-10-29 00:08:45,430] [INFO] [logging.py:68:log_dist] [Rank 0] step=1870, skipped=0, lr=[0.00011023810559999999, 0.00011023810559999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:08:45,433] [INFO] [timer.py:198:stop] 0/1870, RunningAvgSamplesPerSec=394.45506164607275, CurrSamplesPerSec=451.346219549924, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1870/ 1716613 | consumed samples:       478720 | consumed tokens:    980418560 | elapsed time per iteration (ms): 630.7 | learning rate: 1.102E-04 | global batch size:   256 | lm loss: 4.699600E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 441.93 | backward-compute: 155.26 | backward-embedding-all-reduce: 0.01 | optimizer: 14.30 | batch-generator: 21.06
[2022-10-29 00:08:52,220] [INFO] [logging.py:68:log_dist] [Rank 0] step=1880, skipped=0, lr=[0.00011082792959999999, 0.00011082792959999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:08:52,223] [INFO] [timer.py:198:stop] 0/1880, RunningAvgSamplesPerSec=394.5812683993939, CurrSamplesPerSec=414.63360745376923, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1880/ 1716613 | consumed samples:       481280 | consumed tokens:    985661440 | elapsed time per iteration (ms): 679.0 | learning rate: 1.108E-04 | global batch size:   256 | lm loss: 4.712250E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 490.40 | backward-compute: 155.52 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 20.81
[2022-10-29 00:08:58,544] [INFO] [logging.py:68:log_dist] [Rank 0] step=1890, skipped=0, lr=[0.00011141775359999998, 0.00011141775359999998], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:08:58,547] [INFO] [timer.py:198:stop] 0/1890, RunningAvgSamplesPerSec=394.61876967699277, CurrSamplesPerSec=463.83218542542863, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1890/ 1716613 | consumed samples:       483840 | consumed tokens:    990904320 | elapsed time per iteration (ms): 632.4 | learning rate: 1.114E-04 | global batch size:   256 | lm loss: 4.708848E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 444.68 | backward-compute: 154.45 | backward-embedding-all-reduce: 0.01 | optimizer: 14.05 | batch-generator: 21.49
[2022-10-29 00:09:05,141] [INFO] [logging.py:68:log_dist] [Rank 0] step=1900, skipped=0, lr=[0.00011200757759999999, 0.00011200757759999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:09:05,144] [INFO] [timer.py:198:stop] 0/1900, RunningAvgSamplesPerSec=394.8847462128761, CurrSamplesPerSec=466.83128121652265, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1900/ 1716613 | consumed samples:       486400 | consumed tokens:    996147200 | elapsed time per iteration (ms): 659.7 | learning rate: 1.120E-04 | global batch size:   256 | lm loss: 4.703434E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 472.15 | backward-compute: 154.66 | backward-embedding-all-reduce: 0.01 | optimizer: 14.06 | batch-generator: 20.33
------------------------------------------------------------------------------------------------
 validation loss at iteration 1900 | lm loss value: 4.680911E+00 | lm loss PPL: 1.078683E+02 | 
------------------------------------------------------------------------------------------------
[2022-10-29 00:09:16,779] [INFO] [logging.py:68:log_dist] [Rank 0] step=1910, skipped=0, lr=[0.00011259740159999999, 0.00011259740159999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:09:16,781] [INFO] [timer.py:198:stop] 0/1910, RunningAvgSamplesPerSec=394.51260253748916, CurrSamplesPerSec=471.3263802560699, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1910/ 1716613 | consumed samples:       488960 | consumed tokens:   1001390080 | elapsed time per iteration (ms): 1163.8 | learning rate: 1.126E-04 | global batch size:   256 | lm loss: 4.678283E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 975.21 | backward-compute: 154.93 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 32.99
[2022-10-29 00:09:22,848] [INFO] [logging.py:68:log_dist] [Rank 0] step=1920, skipped=0, lr=[0.0001131872256, 0.0001131872256], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:09:22,851] [INFO] [timer.py:198:stop] 0/1920, RunningAvgSamplesPerSec=394.3312940845659, CurrSamplesPerSec=410.53086068569786, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1920/ 1716613 | consumed samples:       491520 | consumed tokens:   1006632960 | elapsed time per iteration (ms): 606.9 | learning rate: 1.132E-04 | global batch size:   256 | lm loss: 4.678552E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 418.37 | backward-compute: 155.80 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 19.80
[2022-10-29 00:09:28,993] [INFO] [logging.py:68:log_dist] [Rank 0] step=1930, skipped=0, lr=[0.00011377704959999999, 0.00011377704959999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:09:28,996] [INFO] [timer.py:198:stop] 0/1930, RunningAvgSamplesPerSec=394.1743728211636, CurrSamplesPerSec=427.50898382878967, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1930/ 1716613 | consumed samples:       494080 | consumed tokens:   1011875840 | elapsed time per iteration (ms): 614.6 | learning rate: 1.138E-04 | global batch size:   256 | lm loss: 4.690718E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 425.49 | backward-compute: 155.88 | backward-embedding-all-reduce: 0.01 | optimizer: 14.00 | batch-generator: 20.09
[2022-10-29 00:09:34,467] [INFO] [logging.py:68:log_dist] [Rank 0] step=1940, skipped=0, lr=[0.00011436687359999998, 0.00011436687359999998], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:09:34,469] [INFO] [timer.py:198:stop] 0/1940, RunningAvgSamplesPerSec=394.2435868645601, CurrSamplesPerSec=464.71225231027046, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1940/ 1716613 | consumed samples:       496640 | consumed tokens:   1017118720 | elapsed time per iteration (ms): 547.4 | learning rate: 1.144E-04 | global batch size:   256 | lm loss: 4.713300E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 357.60 | backward-compute: 156.62 | backward-embedding-all-reduce: 0.01 | optimizer: 14.06 | batch-generator: 19.65
[2022-10-29 00:09:39,619] [INFO] [logging.py:68:log_dist] [Rank 0] step=1950, skipped=0, lr=[0.0001149566976, 0.0001149566976], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:09:39,622] [INFO] [timer.py:198:stop] 0/1950, RunningAvgSamplesPerSec=394.37992148555884, CurrSamplesPerSec=466.0952764600885, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1950/ 1716613 | consumed samples:       499200 | consumed tokens:   1022361600 | elapsed time per iteration (ms): 515.3 | learning rate: 1.150E-04 | global batch size:   256 | lm loss: 4.657762E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 324.06 | backward-compute: 157.80 | backward-embedding-all-reduce: 0.01 | optimizer: 14.11 | batch-generator: 20.74
[2022-10-29 00:09:45,258] [INFO] [logging.py:68:log_dist] [Rank 0] step=1960, skipped=0, lr=[0.00011554652159999999, 0.00011554652159999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:09:45,260] [INFO] [timer.py:198:stop] 0/1960, RunningAvgSamplesPerSec=394.67028035585, CurrSamplesPerSec=472.03584466374525, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1960/ 1716613 | consumed samples:       501760 | consumed tokens:   1027604480 | elapsed time per iteration (ms): 563.8 | learning rate: 1.155E-04 | global batch size:   256 | lm loss: 4.569774E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 373.83 | backward-compute: 157.28 | backward-embedding-all-reduce: 0.01 | optimizer: 14.02 | batch-generator: 20.75
[2022-10-29 00:09:51,995] [INFO] [logging.py:68:log_dist] [Rank 0] step=1970, skipped=0, lr=[0.0001161363456, 0.0001161363456], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:09:51,998] [INFO] [timer.py:198:stop] 0/1970, RunningAvgSamplesPerSec=394.59513214428574, CurrSamplesPerSec=232.78168239729996, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1970/ 1716613 | consumed samples:       504320 | consumed tokens:   1032847360 | elapsed time per iteration (ms): 673.7 | learning rate: 1.161E-04 | global batch size:   256 | lm loss: 4.622344E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 485.07 | backward-compute: 155.47 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 20.43
[2022-10-29 00:09:58,340] [INFO] [logging.py:68:log_dist] [Rank 0] step=1980, skipped=0, lr=[0.00011672616959999999, 0.00011672616959999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:09:58,343] [INFO] [timer.py:198:stop] 0/1980, RunningAvgSamplesPerSec=394.3866695292937, CurrSamplesPerSec=408.4656502023799, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1980/ 1716613 | consumed samples:       506880 | consumed tokens:   1038090240 | elapsed time per iteration (ms): 634.5 | learning rate: 1.167E-04 | global batch size:   256 | lm loss: 4.638388E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 444.84 | backward-compute: 156.29 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 20.71
[2022-10-29 00:10:04,921] [INFO] [logging.py:68:log_dist] [Rank 0] step=1990, skipped=0, lr=[0.00011731599359999998, 0.00011731599359999998], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:10:04,923] [INFO] [timer.py:198:stop] 0/1990, RunningAvgSamplesPerSec=394.20592818901844, CurrSamplesPerSec=346.38620832042943, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     1990/ 1716613 | consumed samples:       509440 | consumed tokens:   1043333120 | elapsed time per iteration (ms): 658.1 | learning rate: 1.173E-04 | global batch size:   256 | lm loss: 4.637276E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 465.52 | backward-compute: 159.47 | backward-embedding-all-reduce: 0.01 | optimizer: 14.02 | batch-generator: 20.31
[2022-10-29 00:10:11,114] [INFO] [logging.py:68:log_dist] [Rank 0] step=2000, skipped=0, lr=[0.00011790581759999998, 0.00011790581759999998], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:10:11,117] [INFO] [timer.py:198:stop] 0/2000, RunningAvgSamplesPerSec=394.4043021135046, CurrSamplesPerSec=452.8675959011111, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2000/ 1716613 | consumed samples:       512000 | consumed tokens:   1048576000 | elapsed time per iteration (ms): 619.3 | learning rate: 1.179E-04 | global batch size:   256 | lm loss: 4.646936E+00 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 427.41 | backward-compute: 158.75 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 20.68
------------------------------------------------------------------------------------------------
 validation loss at iteration 2000 | lm loss value: 4.661055E+00 | lm loss PPL: 1.057476E+02 | 
------------------------------------------------------------------------------------------------
[2022-10-29 00:10:16,154] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:10:16,154] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:10:16,154] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:10:16,154] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2022-10-29 00:10:16,154] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:10:16,154] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2022-10-29 00:10:16,154] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2022-10-29 00:10:16,159] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2022-10-29 00:10:16,154] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:10:16,154] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:10:16,159] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2022-10-29 00:10:16,154] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:10:16,159] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2022-10-29 00:10:16,154] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-29 00:10:16,159] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2022-10-29 00:10:16,165] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2022-10-29 00:10:22,195] [INFO] [logging.py:68:log_dist] [Rank 0] step=2010, skipped=0, lr=[0.0001184956416, 0.0001184956416], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:10:22,197] [INFO] [timer.py:198:stop] 0/2010, RunningAvgSamplesPerSec=394.3679722782737, CurrSamplesPerSec=273.1188988779592, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2010/ 1716613 | consumed samples:       514560 | consumed tokens:   1053818880 | elapsed time per iteration (ms): 1108.1 | learning rate: 1.185E-04 | global batch size:   256 | lm loss: 4.613960E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 915.05 | backward-compute: 158.02 | backward-embedding-all-reduce: 0.01 | optimizer: 15.07 | batch-generator: 31.94
[2022-10-29 00:10:28,781] [INFO] [logging.py:68:log_dist] [Rank 0] step=2020, skipped=0, lr=[0.00011908546559999999, 0.00011908546559999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:10:28,784] [INFO] [timer.py:198:stop] 0/2020, RunningAvgSamplesPerSec=394.3140931695255, CurrSamplesPerSec=270.37839514753017, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2020/ 1716613 | consumed samples:       517120 | consumed tokens:   1059061760 | elapsed time per iteration (ms): 658.7 | learning rate: 1.191E-04 | global batch size:   256 | lm loss: 4.599606E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 467.91 | backward-compute: 158.07 | backward-embedding-all-reduce: 0.01 | optimizer: 14.02 | batch-generator: 19.77
[2022-10-29 00:10:35,503] [INFO] [logging.py:68:log_dist] [Rank 0] step=2030, skipped=0, lr=[0.0001196752896, 0.0001196752896], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:10:35,506] [INFO] [timer.py:198:stop] 0/2030, RunningAvgSamplesPerSec=394.11402103093485, CurrSamplesPerSec=220.55477811044688, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2030/ 1716613 | consumed samples:       519680 | consumed tokens:   1064304640 | elapsed time per iteration (ms): 672.2 | learning rate: 1.197E-04 | global batch size:   256 | lm loss: 4.587049E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 480.47 | backward-compute: 158.20 | backward-embedding-all-reduce: 0.01 | optimizer: 14.20 | batch-generator: 20.31
[2022-10-29 00:10:42,137] [INFO] [logging.py:68:log_dist] [Rank 0] step=2040, skipped=0, lr=[0.00012026511359999999, 0.00012026511359999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:10:42,140] [INFO] [timer.py:198:stop] 0/2040, RunningAvgSamplesPerSec=393.59280433550174, CurrSamplesPerSec=288.3859994413528, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2040/ 1716613 | consumed samples:       522240 | consumed tokens:   1069547520 | elapsed time per iteration (ms): 663.4 | learning rate: 1.203E-04 | global batch size:   256 | lm loss: 4.581563E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 471.13 | backward-compute: 159.03 | backward-embedding-all-reduce: 0.01 | optimizer: 14.01 | batch-generator: 19.77
[2022-10-29 00:10:49,017] [INFO] [logging.py:68:log_dist] [Rank 0] step=2050, skipped=0, lr=[0.00012085493759999999, 0.00012085493759999999], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:10:49,020] [INFO] [timer.py:198:stop] 0/2050, RunningAvgSamplesPerSec=393.2212176317325, CurrSamplesPerSec=247.5273184639842, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2050/ 1716613 | consumed samples:       524800 | consumed tokens:   1074790400 | elapsed time per iteration (ms): 688.0 | learning rate: 1.209E-04 | global batch size:   256 | lm loss: 4.596632E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 496.36 | backward-compute: 158.45 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 19.66
[2022-10-29 00:10:56,156] [INFO] [logging.py:68:log_dist] [Rank 0] step=2060, skipped=0, lr=[0.00012144476159999998, 0.00012144476159999998], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:10:56,158] [INFO] [timer.py:198:stop] 0/2060, RunningAvgSamplesPerSec=392.9779589539698, CurrSamplesPerSec=341.07484867120354, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2060/ 1716613 | consumed samples:       527360 | consumed tokens:   1080033280 | elapsed time per iteration (ms): 713.9 | learning rate: 1.214E-04 | global batch size:   256 | lm loss: 4.587250E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 522.87 | backward-compute: 158.07 | backward-embedding-all-reduce: 0.01 | optimizer: 14.02 | batch-generator: 20.32
[2022-10-29 00:11:02,696] [INFO] [logging.py:68:log_dist] [Rank 0] step=2070, skipped=0, lr=[0.00012203458559999997, 0.00012203458559999997], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:11:02,699] [INFO] [timer.py:198:stop] 0/2070, RunningAvgSamplesPerSec=393.0604403658596, CurrSamplesPerSec=471.02203193542726, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2070/ 1716613 | consumed samples:       529920 | consumed tokens:   1085276160 | elapsed time per iteration (ms): 654.0 | learning rate: 1.220E-04 | global batch size:   256 | lm loss: 4.603811E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 463.14 | backward-compute: 157.62 | backward-embedding-all-reduce: 0.01 | optimizer: 14.03 | batch-generator: 19.83
[2022-10-29 00:11:09,171] [INFO] [logging.py:68:log_dist] [Rank 0] step=2080, skipped=0, lr=[0.0001226244096, 0.0001226244096], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:11:09,173] [INFO] [timer.py:198:stop] 0/2080, RunningAvgSamplesPerSec=393.28734639436976, CurrSamplesPerSec=423.9655060222316, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2080/ 1716613 | consumed samples:       532480 | consumed tokens:   1090519040 | elapsed time per iteration (ms): 647.5 | learning rate: 1.226E-04 | global batch size:   256 | lm loss: 4.599753E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 454.80 | backward-compute: 159.69 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 19.01
[2022-10-29 00:11:15,990] [INFO] [logging.py:68:log_dist] [Rank 0] step=2090, skipped=0, lr=[0.0001232142336, 0.0001232142336], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:11:15,993] [INFO] [timer.py:198:stop] 0/2090, RunningAvgSamplesPerSec=393.1598932482174, CurrSamplesPerSec=312.32309768697354, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2090/ 1716613 | consumed samples:       535040 | consumed tokens:   1095761920 | elapsed time per iteration (ms): 681.9 | learning rate: 1.232E-04 | global batch size:   256 | lm loss: 4.575864E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 489.92 | backward-compute: 159.28 | backward-embedding-all-reduce: 0.01 | optimizer: 14.06 | batch-generator: 20.18
[2022-10-29 00:11:22,942] [INFO] [logging.py:68:log_dist] [Rank 0] step=2100, skipped=0, lr=[0.0001238040576, 0.0001238040576], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:11:22,945] [INFO] [timer.py:198:stop] 0/2100, RunningAvgSamplesPerSec=392.9710760912127, CurrSamplesPerSec=458.0528431700442, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2100/ 1716613 | consumed samples:       537600 | consumed tokens:   1101004800 | elapsed time per iteration (ms): 695.2 | learning rate: 1.238E-04 | global batch size:   256 | lm loss: 4.540260E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 502.38 | backward-compute: 159.35 | backward-embedding-all-reduce: 0.01 | optimizer: 14.01 | batch-generator: 19.40
------------------------------------------------------------------------------------------------
 validation loss at iteration 2100 | lm loss value: 4.554939E+00 | lm loss PPL: 9.510093E+01 | 
------------------------------------------------------------------------------------------------
[2022-10-29 00:11:35,112] [INFO] [logging.py:68:log_dist] [Rank 0] step=2110, skipped=0, lr=[0.0001243938816, 0.0001243938816], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:11:35,115] [INFO] [timer.py:198:stop] 0/2110, RunningAvgSamplesPerSec=392.52063727641263, CurrSamplesPerSec=467.0034585702257, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2110/ 1716613 | consumed samples:       540160 | consumed tokens:   1106247680 | elapsed time per iteration (ms): 1217.0 | learning rate: 1.244E-04 | global batch size:   256 | lm loss: 4.573008E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1024.66 | backward-compute: 158.26 | backward-embedding-all-reduce: 0.01 | optimizer: 14.05 | batch-generator: 31.51
[2022-10-29 00:11:41,697] [INFO] [logging.py:68:log_dist] [Rank 0] step=2120, skipped=0, lr=[0.00012498370559999998, 0.00012498370559999998], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:11:41,700] [INFO] [timer.py:198:stop] 0/2120, RunningAvgSamplesPerSec=392.2952367209612, CurrSamplesPerSec=400.64873836197506, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2120/ 1716613 | consumed samples:       542720 | consumed tokens:   1111490560 | elapsed time per iteration (ms): 658.5 | learning rate: 1.250E-04 | global batch size:   256 | lm loss: 4.555441E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 468.02 | backward-compute: 157.48 | backward-embedding-all-reduce: 0.01 | optimizer: 14.05 | batch-generator: 19.71
[2022-10-29 00:11:47,206] [INFO] [logging.py:68:log_dist] [Rank 0] step=2130, skipped=0, lr=[0.00012557352959999997, 0.00012557352959999997], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-29 00:11:47,208] [INFO] [timer.py:198:stop] 0/2130, RunningAvgSamplesPerSec=392.343111873222, CurrSamplesPerSec=459.60096017203654, MemAllocated=1.64GB, MaxMemAllocated=5.66GB
 iteration     2130/ 1716613 | consumed samples:       545280 | consumed tokens:   1116733440 | elapsed time per iteration (ms): 550.9 | learning rate: 1.256E-04 | global batch size:   256 | lm loss: 4.569905E+00 | loss scale: 32768.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 358.53 | backward-compute: 159.65 | backward-embedding-all-reduce: 0.01 | optimizer: 14.06 | batch-generator: 20.24
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 276, in <module>
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 276, in <module>
[2022-10-29 00:11:48,109] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 58799
        pretrain(train_valid_test_datasets_provider, model_provider, forward_step,pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
    
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 166, in pretrain
pretrain(train_valid_test_datasets_provider, model_provider, forward_step,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 166, in pretrain
    
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 166, in pretrain
pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 166, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,    
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 166, in pretrain
pretrain(train_valid_test_datasets_provider, model_provider, forward_step,    
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 166, in pretrain
pretrain(train_valid_test_datasets_provider, model_provider, forward_step,    
iteration = train(forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 166, in pretrain
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 891, in train
iteration = train(forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 891, in train
iteration = train(forward_step_func,    
iteration = train(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 891, in train
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 891, in train
    iteration = train(forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 891, in train
iteration = train(forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 891, in train
iteration = train(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 891, in train
    train_step(forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 502, in train_step
train_step(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 502, in train_step
    train_step(forward_step_func,    
train_step(forward_step_func,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 502, in train_step

      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 502, in train_step
train_step(forward_step_func,    
train_step(forward_step_func,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 502, in train_step

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 502, in train_step
    train_step(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/training.py", line 502, in train_step
    losses_reduced = forward_backward_func(
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 147, in forward_backward_no_pipelining
losses_reduced = forward_backward_func(
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 147, in forward_backward_no_pipelining
losses_reduced = forward_backward_func(    
losses_reduced = forward_backward_func(
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 147, in forward_backward_no_pipelining
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 147, in forward_backward_no_pipelining
    losses_reduced = forward_backward_func(
losses_reduced = forward_backward_func(
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 147, in forward_backward_no_pipelining
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 147, in forward_backward_no_pipelining
losses_reduced = forward_backward_func(
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 147, in forward_backward_no_pipelining
    output_tensor = forward_step(forward_step_func, data_iterator, model,    
output_tensor = forward_step(forward_step_func, data_iterator, model,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 61, in forward_step

      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 61, in forward_step
output_tensor = forward_step(forward_step_func, data_iterator, model,    
output_tensor = forward_step(forward_step_func, data_iterator, model,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 61, in forward_step
    
output_tensor = forward_step(forward_step_func, data_iterator, model,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 61, in forward_step
    
output_tensor = forward_step(forward_step_func, data_iterator, model,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 61, in forward_step
    
output_tensor = forward_step(forward_step_func, data_iterator, model,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 61, in forward_step
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/schedules.py", line 61, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 208, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 208, in forward_step
output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 208, in forward_step
output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 208, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 208, in forward_step
        output_tensor, *other_losses = model(tokens, position_ids, attention_mask,output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)
    
output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 208, in forward_step

      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/examples/MoE/../../pretrain_gpt.py", line 208, in forward_step
output_tensor, *other_losses = model(tokens, position_ids, attention_mask,    
output_tensor, *other_losses = model(tokens, position_ids, attention_mask,    
output_tensor, *other_losses = model(tokens, position_ids, attention_mask,  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    output_tensor, *other_losses = model(tokens, position_ids, attention_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    output_tensor, *other_losses = model(tokens, position_ids, attention_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    output_tensor, *other_losses = model(tokens, position_ids, attention_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)    
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
return forward_call(*input, **kwargs)    
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
                return func(*args, **kwargs)    return func(*args, **kwargs)
return func(*args, **kwargs)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1736, in forward

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1736, in forward
    return func(*args, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1736, in forward
return func(*args, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1736, in forward
return func(*args, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1736, in forward

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1736, in forward
    return func(*args, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1736, in forward
                    loss = self.module(*inputs, **kwargs)
loss = self.module(*inputs, **kwargs)  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

loss = self.module(*inputs, **kwargs)  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    loss = self.module(*inputs, **kwargs)    
loss = self.module(*inputs, **kwargs)  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
loss = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
loss = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/gpt_model.py", line 120, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/gpt_model.py", line 120, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/gpt_model.py", line 120, in forward
        return forward_call(*input, **kwargs)
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/gpt_model.py", line 120, in forward
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/gpt_model.py", line 120, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/gpt_model.py", line 120, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/gpt_model.py", line 120, in forward
    lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/language_model.py", line 390, in forward
    lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    lm_output, *moe_losses = self.language_model(    
lm_output, *moe_losses = self.language_model(  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/language_model.py", line 390, in forward
        return forward_call(*input, **kwargs)
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/transformer.py", line 841, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/language_model.py", line 390, in forward
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/language_model.py", line 390, in forward
    encoder_output, *moe_losses = self.encoder(encoder_input,encoder_output, *moe_losses = self.encoder(encoder_input,

      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    encoder_output, *moe_losses = self.encoder(encoder_input,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/transformer.py", line 815, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/transformer.py", line 815, in forward
return forward_call(*input, **kwargs)    
return forward_call(*input, **kwargs)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/transformer.py", line 815, in forward

      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/transformer.py", line 833, in forward
encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/transformer.py", line 833, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/transformer.py", line 815, in forward
    part_hidden_states, _ = layer(part_hidden_states,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
part_hidden_states, _ = layer(part_hidden_states,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    sampled_indices, part_attention_mask = gpt_sample_tokens(attention_mask,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/ops/token_dropping/dropping_utils.py", line 23, in gpt_sample_tokens
hidden_states = ScatterTokens.apply(hidden_states, part_hidden_states, sampled_indices[index-1, :, :])
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/ops/token_dropping/dropping_utils.py", line 127, in forward
sampled_indices, part_attention_mask = gpt_sample_tokens(attention_mask,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/ops/token_dropping/dropping_utils.py", line 23, in gpt_sample_tokens
    sampled_indices, part_attention_mask = gpt_sample_tokens(attention_mask,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/ops/token_dropping/dropping_utils.py", line 23, in gpt_sample_tokens
sampled_indices, part_attention_mask = gpt_sample_tokens(attention_mask,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/ops/token_dropping/dropping_utils.py", line 23, in gpt_sample_tokens
    return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/transformer.py", line 485, in forward
return forward_call(*input, **kwargs)
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/transformer.py", line 485, in forward
[2022-10-29 00:11:48,209] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 58799
  File "/opt/conda/bin/deepspeed", line 6, in <module>
    sampled_indices = torch.multinomial(prob_dist, reserved_length)
    KeyboardInterruptsampled_indices = torch.multinomial(prob_dist, reserved_length)    

scatter_results = token_dropping_module.token_scatter_(
    sampled_indices = torch.multinomial(prob_dist, reserved_length)
KeyboardInterruptKeyboardInterrupt    sampled_indices = torch.multinomial(prob_dist, reserved_length)KeyboardInterrupt



KeyboardInterrupt
    main()
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/launcher/runner.py", line 519, in main
    self.attention(layernorm_output,
    self.attention(layernorm_output,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/transformer.py", line 348, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/model/transformer.py", line 324, in forward
    context_layer = torch.bmm(attention_probs, value_layer.transpose(0, 1))
    attention_probs = self.attention_dropout(attention_probs)
      File "/opt/conda/lib/python3.8/contextlib.py", line 120, in __exit__
KeyboardInterruptresult.wait()

  File "/opt/conda/lib/python3.8/subprocess.py", line 1083, in wait
    next(self.gen)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/mpu/random.py", line 196, in fork
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.8/subprocess.py", line 1808, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.8/subprocess.py", line 1766, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/launcher/runner.py", line 511, in sigkill_handler
    _set_cuda_rng_state(orig_cuda_rng_state)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/mpu/random.py", line 101, in _set_cuda_rng_state
    _lazy_call(cb)    
result_kill = subprocess.Popen(kill_cmd, env=env)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 155, in _lazy_call
NameError: free variable 'kill_cmd' referenced before assignment in enclosing scope
    callable()
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal/megatron/mpu/random.py", line 99, in cb
    default_generator.set_state(new_state)
KeyboardInterrupt
[2022-10-29 00:11:49,560] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 58800
[2022-10-29 00:11:50,671] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 58801
[2022-10-29 00:11:51,526] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 58803
[2022-10-29 00:11:51,780] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 58805
[2022-10-29 00:11:52,514] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 58807
[2022-10-29 00:11:52,516] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 58809
[2022-10-29 00:11:52,527] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 58811
[2022-10-29 00:11:52,534] [INFO] [launch.py:295:sigkill_handler] Main process received SIGTERM, exiting
