Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.2851393222808838 seconds
Warning: Permanently added '[192.168.0.193]:41375' (ECDSA) to the list of known hosts.
[2022-10-31 10:49:12,266] [INFO] [runner.py:415:main] Using IP address of 192.168.0.193 for node worker-0
[2022-10-31 10:49:12,267] [INFO] [runner.py:504:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ3b3JrZXItMCI6IFswLCAxLCAyLCAzLCA0LCA1LCA2LCA3XX0= --master_addr=192.168.0.193 --master_port=29500 /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py --override-lr-scheduler --adam-beta1 0.9 --adam-beta2 0.95 --tensor-model-parallel-size 1 --init-method-std 0.02 --lr-decay-tokens 2600000000 --lr-warmup-tokens 3000000000 --micro-batch-size 4 --exit-duration-in-mins 30000000 --global-batch-size 256 --num-layers 12 --hidden-size 768 --num-attention-heads 12 --seq-length 2048 --max-position-embeddings 2048 --train-tokens 30000000000 --train-samples 43945312 --lr 3.0e-4 --min-lr 3.0e-5 --lr-decay-style cosine --split 98,2,0 --log-interval 10 --eval-interval 100 --eval-iters 10 --save-interval 10000 --weight-decay 0.1 --clip-grad 1.0 --hysteresis 2 --num-workers 0 --fp16 --increse-length-token-interval 0.05 --initial-sequence-length 256 --load /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4 --save /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4 --tensorboard-queue-size 1 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir tensorboard/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4_azwus2f200000DQ_2022.10.31-10.49.05 --log-optimizer-states-to-tensorboard --vocab-file /blob//data/the_pile_public_merged_nopreprocessing/gpt2-vocab.json --merge-file /blob//data/the_pile_public_merged_nopreprocessing/gpt2-merges.txt --data-path /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document --data-impl mmap --deepspeed --deepspeed_config ds_config_gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4.json --zero-stage 0 --pipeline-model-parallel-size 1 --no-pipeline-parallel
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.23988866806030273 seconds
[2022-10-31 10:49:14,719] [INFO] [launch.py:129:main] 0 NCCL_IB_PCI_RELAXED_ORDERING=1
[2022-10-31 10:49:14,719] [INFO] [launch.py:129:main] 0 NCCL_VERSION=2.9.8
[2022-10-31 10:49:14,723] [INFO] [launch.py:129:main] 0 NCCL_SOCKET_IFNAME=eth0
[2022-10-31 10:49:14,723] [INFO] [launch.py:129:main] 0 NCCL_NET_GDR_LEVEL=5
[2022-10-31 10:49:14,723] [INFO] [launch.py:129:main] 0 NCCL_DEBUG=INFO
[2022-10-31 10:49:14,723] [INFO] [launch.py:129:main] 0 NCCL_TREE_THRESHOLD=0
[2022-10-31 10:49:14,723] [INFO] [launch.py:129:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2022-10-31 10:49:14,727] [INFO] [launch.py:129:main] 0 NCCL_IB_TIMEOUT=20
[2022-10-31 10:49:14,727] [INFO] [launch.py:129:main] 0 NCCL_TOPO_FILE=/opt/msft/topo.xml
[2022-10-31 10:49:14,727] [INFO] [launch.py:136:main] WORLD INFO DICT: {'worker-0': [0, 1, 2, 3, 4, 5, 6, 7]}
[2022-10-31 10:49:14,730] [INFO] [launch.py:142:main] nnodes=1, num_local_procs=8, node_rank=0
[2022-10-31 10:49:14,730] [INFO] [launch.py:155:main] global_rank_mapping=defaultdict(<class 'list'>, {'worker-0': [0, 1, 2, 3, 4, 5, 6, 7]})
[2022-10-31 10:49:14,730] [INFO] [launch.py:156:main] dist_world_size=8
[2022-10-31 10:49:14,730] [INFO] [launch.py:158:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.5998537540435791 seconds
Loading extension module token_dropping...
Time to load token_dropping op: 0.24362730979919434 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
using world size: 8, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_token_layers ........................... 0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning ............................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 8
  data_path ....................................... ['/blob/data/the_pile_public_merged_nopreprocessing/pile_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ ds_config_gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 2048
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 3072
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 768
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  increse_length_token_interval ................... 0.05
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  initial_sequence_length ......................... 256
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 64
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4
  load_teacher .................................... None
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. True
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0003
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 2600000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 3000000000
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /blob//data/the_pile_public_merged_nopreprocessing/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 12
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 12
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... True
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4
  save_interval ................................... 10000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 2048
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. tensorboard/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4_azwus2f200000DQ_2022.10.31-10.49.05
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_iters ..................................... None
  train_samples ................................... 43945312
  train_tokens .................................... 30000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /blob//data/the_pile_public_merged_nopreprocessing/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 8
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 0
-------------------- end of arguments ---------------------
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
[2022-10-31 10:49:18,628] [INFO] [comm.py:633:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.5618281364440918 seconds
Loading extension module token_dropping...
Time to load token_dropping op: 0.424898624420166 seconds
Loading extension module token_dropping...
Time to load token_dropping op: 0.41274309158325195 seconds
Loading extension module token_dropping...
Time to load token_dropping op: 0.39217162132263184 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
Loading extension module token_dropping...ninja
 .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
Time to load token_dropping op: 0.3489096164703369 seconds
Loading extension module token_dropping...
Time to load token_dropping op: 0.29241371154785156 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report--------------------------------------------------

--------------------------------------------------DeepSpeed C++/CUDA extension op report

NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.--------------------------------------------------

--------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.

JIT compiled ops requires ninja--------------------------------------------------

JIT compiled ops requires ninjaninja
 ..................ninja  [92m[OKAY][0m
--------------------------------------------------
..................op name  [92m[OKAY][0m................
 --------------------------------------------------installed
 op name..  ................compatible 
installed-------------------------------------------------- 
.. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m cpu_adam.......  ...............[92m[OKAY][0m 
[93m[NO][0m fused_adam.......  .............[92m[OKAY][0m 
[93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad fused_lamb............  .............[93m[NO][0m  [93m[NO][0m.......  .......[92m[OKAY][0m 
[92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja sparse_attn.................. [92m[OKAY][0m 
............sparse_attn--------------------------------------------------  
[93m[NO][0m............op name   .......[93m[NO][0m................   [92m[OKAY][0m.......installed
  transformer[92m[OKAY][0m.. 
 ............compatible transformer
[93m[NO][0m -------------------------------------------------- ............
.......  [93m[NO][0m[92m[OKAY][0m 
.......cpu_adam stochastic_transformer [92m[OKAY][0m ...............
.  stochastic_transformer[93m[NO][0m[93m[NO][0m   ...............  [92m[OKAY][0m
 [92m[OKAY][0mcpu_adagrad
[93m[NO][0m  ...................  [93m[NO][0m[92m[OKAY][0m 
....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0masync_io .......  ...............[92m[OKAY][0m 
[93m[NO][0m ....... --------------------------------------------------[92m[OKAY][0m

utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
DeepSpeed general environment info:
transformer_inferencetorch install path  .................  [93m[NO][0m ....... ['/opt/conda/lib/python3.8/site-packages/torch'][92m[OKAY][0m

torch version .................... 1.11.0+cu113
token_droppingtorch cuda version  ........................  [93m[NO][0m11.3 
.......torch hip version  [92m[OKAY][0m................
 None
--------------------------------------------------nvcc version
 ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
> setting tensorboard ...
2022-10-31 10:49:19.571349: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> compiling dataset index builder ...
make: Entering directory '/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/data'
>>> done with dataset index builder. Compilation time: 0.131 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
azwus2f200000DQ:19338:19338 [0] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:19338:19338 [0] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:19338:19338 [0] NCCL INFO P2P plugin IBext
azwus2f200000DQ:19338:19338 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:19338:19338 [0] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:19338:19338 [0] NCCL INFO Using network IBext
NCCL version 2.10.3+cuda11.3
azwus2f200000DQ:19346:19346 [5] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:19344:19344 [4] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:19348:19348 [6] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:19339:19339 [1] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:19342:19342 [3] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:19340:19340 [2] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:19339:19339 [1] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:19344:19344 [4] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:19339:19339 [1] NCCL INFO P2P plugin IBext
azwus2f200000DQ:19346:19346 [5] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:19339:19339 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:19344:19344 [4] NCCL INFO P2P plugin IBext
azwus2f200000DQ:19342:19342 [3] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:19344:19344 [4] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:19346:19346 [5] NCCL INFO P2P plugin IBext
azwus2f200000DQ:19348:19348 [6] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:19342:19342 [3] NCCL INFO P2P plugin IBext
azwus2f200000DQ:19346:19346 [5] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:19348:19348 [6] NCCL INFO P2P plugin IBext
azwus2f200000DQ:19342:19342 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:19348:19348 [6] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:19340:19340 [2] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:19340:19340 [2] NCCL INFO P2P plugin IBext
azwus2f200000DQ:19340:19340 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:19350:19350 [7] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:19350:19350 [7] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:19350:19350 [7] NCCL INFO P2P plugin IBext
azwus2f200000DQ:19350:19350 [7] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:19342:19342 [3] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:19342:19342 [3] NCCL INFO Using network IBext
azwus2f200000DQ:19348:19348 [6] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:19340:19340 [2] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:19348:19348 [6] NCCL INFO Using network IBext
azwus2f200000DQ:19340:19340 [2] NCCL INFO Using network IBext
azwus2f200000DQ:19344:19344 [4] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:19346:19346 [5] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:19346:19346 [5] NCCL INFO Using network IBext
azwus2f200000DQ:19344:19344 [4] NCCL INFO Using network IBext
azwus2f200000DQ:19339:19339 [1] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:19339:19339 [1] NCCL INFO Using network IBext
azwus2f200000DQ:19350:19350 [7] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:19350:19350 [7] NCCL INFO Using network IBext
azwus2f200000DQ:19348:20546 [6] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:19344:20548 [4] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:19339:20551 [1] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:19338:20491 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:19350:20554 [7] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:19342:20545 [3] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:19346:20549 [5] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:19340:20547 [2] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:19342:20545 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:19346:20549 [5] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:19344:20548 [4] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:19340:20547 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:19339:20551 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:19350:20554 [7] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:19338:20491 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:19348:20546 [6] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:19348:20546 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
azwus2f200000DQ:19348:20546 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000DQ:19350:20554 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19339:20551 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19350:20554 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19340:20547 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19339:20551 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000DQ:19342:20545 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
azwus2f200000DQ:19344:20548 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
azwus2f200000DQ:19346:20549 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
azwus2f200000DQ:19344:20548 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000DQ:19342:20545 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000DQ:19346:20549 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19340:20547 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:20491 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
azwus2f200000DQ:19338:20491 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 00 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 01 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 02 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 03 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 00 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 00 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 00 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 04 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 00 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 00 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 01 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 01 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 05 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 01 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 00 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 01 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 01 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 02 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 02 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 06 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 02 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 00 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 01 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 02 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 02 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 03 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 03 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 07 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 03 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 01 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 02 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 03 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 03 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 04 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 04 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 04 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 08 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 02 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 03 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 04 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 04 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 05 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 05 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 05 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 03 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 09 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 04 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 05 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 05 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 06 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 06 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 04 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 06 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 10 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 05 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 06 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 06 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 07 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 07 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 05 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 07 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 11 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 06 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 07 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 07 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 08 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 08 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 08 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 08 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 06 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 08 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 12 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 07 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 09 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 09 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 09 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 09 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 07 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 09 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 13 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 08 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 10 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 10 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 10 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 10 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 08 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 10 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 14 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 09 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 11 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 11 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 11 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 11 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 12 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 09 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 12 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 12 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 15 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 10 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 11 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 13 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 12 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 13 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 13 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 16 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 11 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 12 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 10 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 14 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 13 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 12 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 14 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 14 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 17 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 13 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 11 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 15 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 13 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 14 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 15 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 15 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 18 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 14 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 12 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 16 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 14 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 15 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 16 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 16 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 19 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 15 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 13 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 17 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 15 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 16 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 17 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 17 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 20 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 16 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 14 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 18 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 16 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 18 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 18 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 17 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 21 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 17 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 19 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 15 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 17 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 19 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 22 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 19 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 18 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 20 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 18 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 16 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 18 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 20 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 23 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 19 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 21 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 20 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 17 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 19 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 19 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 21 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 20 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 22 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 21 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 18 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 22 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 20 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 20 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 23 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 21 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 22 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 19 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 23 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 21 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 21 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 22 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 23 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 22 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 20 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 22 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 23 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Channel 23 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 21 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 23 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 22 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 23 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Connected all rings
azwus2f200000DQ:19342:20545 [3] NCCL INFO Connected all rings
azwus2f200000DQ:19339:20551 [1] NCCL INFO Connected all rings
azwus2f200000DQ:19338:20491 [0] NCCL INFO Connected all rings
azwus2f200000DQ:19350:20554 [7] NCCL INFO Connected all rings
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 00 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Connected all rings
azwus2f200000DQ:19346:20549 [5] NCCL INFO Connected all rings
azwus2f200000DQ:19348:20546 [6] NCCL INFO Connected all rings
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 01 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 02 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 03 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 04 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 05 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 06 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 07 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 08 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 09 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 10 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 11 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 12 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 13 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 14 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 15 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 16 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 17 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 18 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 19 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 20 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 21 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 00 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 22 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 01 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 00 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 00 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:20554 [7] NCCL INFO Channel 23 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 02 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 01 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 01 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 03 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 02 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 00 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 00 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 00 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 02 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 03 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 04 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 01 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 01 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 01 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 03 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 05 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 04 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 02 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 02 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 02 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 06 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 04 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 05 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 03 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 03 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 03 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 07 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 05 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 06 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 04 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 04 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 08 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 04 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 06 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 07 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 05 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 05 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 09 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 05 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 07 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 08 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 06 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 06 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 10 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 06 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 08 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 09 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 07 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 11 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 07 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 07 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 10 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 09 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 08 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 12 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 08 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 08 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 11 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 10 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 13 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 09 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 09 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 12 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 11 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 09 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 14 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 10 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 10 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 13 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 12 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 10 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 15 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 11 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 11 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 11 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 14 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 13 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 16 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 12 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 12 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 12 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 15 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 14 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 17 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 13 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 13 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 13 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 16 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 15 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 18 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 14 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 14 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 17 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 14 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 16 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 19 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 15 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 15 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 18 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 15 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 17 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 20 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 16 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 16 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 16 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 19 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 18 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 21 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 17 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 17 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 17 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 20 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 19 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 22 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 18 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 18 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 18 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 21 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 20 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:20547 [2] NCCL INFO Channel 23 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 19 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 22 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 19 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 21 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 19 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 20 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:20551 [1] NCCL INFO Channel 23 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 20 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 22 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 20 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 21 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 21 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:20545 [3] NCCL INFO Channel 23 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 21 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 22 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 22 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 22 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:20549 [5] NCCL INFO Channel 23 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:20546 [6] NCCL INFO Channel 23 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19344:20548 [4] NCCL INFO Channel 23 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19338:20491 [0] NCCL INFO Connected all trees
azwus2f200000DQ:19338:20491 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19338:20491 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19350:20554 [7] NCCL INFO Connected all trees
azwus2f200000DQ:19350:20554 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19350:20554 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19339:20551 [1] NCCL INFO Connected all trees
azwus2f200000DQ:19339:20551 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19340:20547 [2] NCCL INFO Connected all trees
azwus2f200000DQ:19340:20547 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19348:20546 [6] NCCL INFO Connected all trees
azwus2f200000DQ:19348:20546 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19339:20551 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19346:20549 [5] NCCL INFO Connected all trees
azwus2f200000DQ:19344:20548 [4] NCCL INFO Connected all trees
azwus2f200000DQ:19346:20549 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19344:20548 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19342:20545 [3] NCCL INFO Connected all trees
azwus2f200000DQ:19342:20545 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19340:20547 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19348:20546 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19344:20548 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19346:20549 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19342:20545 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19342:20545 [3] NCCL INFO comm 0x7fd860002fb0 rank 3 nranks 8 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000DQ:19338:20491 [0] NCCL INFO comm 0x7f44ac002fb0 rank 0 nranks 8 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000DQ:19340:20547 [2] NCCL INFO comm 0x7f0c38002fb0 rank 2 nranks 8 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000DQ:19344:20548 [4] NCCL INFO comm 0x7f1930002fb0 rank 4 nranks 8 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000DQ:19348:20546 [6] NCCL INFO comm 0x7f154c002fb0 rank 6 nranks 8 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000DQ:19350:20554 [7] NCCL INFO comm 0x7efb08002fb0 rank 7 nranks 8 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000DQ:19339:20551 [1] NCCL INFO comm 0x7f186c002fb0 rank 1 nranks 8 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000DQ:19346:20549 [5] NCCL INFO comm 0x7f645c002fb0 rank 5 nranks 8 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000DQ:19338:19338 [0] NCCL INFO Launch mode Parallel
>>> done with compiling and loading fused kernels. Compilation time: 14.384 seconds
time to initialize megatron (seconds): -16.520
[after megatron is initialized] datetime: 2022-10-31 10:49:35 
building GPT model ...
[2022-10-31 10:49:35,677] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2022-10-31 10:49:35,678] [INFO] [utils.py:828:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2022-10-31 10:49:35,682] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 56.25 GB, percent = 3.2%
[2022-10-31 10:49:35,840] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2022-10-31 10:49:35,841] [INFO] [utils.py:828:see_memory_usage] MA 0.24 GB         Max_MA 0.24 GB         CA 0.25 GB         Max_CA 0 GB 
[2022-10-31 10:49:35,845] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 56.29 GB, percent = 3.2%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 125262336
setting training iterations to 171661
> learning rate decay style: cosine
DeepSpeed is enabled.
[2022-10-31 10:49:35,855] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.3+d505437d, git-hash=d505437d, git-branch=xiaoxia/token-drop-dynamic-train
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19339:21423 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
azwus2f200000DQ:19340:21421 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19342:21420 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
azwus2f200000DQ:19339:21423 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19340:21421 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19342:21420 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19346:21422 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
azwus2f200000DQ:19344:21424 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
azwus2f200000DQ:19346:21422 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19350:21425 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19348:21419 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19350:21425 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000DQ:19344:21424 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19348:21419 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:19338:21418 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
azwus2f200000DQ:19338:21418 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 00 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 00 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 00 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 01 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 01 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 01 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 02 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 02 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 02 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 03 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 00 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 00 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 00 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 00 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 03 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 03 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 04 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 00 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 01 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 01 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 01 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 01 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 04 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 05 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 01 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 04 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 02 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 05 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 02 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 06 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 02 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 02 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 05 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 02 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 06 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 03 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 07 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 03 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 03 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 06 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 03 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 03 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 07 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 08 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 04 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 04 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 04 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 07 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 04 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 04 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 08 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 05 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 09 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 05 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 05 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 08 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 05 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 05 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 06 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 06 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 06 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 09 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 06 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 09 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 10 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 06 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 07 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 07 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 07 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 07 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 10 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 10 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 07 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 11 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 08 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 08 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 08 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 08 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 08 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 11 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 11 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 12 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 09 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 09 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 09 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 09 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 09 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 12 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 12 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 13 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 10 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 10 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 10 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 10 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 10 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 13 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 13 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 14 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 11 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 11 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 11 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 11 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 11 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 14 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 15 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 14 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 12 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 12 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 12 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 12 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 12 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 15 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 16 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 15 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 13 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 13 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 13 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 13 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 16 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 13 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 17 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 16 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 14 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 14 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 14 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 14 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 17 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 14 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 18 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 17 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 15 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 15 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 15 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 15 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 18 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 15 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 18 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 19 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 16 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 16 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 16 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 16 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 16 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 20 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 19 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 19 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 17 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 17 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 17 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 17 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 17 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 21 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 20 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 20 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 18 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 18 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 18 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 22 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 21 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 21 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 18 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 18 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 19 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 19 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 23 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 19 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 22 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 22 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 19 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 19 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 20 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 20 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 20 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 23 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 23 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 20 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 20 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 21 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 21 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 21 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 21 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 21 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 22 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 22 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 22 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 22 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 22 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 23 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 23 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 23 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19338:21418 [0] NCCL INFO Channel 23 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 23 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Connected all rings
azwus2f200000DQ:19348:21419 [6] NCCL INFO Connected all rings
azwus2f200000DQ:19350:21425 [7] NCCL INFO Connected all rings
azwus2f200000DQ:19338:21418 [0] NCCL INFO Connected all rings
azwus2f200000DQ:19342:21420 [3] NCCL INFO Connected all rings
azwus2f200000DQ:19344:21424 [4] NCCL INFO Connected all rings
azwus2f200000DQ:19346:21422 [5] NCCL INFO Connected all rings
azwus2f200000DQ:19339:21423 [1] NCCL INFO Connected all rings
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 00 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 01 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 02 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 03 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 04 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 05 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 06 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 07 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 08 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 09 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 10 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 11 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 12 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 13 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 14 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 00 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 00 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 15 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 01 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 00 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 16 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 00 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 01 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 00 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 02 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 00 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 01 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 17 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 01 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 02 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 01 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 01 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 03 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 02 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 18 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 02 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 03 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 02 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 02 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 04 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 03 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 19 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 03 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 04 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 03 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 05 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 03 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 04 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 04 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 05 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 20 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 04 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 06 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 04 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 05 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 05 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 06 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 21 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 05 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 07 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 05 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 06 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 06 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 07 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 22 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 06 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 06 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 07 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 07 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 08 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Channel 23 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 08 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 07 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 07 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 09 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 08 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 08 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 09 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 08 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 10 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 08 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 09 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 09 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 10 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 09 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 11 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 09 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 10 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 10 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 11 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 10 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 12 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 10 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 11 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 11 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 12 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 11 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 13 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 11 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 12 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 12 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 13 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 12 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 14 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 12 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 13 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 14 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 13 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 13 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 15 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 14 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 13 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 15 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 14 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 14 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 15 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 16 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 14 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 16 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 15 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 15 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 16 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 17 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 15 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 17 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 16 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 16 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 17 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 16 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 17 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 18 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 18 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 17 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 17 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 18 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 19 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 18 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 19 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 18 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 19 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 18 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 20 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 19 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 20 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 19 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 20 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 19 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 21 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 20 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 21 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 21 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 20 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 20 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 22 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 21 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 22 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 22 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 21 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 21 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19340:21421 [2] NCCL INFO Channel 23 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 22 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 22 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19344:21424 [4] NCCL INFO Channel 23 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 22 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19348:21419 [6] NCCL INFO Channel 23 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:19342:21420 [3] NCCL INFO Channel 23 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:19346:21422 [5] NCCL INFO Channel 23 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:19339:21423 [1] NCCL INFO Channel 23 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:19350:21425 [7] NCCL INFO Connected all trees
azwus2f200000DQ:19350:21425 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19338:21418 [0] NCCL INFO Connected all trees
azwus2f200000DQ:19338:21418 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19350:21425 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19338:21418 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19348:21419 [6] NCCL INFO Connected all trees
azwus2f200000DQ:19348:21419 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19340:21421 [2] NCCL INFO Connected all trees
azwus2f200000DQ:19342:21420 [3] NCCL INFO Connected all trees
azwus2f200000DQ:19339:21423 [1] NCCL INFO Connected all trees
azwus2f200000DQ:19340:21421 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19342:21420 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19344:21424 [4] NCCL INFO Connected all trees
azwus2f200000DQ:19339:21423 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19346:21422 [5] NCCL INFO Connected all trees
azwus2f200000DQ:19344:21424 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19346:21422 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:19348:21419 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19340:21421 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19342:21420 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19339:21423 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19344:21424 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19346:21422 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19339:21423 [1] NCCL INFO comm 0x7f1314002fb0 rank 1 nranks 8 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000DQ:19342:21420 [3] NCCL INFO comm 0x7fd2b4002fb0 rank 3 nranks 8 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000DQ:19350:21425 [7] NCCL INFO comm 0x7ef5ec002fb0 rank 7 nranks 8 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000DQ:19346:21422 [5] NCCL INFO comm 0x7f5f2c002fb0 rank 5 nranks 8 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000DQ:19340:21421 [2] NCCL INFO comm 0x7f06c4002fb0 rank 2 nranks 8 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000DQ:19348:21419 [6] NCCL INFO comm 0x7f0ffc002fb0 rank 6 nranks 8 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000DQ:19338:21418 [0] NCCL INFO comm 0x7f3f6c002fb0 rank 0 nranks 8 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000DQ:19344:21424 [4] NCCL INFO comm 0x7f13cc002fb0 rank 4 nranks 8 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000DQ:19338:19338 [0] NCCL INFO Launch mode Parallel
[2022-10-31 10:49:41,004] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-10-31 10:49:41,005] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-10-31 10:49:41,012] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-10-31 10:49:41,015] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = {basic_optimizer.__class__.__name__}
[2022-10-31 10:49:41,016] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
here 
here language_model
here language_model.embedding
here language_model.embedding.word_embeddings
herehere language_model.embedding.position_embeddings 
herehere
  language_model.embedding.embedding_dropoutherehere
 
language_modelhere here
here   here here language_model.embedding
language_model.encoderhere 
language_model.embedding.word_embeddingshere
 language_model.encoder.layers
here

here here herelanguage_model.embedding.position_embeddingslanguage_model language_model.encoder.layers.0 
language_modelhere
[2022-10-31 10:49:41,094] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam


herehere language_model language_model.encoder.layers.0.randomltd_layerlanguage_model


herehereherehere   
 language_model.embedding.embedding_dropout[2022-10-31 10:49:41,100] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
language_model.encoder.layers.0.randomltd_layer.input_layernormlanguage_model.embedding language_model.embedding
here
here language_model.embedding

herehere 
 language_model.embedding.word_embeddingslanguage_model.embedding.word_embeddings
here
here  hereherelanguage_model.embedding.word_embeddingslanguage_model.embedding.position_embeddings
 
  
language_model.embedding.position_embeddingshereherelanguage_model.encoderlanguage_model.embeddinghere 
here 

[2022-10-31 10:49:41,100] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f46fc826280>
language_model here language_model.embedding.position_embeddings
herehere language_model.embedding.embedding_dropout 
language_model.encoder.layersherehere
  here
language_model.embedding.word_embeddingslanguage_model.encoder 

language_model.encoder.layers.0language_model.encoder.layers.0.randomltd_layer.attentionhereherehere
   
 language_model.embeddingherelanguage_model.embedding.position_embeddingslanguage_model.embedding.embedding_dropoutlanguage_model.embedding.embedding_dropoutlanguage_model.encoder.layershere
 

[2022-10-31 10:49:41,103] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]


 herelanguage_model.encoder.layers.0.randomltd_layerhereherehereherelanguage_model.encoder.layers.0.randomltd_layer.attention.query_key_value 
  language_model.encoder
 here language_model.encoder language_model.encoder.layers

language_model.encoder.layers.0here
here
  language_model.encoder.layersherelanguage_model.encoder.layers.0language_model.embedding.word_embeddingshere
 
 
hereherelanguage_model.encoder.layers.0.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.0.randomltd_layerhere here
language_model.embedding.embedding_dropout
  herelanguage_model.encoder.layers.0here here
language_model.encoder.layers.0.randomltd_layer.input_layernormlanguage_model.encoder.layers.0.randomltd_layer 
 language_model.embedding.position_embeddings herehere

language_model.encoder.layers.0.randomltd_layer.attention.attention_dropout
language_model.encoder.layers.0.randomltd_layer.input_layernorm  herehere 
language_model.encoder.layers.0.randomltd_layer.attention
herehere  
language_model.encoder.layers.0.randomltd_layer.attention.denselanguage_model.encoder.layers.0.randomltd_layer.attention.query_key_value

herehereherehere   
language_model.encoder.layers.0.randomltd_layer.post_attention_layernorm language_model.embedding.embedding_dropoutlanguage_model.encoder.layers.0.randomltd_layer.attention.scale_mask_softmax
language_model.encoder.layers.0.randomltd_layerlanguage_modelhere


here
language_model.encoder hereherehere  herelanguage_model.encoder.layers.0.randomltd_layer.attention 
  language_model.encoder.layers.0.randomltd_layer.input_layernormlanguage_model.encoder.layers.0.randomltd_layer.mlp 
language_model.encoderherelanguage_model.encoder.layers.0.randomltd_layer.attention.attention_dropout
herehere  language_model.encoder.layers.0.randomltd_layer.attention.denselanguage_model.encoder.layers.0.randomltd_layer.input_layernormlanguage_model.encoder.layers


here
herehere   here
language_model.encoder.layers.0.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.0.randomltd_layer.attentionlanguage_model.encoder.layers.0 

language_model.embedding
herelanguage_model.encoder.layers.0.randomltd_layer.attentionherehere 


here   language_model.encoder.layers.0.randomltd_layer.mlp.dense_h_to_4hhereherehere language_model.encoder.layers.0.randomltd_layer.mlplanguage_model.encoder.layers.0.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.0.randomltd_layer.attention.query_key_value
   language_model.encoder.layers.0.randomltd_layer


herelanguage_model.encoder.layers
language_model.embedding.word_embeddingshere 
language_model.encoder.layers.0
language_model.encoder.layers.0.randomltd_layer.attention.query_key_valueherehere 
 
language_model.embedding.position_embeddingsherelanguage_model.encoder.layers.0.randomltd_layer
 herehere
herelanguage_model.encoder.layers.0.randomltd_layer.attention.scale_mask_softmax   language_model.embedding.embedding_dropoutherehere
language_model.encoder.layers.0.randomltd_layer.input_layernormlanguage_model.encoder.layers.0.randomltd_layer.mlp.dense_h_to_4h
here  
here
here  language_model.encoder.layers.0.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.0.randomltd_layer.input_layernormhere here language_model.encoder.layers.0.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.0.randomltd_layer.attention.scale_mask_softmax
here
 language_model.encoder.layers.0.randomltd_layer.attention.attention_dropout
herehere   language_model.encoder.layers.0.randomltd_layer.attentionlanguage_model.encoder.layers.0.randomltd_layer.attention.denselanguage_model.encoder.layers.0.randomltd_layer.attention


hereherehere   language_model.encoder.layers.0.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.0.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.0.randomltd_layer.attention.query_key_value
here
here here  language_model.encoder.layers.0.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.0.randomltd_layer.mlplanguage_model.encoder.layers.0.randomltd_layer.attention.scale_mask_softmax


hereherehere  language_model.encoder.layers.0.randomltd_layer.attention.attention_dropout language_model.encoder.layers.0.randomltd_layer.mlp.dense_h_to_4h
language_model.encoder.layers.0.randomltd_layer.attention.attention_dropout
here
here here  language_model.encoder.layers.0.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.0.randomltd_layer.attention.denselanguage_model.encoder.layers.0.randomltd_layer.attention.dense


hereherehere  language_model.encoder.layers.0.randomltd_layer.attention.attention_dropout
here  language_model.encoder.layers.0.randomltd_layer.attention.denselanguage_model.encoder.layers.0.randomltd_layer.mlp.dense_4h_to_h

herelanguage_model.encoderhere  
language_model.encoder.layers.0.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.1here

 
hereherelanguage_model.encoder.layershere   
 language_model.encoder.layers.1chec here, 0language_model.encoder.layers.1language_model.encoder.layers.0.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.0.randomltd_layer.mlpherelanguage_model.encoder.layers.0.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.0.randomltd_layer.post_attention_layernorm




 

herehereherelanguage_model.encoder.layers.0herehere   chec here, 0
here
 language_model.encoder.layers.1.randomltd_layer
here here  language_model.encoder.layers.0.randomltd_layer language_model.encoder.layers.0.randomltd_layer.attention.denselanguage_model.encoder.layers.1.randomltd_layer.input_layernorm

language_model.encoder.layers.0.randomltd_layer.mlplanguage_model.encoder.layers.1.randomltd_layer
hereherehere
language_model.encoder.layers.0.randomltd_layer.mlp
   herelanguage_model.encoder.layers.0.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.0.randomltd_layer.input_layernorm
language_model.encoder.layers.0.randomltd_layer.post_attention_layernorm

herechec here, 0language_model.encoder.layers.1.randomltd_layer.attention 
hereherehere 

language_model.encoder.layers.0.randomltd_layer.mlp.dense_h_to_4hhere   language_model.encoder.layers.1.randomltd_layer.input_layernormherehere
 language_model.encoder.layers.0.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.0.randomltd_layer.attentionlanguage_model.encoder.layers.0.randomltd_layer.mlp
  language_model.encoder.layers.1.randomltd_layer
herehere  language_model.encoder.layers.1.randomltd_layer.input_layernormlanguage_model.encoder.layers.0.randomltd_layer.mlp.dense_4h_to_h
language_model.encoder.layers.0.randomltd_layer.mlp.dense_4h_to_h
here 
here
language_model.encoder.layers.1.randomltd_layer.attentionhere 
 
herelanguage_model.encoder.layers.1herelanguage_model.encoder.layers.1
 
here 
language_model.encoder.layers.0.randomltd_layer.mlp.dense_4h_to_hhere language_model.encoder.layers.1.randomltd_layer.attention.query_key_valuehere
language_model.encoder.layers.0.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.1.randomltd_layer.attention.query_key_value
 chec here, 0 herechec here, 0

herelanguage_model.encoder.layers.1.randomltd_layer.attention
language_model.encoder.layers.0.randomltd_layer.mlp.dense_h_to_4h 
herehere 

herelanguage_model.encoder.layers.1   language_model.encoder.layers.1.randomltd_layer.attention.scale_mask_softmaxherehere herelanguage_model.encoder.layers.1.randomltd_layer.attention.query_key_value
 here
language_model.encoder.layers.0.randomltd_layer.mlp.dense_4h_to_h language_model.encoder.layers.1.randomltd_layer.attention.scale_mask_softmax
language_model.encoder.layers.1.randomltd_layer
here 
herelanguage_model.encoder.layers.0.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.1here  
language_model.encoder.layers.1.randomltd_layer.attention.scale_mask_softmax
language_model.encoder.layers.1.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.1.randomltd_layer.input_layernormchec here, 0


here

 hereherehere herechec here, 0language_model.encoder.layers.1.randomltd_layer  here language_model.encoder.layers.0.randomltd_layer.attention.attention_dropout 

language_model.encoder.layers.1.randomltd_layer.attention.denselanguage_model.encoder.layers.1.randomltd_layer.attention.attention_dropout language_model.encoder.layers.1.randomltd_layer.attention.attention_dropout
language_model.encoder.layers.1.randomltd_layer.attentionherehere

language_model.encoder.layers.1.randomltd_layer

here language_model.encoder.layers.1.randomltd_layer.input_layernormhere
 hereherelanguage_model.encoder.layers.1.randomltd_layer.attention.dense 

 language_model.encoder.layers.1.randomltd_layer.attentionhere
language_model.encoder.layers.0.randomltd_layer.attention.dense here here
 language_model.encoder.layers.1.randomltd_layer.input_layernorm language_model.encoder.layers.1.randomltd_layer.post_attention_layernorm language_model.encoder.layers.1.randomltd_layer.attention.query_key_valuehere

herelanguage_model.encoder.layers.1.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.1.randomltd_layer
 herehereherehere

here language_model.encoder.layers.0.randomltd_layer.post_attention_layernorm     herelanguage_model.encoder.layers.1.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.1.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.1.randomltd_layer.attentionlanguage_model.encoder.layers.1.randomltd_layer.attention.denselanguage_model.encoder.layers.1.randomltd_layer.mlplanguage_model.encoder.layers.1.randomltd_layer.attention.scale_mask_softmax 
herehere language_model.encoder.layers.1.randomltd_layer.mlp 

language_model.encoder.layers.0.randomltd_layer.mlphere
here 
here language_model.encoder.layers.1.randomltd_layer.mlp.dense_h_to_4h language_model.encoder.layers.1.randomltd_layer.attention.attention_dropout
here
language_model.encoder.layers.0.randomltd_layer.mlp.dense_h_to_4h
 here
here
herelanguage_model.encoder.layers.1.randomltd_layer.attention.query_key_value 
 herehere language_model.encoder.layers.1.randomltd_layer.input_layernorm
language_model.encoder.layers.1.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.1.randomltd_layer.mlp.dense_4h_to_h here language_model.encoder.layers.1.randomltd_layer.attention.dense
here

language_model.encoder.layers.0.randomltd_layer.mlp.dense_4h_to_h language_model.encoder.layers.1.randomltd_layer.mlp.dense_h_to_4h
 herehere
herelanguage_model.encoder.layers.1.randomltd_layer.attention.attention_dropout
herelanguage_model.encoder.layers.1.randomltd_layer.attention.scale_mask_softmax  here 
here language_model.encoder.layers.1.randomltd_layer.post_attention_layernorm

here language_model.encoder.layers.1.randomltd_layer.mlphere
language_model.encoder.layers.1.randomltd_layer.attention herelanguage_model.encoder.layers.1.randomltd_layer.attention.attention_dropout
language_model.encoder.layers.1.randomltd_layer.mlphere 
 
language_model.encoder.layers.1.randomltd_layer.mlp.dense_h_to_4h herelanguage_model.encoder.layers.1.randomltd_layer.attention.query_key_valuehere
language_model.encoder.layers.1
language_model.encoder.layers.2  here
herehere
language_model.encoder.layers.1.randomltd_layer.attention.denselanguage_model.encoder.layers.1.randomltd_layer.mlp.dense_h_to_4h    

language_model.encoder.layers.1.randomltd_layer.mlp.dense_4h_to_hchec here, 0language_model.encoder.layers.1.randomltd_layer.attention.scale_mask_softmaxchec here, 1language_model.encoder.layers.1.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.1.randomltd_layer.attention.denseherehere



here

  hereherehere herehere language_model.encoder.layers.1.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.1.randomltd_layer.post_attention_layernormhere 
language_model.encoder.layers.1.randomltd_layer.mlplanguage_model.encoder.layers.1.randomltd_layer.mlp.dense_4h_to_hhere
 
 herelanguage_model.encoder.layers.1.randomltd_layer.mlpherelanguage_model.encoder.layers.2  
 
language_model.encoder.layers.1.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.1.randomltd_layer herelanguage_model.encoder.layers.2

chec here, 1language_model.encoder.layers.1.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.2.randomltd_layer 
herehere
 language_model.encoder.layers.1.randomltd_layer.mlp.dense_h_to_4h

  herechec here, 1language_model.encoder.layers.2
hereherelanguage_model.encoder.layers.1.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.1.randomltd_layer.input_layernorm 

here  

language_model.encoder.layers.2.randomltd_layer
herehere  language_model.encoder.layers.2.randomltd_layer.input_layernormlanguage_model.encoder.layers.2.randomltd_layer 

language_model.encoder.layers.1.randomltd_layer.mlp.dense_4h_to_hhereherelanguage_model.encoder.layers.1.randomltd_layer.attention.dense
  
language_model.encoder.layers.2.randomltd_layer.attentionchec here, 1herelanguage_model.encoder.layers.2.randomltd_layer.input_layernormhere
language_model.encoder.layers.2.randomltd_layer.input_layernormhere
 
 here
 herelanguage_model.encoder.layers.2hereherelanguage_model.encoder.layers.1.randomltd_layer.post_attention_layernorm herelanguage_model.encoder.layers.2.randomltd_layer.attention.query_key_value 
  
language_model.encoder.layers.2here 
language_model.encoder.layers.2.randomltd_layerlanguage_model.encoder.layers.1.randomltd_layer.attentionlanguage_model.encoder.layers.2.randomltd_layer.attention
chec here, 1 language_model.encoder.layers.2.randomltd_layer.attention
herehere  language_model.encoder.layers.2.randomltd_layer.attention.query_key_value
language_model.encoder.layers.2.randomltd_layer.attention.scale_mask_softmax

herehere
here   language_model.encoder.layers.2.randomltd_layer.input_layernorm
language_model.encoder.layers.2.randomltd_layer.attention.scale_mask_softmaxherelanguage_model.encoder.layers.2.randomltd_layer.attention.attention_dropout



 herehereherechec here, 1language_model.encoder.layers.1.randomltd_layer.attention.query_key_valueherehere language_model.encoder.layers.1.randomltd_layer.mlp  

  language_model.encoder.layers.2.randomltd_layer.attention.query_key_value
language_model.encoder.layers.2.randomltd_layer.attentionlanguage_model.encoder.layers.2.randomltd_layer.attention.densehereherelanguage_model.encoder.layers.2.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.2.randomltd_layer
here

 language_model.encoder.layers.2.randomltd_layer
 here language_model.encoder.layers.1.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.2.randomltd_layer.input_layernorm

here
 herehere
language_model.encoder.layers.1.randomltd_layer.attention.attention_dropout  
language_model.encoder.layers.2.randomltd_layer.attention.denseherelanguage_model.encoder.layers.2.randomltd_layer.attentionherehere
  
  herelanguage_model.encoder.layers.1.randomltd_layer.attention.denseherelanguage_model.encoder.layers.2.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.1.randomltd_layer.mlp.dense_h_to_4hherelanguage_model.encoder.layers.2.randomltd_layer.input_layernorm here
 

 
language_model.encoder.layers.2.randomltd_layer.post_attention_layernormhere herelanguage_model.encoder.layers.2.randomltd_layer.attention.query_key_valuehereherelanguage_model.encoder.layers.2.randomltd_layer.attention.query_key_value
 language_model.encoder.layers.2.randomltd_layer.post_attention_layernorm 
  
hereherelanguage_model.encoder.layers.2.randomltd_layer.attention
language_model.encoder.layers.1.randomltd_layer.post_attention_layernormherelanguage_model.encoder.layers.1.randomltd_layer.mlp.dense_4h_to_h
language_model.encoder.layers.2.randomltd_layer.attention.attention_dropouthere 
language_model.encoder.layers.2
here  chec here, 1language_model.encoder.layers.2.randomltd_layer.attention.denselanguage_model.encoder.layers.2.randomltd_layer.mlp 


language_model.encoder.layers.2.randomltd_layer.attention.scale_mask_softmaxherehere
here
   language_model.encoder.layers.2.randomltd_layerherehereherelanguage_model.encoder.layers.2.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.2.randomltd_layer.mlp.dense_h_to_4h

   

 herelanguage_model.encoder.layers.2.randomltd_layer.mlplanguage_model.encoder.layers.2.randomltd_layer.attention.attention_dropoutherelanguage_model.encoder.layers.2.randomltd_layer.attention.query_key_valuehereherelanguage_model.encoder.layers.2.randomltd_layer.attention.scale_mask_softmax 

 
 here 
language_model.encoder.layers.2.randomltd_layer.input_layernormhereherelanguage_model.encoder.layers.1.randomltd_layer.mlplanguage_model.encoder.layers.2.randomltd_layer.mlp language_model.encoder.layers.2.randomltd_layer.mlp.dense_4h_to_hhere
  

language_model.encoder.layers.2.randomltd_layer.attention.scale_mask_softmax
 herelanguage_model.encoder.layers.2.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.2.randomltd_layer.attention.denseherehere
hereherelanguage_model.encoder.layers.2.randomltd_layer.attention.attention_dropout language_model.encoder.layers.2.randomltd_layer.attention

herehere 
 language_model.encoder.layers.2.randomltd_layer.attention.query_key_value
language_model.encoder.layers.2.randomltd_layer.mlp.dense_4h_to_hhere here
  language_model.encoder.layers.2.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.1.randomltd_layer.mlp.dense_h_to_4h herelanguage_model.encoder.layers.2.randomltd_layer.attention.scale_mask_softmax


  language_model.encoder.layers.2.randomltd_layer.mlp.dense_h_to_4hhereherehere language_model.encoder.layers.3
language_model.encoder.layers.3 
  
language_model.encoder.layers.2.randomltd_layer.attention.attention_dropouthere
language_model.encoder.layers.2.randomltd_layer.mlpherelanguage_model.encoder.layers.1.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.2.randomltd_layer.attention.attention_dropout
chec here, 2 
 herechec here, 2

herehere
language_model.encoder.layers.2.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.2.randomltd_layer.attention.dense 
here  here

language_model.encoder.layers.2.randomltd_layer.mlp.dense_h_to_4hhere language_model.encoder.layers.2
language_model.encoder.layers.2.randomltd_layer.attention.dense
chec here, 1herelanguage_model.encoder.layers.2.randomltd_layer.attention.dense
 
language_model.encoder.layers.2.randomltd_layer.post_attention_layernorm herehere
 language_model.encoder.layers.3.randomltd_layerhere herelanguage_model.encoder.layers.2.randomltd_layer

herelanguage_model.encoder.layers.2.randomltd_layer.post_attention_layernorm  herehere

 language_model.encoder.layers.3language_model.encoder.layers.2.randomltd_layer.mlp   language_model.encoder.layers.2.randomltd_layer.post_attention_layernormherehere

language_model.encoder.layers.3.randomltd_layer.input_layernormlanguage_model.encoder.layers.3.randomltd_layerlanguage_model.encoder.layers.2.randomltd_layer.input_layernorm
 here here

here chec here, 2language_model.encoder.layers.3.randomltd_layer.input_layernorm


herehere  language_model.encoder.layers.2.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.3.randomltd_layerlanguage_model.encoder.layers.3.randomltd_layer.attentionhere

 
 hereherelanguage_model.encoder.layers.2.randomltd_layer.mlplanguage_model.encoder.layers.2.randomltd_layer.mlplanguage_model.encoder.layers.2.randomltd_layer.attentionhere  
 
 
language_model.encoder.layers.3.randomltd_layer.input_layernormlanguage_model.encoder.layers.3hereherelanguage_model.encoder.layers.2.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.3.randomltd_layer.attention.query_key_valueherehere

  

  herelanguage_model.encoder.layers.3.randomltd_layer.attentionlanguage_model.encoder.layers.2.randomltd_layer.mlp.dense_h_to_4hherechec here, 2herelanguage_model.encoder.layers.2.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.2.randomltd_layer.attention.query_key_value 

here  language_model.encoder.layers.2.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.2.randomltd_layer.mlp.dense_4h_to_h


herehere here  language_model.encoder.layers.3 language_model.encoder.layers.3
language_model.encoder.layers.3.randomltd_layer.attention.scale_mask_softmax
language_model.encoder.layers.3.randomltd_layer

chec here, 2

herehere
chec here, 2herelanguage_model.encoder.layers.3.randomltd_layer.attention  herelanguage_model.encoder.layers.2.randomltd_layer.mlp.dense_4h_to_hherehere
 
language_model.encoder.layers.3.randomltd_layer.attention.attention_dropout 
  herelanguage_model.encoder.layers.3.randomltd_layer.input_layernormhere
language_model.encoder.layers.3.randomltd_layerhereherelanguage_model.encoder.layers.2.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.3.randomltd_layer.attention.query_key_value 
 here language_model.encoder.layers.3.randomltd_layer.attention

here language_model.encoder.layers.3.randomltd_layer.attention.query_key_valuehere
  herelanguage_model.encoder.layers.3.randomltd_layer.attention.denselanguage_model.encoder.layers.3.randomltd_layer.input_layernorm  

language_model.encoder.layers.3.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.3
herehere
 

 hereherelanguage_model.encoder.layers.3.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.3.randomltd_layer.attention herechec here, 2 language_model.encoder.layers.3.randomltd_layer

language_model.encoder.layers.2.randomltd_layer.attention.attention_dropouthere language_model.encoder.layers.3.randomltd_layer.attention.query_key_value
language_model.encoder.layers.3.randomltd_layer.attention.attention_dropout
here
 language_model.encoder.layers.3.randomltd_layer.attention.scale_mask_softmax
here
here herelanguage_model.encoder.layers.3.randomltd_layer.attention.query_key_value
here here language_model.encoder.layers.3.randomltd_layer.mlp 
here language_model.encoder.layers.3.randomltd_layer language_model.encoder.layers.3.randomltd_layer.input_layernorm
language_model.encoder.layers.2.randomltd_layer.attention.densehere language_model.encoder.layers.3.randomltd_layer.attention.scale_mask_softmax
language_model.encoder.layers.3.randomltd_layer.attention.densehere
here
 language_model.encoder.layers.3.randomltd_layer.attention.scale_mask_softmax
herelanguage_model.encoder.layers.3.randomltd_layer.attention.attention_dropout language_model.encoder.layers.3.randomltd_layer.attention.attention_dropout


herehere  
herelanguage_model.encoder.layers.3.randomltd_layer.attention.denselanguage_model.encoder.layers.3.randomltd_layer.attention.dense 
 here
language_model.encoder.layers.3.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.3.randomltd_layer.input_layernormhere herehere

  language_model.encoder.layers.3.randomltd_layer.post_attention_layernorm  herehereherelanguage_model.encoder.layers.3.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.3.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.3.randomltd_layer.attention
language_model.encoder.layers.3.randomltd_layer.post_attention_layernorm   


herehere
language_model.encoder.layers.3.randomltd_layer.attention.denselanguage_model.encoder.layers.3.randomltd_layer.attentionlanguage_model.encoder.layers.2.randomltd_layer.post_attention_layernormherehere  here


  language_model.encoder.layers.3.randomltd_layer.mlplanguage_model.encoder.layers.3.randomltd_layer.attention.query_key_value hereherehere language_model.encoder.layers.3.randomltd_layer.mlplanguage_model.encoder.layers.2.randomltd_layer.mlp

hereherelanguage_model.encoder.layers.3.randomltd_layer.mlp.dense_4h_to_h  language_model.encoder.layers.2.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.3.randomltd_layer.mlp.dense_h_to_4h



herehere 
herehere  language_model.encoder.layers.4language_model.encoder.layers.3.randomltd_layer.mlp herelanguage_model.encoder.layers.2.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.3.randomltd_layer.mlp.dense_4h_to_h
 
language_model.encoder.layers.3.randomltd_layer.mlp.dense_h_to_4h 
 
language_model.encoder.layers.3.randomltd_layer.post_attention_layernorm
chec here, 3herelanguage_model.encoder.layers.3.randomltd_layer.attention.scale_mask_softmaxherelanguage_model.encoder.layers.3.randomltd_layer.attention.query_key_valuehere
here
 
 
 here herelanguage_model.encoder.layers.3.randomltd_layer.mlp.dense_h_to_4hherelanguage_model.encoder.layers.3herelanguage_model.encoder.layers.4 language_model.encoder.layers.3.randomltd_layer.mlp.dense_4h_to_h 
 language_model.encoder.layers.3.randomltd_layer.attention.attention_dropout

here language_model.encoder.layers.3.randomltd_layer.attention.dense
 herechec here, 2 language_model.encoder.layers.3.randomltd_layer.attention.scale_mask_softmax

language_model.encoder.layers.3.randomltd_layer.post_attention_layernorm

herelanguage_model.encoder.layers.3.randomltd_layer.mlpherehere chec here, 3

  language_model.encoder.layers.3.randomltd_layer
language_model.encoder.layers.3.randomltd_layer.mlplanguage_model.encoder.layers.4.randomltd_layerhereherelanguage_model.encoder.layers.3.randomltd_layer.attention.attention_dropout
herehere
 
 
here  herelanguage_model.encoder.layers.3.randomltd_layer.mlp.dense_h_to_4hherelanguage_model.encoder.layers.4here language_model.encoder.layers.3.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.4.randomltd_layer 
 
 language_model.encoder.layers.3.randomltd_layer.input_layernorm

language_model.encoder.layers.3.randomltd_layer.mlp.dense_h_to_4hherelanguage_model.encoder.layers.4.randomltd_layer.input_layernormlanguage_model.encoder.layers.3.randomltd_layer.attention.dense
herehere language_model.encoder.layers.3.randomltd_layer.attention 
language_model.encoder.layers.4chec here, 3here
 
herelanguage_model.encoder.layers.3.randomltd_layer.attention.query_key_valueherechec here, 3
 
 
herelanguage_model.encoder.layers.4.randomltd_layerlanguage_model.encoder.layers.4.randomltd_layer.input_layernorm herehere 


here   
herelanguage_model.encoder.layers.3.randomltd_layer.mlp.dense_4h_to_h language_model.encoder.layers.3.randomltd_layer.post_attention_layernorm

language_model.encoder.layers.3.randomltd_layer.attention.scale_mask_softmaxherehere  
herelanguage_model.encoder.layers.3.randomltd_layer.mlplanguage_model.encoder.layers.4here

  language_model.encoder.layers.4.randomltd_layerherelanguage_model.encoder.layers.4.randomltd_layer.input_layernormlanguage_model.encoder.layers.3.randomltd_layer.attention.attention_dropoutchec here, 3here 



language_model.encoder.layers.4.randomltd_layer.attention language_model.encoder.layers.3.randomltd_layer.mlp.dense_h_to_4hherehereherelanguage_model.encoder.layers.3.randomltd_layer.mlp.dense_4h_to_hhere
language_model.encoder.layers.4.randomltd_layer.attention
   
 here
herelanguage_model.encoder.layers.4.randomltd_layer.input_layernormlanguage_model.encoder.layers.4.randomltd_layer.attentionlanguage_model.encoder.layers.3.randomltd_layer.attention.denseherelanguage_model.encoder.layers.4.randomltd_layer here  language_model.encoder.layers.4.randomltd_layer.attention.query_key_value
language_model.encoder.layers.3.randomltd_layer.mlp.dense_4h_to_hhere

 language_model.encoder.layers.4.randomltd_layer.attention.scale_mask_softmaxhere

here here 
language_model.encoder.layers.4.randomltd_layer.attentionhere language_model.encoder.layers.4
  herelanguage_model.encoder.layers.4.randomltd_layer.attention.attention_dropout 

herelanguage_model.encoder.layers.4language_model.encoder.layers.4.randomltd_layer.attention.query_key_value
language_model.encoder.layers.3.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.4.randomltd_layer.attention.query_key_value
 here
chec here, 3chec here, 3here

language_model.encoder.layers.4.randomltd_layer.attention.query_key_value here

 here
herelanguage_model.encoder.layers.4.randomltd_layer.input_layernorm hereherelanguage_model.encoder.layers.4.randomltd_layer.attention.dense here language_model.encoder.layers.4.randomltd_layer.attention.scale_mask_softmax

here language_model.encoder.layers.4.randomltd_layer.attention.attention_dropouthere
language_model.encoder.layers.4.randomltd_layer.attention.scale_mask_softmax here 
language_model.encoder.layers.4.randomltd_layer.attentionlanguage_model.encoder.layers.4.randomltd_layer.attention.dense here

 herelanguage_model.encoder.layers.4.randomltd_layer herelanguage_model.encoder.layers.4.randomltd_layer.attention.attention_dropout 
 
language_model.encoder.layers.4.randomltd_layer
language_model.encoder.layers.4.randomltd_layer.post_attention_layernormherelanguage_model.encoder.layers.4.randomltd_layer.attention.query_key_value
language_model.encoder.layers.3.randomltd_layer.mlpherehere
 
 here  
herelanguage_model.encoder.layers.4.randomltd_layer.input_layernormherelanguage_model.encoder.layers.4.randomltd_layer.attention.scale_mask_softmax language_model.encoder.layers.4.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.4.randomltd_layer.attention.dense 
here 
language_model.encoder.layers.4.randomltd_layer.input_layernormhere

language_model.encoder.layers.4.randomltd_layer.mlphere language_model.encoder.layers.4.randomltd_layer.attention.scale_mask_softmax
 herehere
 language_model.encoder.layers.3.randomltd_layer.mlp.dense_h_to_4h
herelanguage_model.encoder.layers.4.randomltd_layer.attention.attention_dropout  herelanguage_model.encoder.layers.4.randomltd_layer.attention
here 
language_model.encoder.layers.4.randomltd_layer.mlplanguage_model.encoder.layers.4.randomltd_layer.post_attention_layernorm 
here language_model.encoder.layers.4.randomltd_layer.attention.attention_dropout
language_model.encoder.layers.4.randomltd_layer.attentionhere 
language_model.encoder.layers.4.randomltd_layer.attention.denseherehere
 here
 language_model.encoder.layers.4.randomltd_layer.attention.query_key_value language_model.encoder.layers.4.randomltd_layer.attention.dense
here
language_model.encoder.layers.4.randomltd_layer.post_attention_layernorm
 language_model.encoder.layers.4.randomltd_layer.mlp.dense_h_to_4hherehere
hereherelanguage_model.encoder.layers.4.randomltd_layer.mlp.dense_h_to_4h 
  here 
 herelanguage_model.encoder.layers.4.randomltd_layer.attention.scale_mask_softmaxherelanguage_model.encoder.layers.4.randomltd_layer.mlplanguage_model.encoder.layers.3.randomltd_layer.mlp.dense_4h_to_h language_model.encoder.layers.4.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.4.randomltd_layer.attention.query_key_value 
 

language_model.encoder.layers.4.randomltd_layer.mlp

language_model.encoder.layers.4.randomltd_layer.mlp.dense_4h_to_hhereherelanguage_model.encoder.layers.4.randomltd_layer.mlp.dense_4h_to_h
herehere  language_model.encoder.layers.5language_model.encoder.layers.4.randomltd_layer.mlp.dense_h_to_4h


hereherehere chec here, 4 language_model.encoder.layers.4.randomltd_layer.mlp.dense_h_to_4h 
herelanguage_model.encoder.layers.4.randomltd_layer.mlp.dense_4h_to_h
language_model.encoder.layers.4here
 
here
  language_model.encoder.layers.4.randomltd_layer.mlpherehere language_model.encoder.layers.5.randomltd_layer 
language_model.encoder.layers.4.randomltd_layer.attention.scale_mask_softmax chec here, 3 language_model.encoder.layers.4.randomltd_layer.mlp.dense_4h_to_h
language_model.encoder.layers.4.randomltd_layer.attention.attention_dropouthere
language_model.encoder.layers.5
language_model.encoder.layers.5
here
 here

here language_model.encoder.layers.4.randomltd_layerhere
 here language_model.encoder.layers.5 language_model.encoder.layers.5.randomltd_layer.input_layernorm
language_model.encoder.layers.4.randomltd_layer.input_layernormhere

chec here, 4 chec here, 4herehere
 language_model.encoder.layers.4.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.4.randomltd_layer.attention.dense 
herelanguage_model.encoder.layers.4.randomltd_layer.attention 

language_model.encoder.layers.5.randomltd_layer.attentionherechec here, 4 
language_model.encoder.layers.4.randomltd_layer.attention.attention_dropoutherehere
 
language_model.encoder.layers.5.randomltd_layerhere
  herelanguage_model.encoder.layers.5.randomltd_layerhere
 herelanguage_model.encoder.layers.4.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.4.randomltd_layer.post_attention_layernorm
 here language_model.encoder.layers.5.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.4.randomltd_layer.mlp


herehere here  language_model.encoder.layers.5.randomltd_layerhere language_model.encoder.layers.5.randomltd_layer.input_layernormlanguage_model.encoder.layers.5.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.4.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.4.randomltd_layer.attention.query_key_value
 

 

language_model.encoder.layers.5.randomltd_layer.input_layernormherehere
herelanguage_model.encoder.layers.4.randomltd_layer.attention.denseherehere
  here 
  herelanguage_model.encoder.layers.5.randomltd_layer.input_layernormlanguage_model.encoder.layers.5.randomltd_layer.attention language_model.encoder.layers.5.randomltd_layer.attention.attention_dropoutherelanguage_model.encoder.layers.4.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.4.randomltd_layer.attention.scale_mask_softmax 

herelanguage_model.encoder.layers.5
 

herelanguage_model.encoder.layers.5.randomltd_layer.attention language_model.encoder.layers.4.randomltd_layer.attention.attention_dropout

hereherehere    language_model.encoder.layers.5.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.5.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.4.randomltd_layer.attention.dense


language_model.encoder.layers.5.randomltd_layer.attention
herehere
here here herelanguage_model.encoder.layers.4.randomltd_layer.post_attention_layernormchec here, 4 language_model.encoder.layers.5.randomltd_layer.attention.scale_mask_softmaxhere language_model.encoder.layers.5.randomltd_layer.attention.scale_mask_softmax language_model.encoder.layers.5.randomltd_layer.attention.dense


 language_model.encoder.layers.4.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.5.randomltd_layer.attention.query_key_value
herehereherelanguage_model.encoder.layers.5
here
here   language_model.encoder.layers.5.randomltd_layer.attention.attention_dropout

here language_model.encoder.layers.5.randomltd_layer.attention.dense
herechec here, 4here
   herelanguage_model.encoder.layers.4.randomltd_layer.mlplanguage_model.encoder.layers.5.randomltd_layer.attention.attention_dropouthere language_model.encoder.layers.5.randomltd_layer.post_attention_layernorm

 language_model.encoder.layers.5.randomltd_layer 
herelanguage_model.encoder.layers.4.randomltd_layer.mlpherelanguage_model.encoder.layers.5.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.5.randomltd_layer.attention.scale_mask_softmaxhere 
language_model.encoder.layers.4.randomltd_layer.mlp.dense_h_to_4h 
language_model.encoder.layers.5.randomltd_layer.attention.densehere
 here
here
 herelanguage_model.encoder.layers.5.randomltd_layer.mlp here herelanguage_model.encoder.layers.5.randomltd_layer.input_layernorm language_model.encoder.layers.5.randomltd_layer
here language_model.encoder.layers.5.randomltd_layer.input_layernorm

herehere language_model.encoder.layers.4.randomltd_layer.mlp.dense_h_to_4h language_model.encoder.layers.5.randomltd_layer.attentionlanguage_model.encoder.layers.5.randomltd_layer.mlp.dense_h_to_4h
 here

 language_model.encoder.layers.4.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.5.randomltd_layer.mlphereherelanguage_model.encoder.layers.4.randomltd_layer.mlp.dense_4h_to_h 
  

language_model.encoder.layers.5.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.5.randomltd_layer.mlp.dense_4h_to_h
herelanguage_model.encoder.layers.5.randomltd_layer.attention.query_key_valuehereherelanguage_model.encoder.layers.5.randomltd_layer.attention.attention_dropout

 
here  
hereherelanguage_model.encoder.layers.5here language_model.encoder.layers.5.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.5here  
 language_model.encoder.layers.5.randomltd_layer.attention

 language_model.encoder.layers.5.randomltd_layer.mlplanguage_model.encoder.layers.6language_model.encoder.layers.5.randomltd_layer.attention.scale_mask_softmax
herechec here, 4language_model.encoder.layers.5.randomltd_layer.attention.dense

chec here, 4
here
 language_model.encoder.layers.5.randomltd_layerhere
herehere   language_model.encoder.layers.5.randomltd_layer.attention.attention_dropout language_model.encoder.layers.5.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.5.randomltd_layer.input_layernormlanguage_model.encoder.layers.5.randomltd_layer.mlp.dense_4h_to_h




here
herehereherehere  here   herelanguage_model.encoder.layers.5.randomltd_layer.attention.densechec here, 5language_model.encoder.layers.6language_model.encoder.layers.5.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.5.randomltd_layer.attention language_model.encoder.layers.5.randomltd_layer 




language_model.encoder.layers.5.randomltd_layer.mlp.dense_h_to_4h
language_model.encoder.layers.5.randomltd_layer.post_attention_layernormhereherehereherechec here, 5
here
here language_model.encoder.layers.5.randomltd_layer.mlp 
language_model.encoder.layers.5.randomltd_layer.post_attention_layernormhere
  herelanguage_model.encoder.layers.5.randomltd_layer.mlp.dense_h_to_4h language_model.encoder.layers.6.randomltd_layer 
language_model.encoder.layers.5.randomltd_layer.mlp
language_model.encoder.layers.5.randomltd_layer.attention.attention_dropouthere here

  
language_model.encoder.layers.5.randomltd_layer.attention.query_key_valuehereherelanguage_model.encoder.layers.5.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.6.randomltd_layer.input_layernormhere
 here 
 
 language_model.encoder.layers.5.randomltd_layer.mlp.dense_h_to_4hhere language_model.encoder.layers.5.randomltd_layer.attention.denseherelanguage_model.encoder.layers.5.randomltd_layer.input_layernormherelanguage_model.encoder.layers.5.randomltd_layer.mlp.dense_4h_to_h
 language_model.encoder.layers.6.randomltd_layer
 
 
herelanguage_model.encoder.layers.5.randomltd_layer.attention.scale_mask_softmax
language_model.encoder.layers.6hereherelanguage_model.encoder.layers.6.randomltd_layer.attentionhere language_model.encoder.layers.5.randomltd_layer.mlp.dense_4h_to_h

here language_model.encoder.layers.6here
here language_model.encoder.layers.5.randomltd_layer.attention.attention_dropout chec here, 5

language_model.encoder.layers.6.randomltd_layer.input_layernorm

here herehere chec here, 5language_model.encoder.layers.5.randomltd_layer.post_attention_layernorm   language_model.encoder.layers.5.randomltd_layer.attention.dense


language_model.encoder.layers.5.randomltd_layer.attentionlanguage_model.encoder.layers.6.randomltd_layerlanguage_model.encoder.layers.6.randomltd_layer.attention
herehere
here 

 herehere here language_model.encoder.layers.6herelanguage_model.encoder.layers.6.randomltd_layer  language_model.encoder.layers.5.randomltd_layer.mlp language_model.encoder.layers.6.randomltd_layer.attention.query_key_value
 
language_model.encoder.layers.6.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.5.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.5.randomltd_layer.attention.query_key_value
language_model.encoder.layers.6.randomltd_layer.input_layernormhere

here language_model.encoder.layers.5.randomltd_layer.mlp.dense_h_to_4h

here language_model.encoder.layers.5.randomltd_layer.mlp.dense_4h_to_hhere
chec here, 5 here
 language_model.encoder.layers.5.randomltd_layer.attention.scale_mask_softmaxhereherelanguage_model.encoder.layers.6
 here 

language_model.encoder.layers.6.randomltd_layer language_model.encoder.layers.6.randomltd_layer.attention.scale_mask_softmax
 herelanguage_model.encoder.layers.5.randomltd_layer.attention.attention_dropout
 chec here, 5herehere
language_model.encoder.layers.6.randomltd_layer.input_layernormhereherelanguage_model.encoder.layers.6.randomltd_layer.attention
  here
  
herelanguage_model.encoder.layers.6.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.6.randomltd_layer.input_layernorm herelanguage_model.encoder.layers.6.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.5.randomltd_layer.mlphere 
here language_model.encoder.layers.6.randomltd_layer.attention.attention_dropout

herehere language_model.encoder.layers.5.randomltd_layer.attention.dense language_model.encoder.layers.6.randomltd_layer.attention.denselanguage_model.encoder.layers.6.randomltd_layer.attention
 

herelanguage_model.encoder.layers.6.randomltd_layer.attentionhere
here 
 
 language_model.encoder.layers.5.randomltd_layer.post_attention_layernormhereherelanguage_model.encoder.layers.6.randomltd_layer.post_attention_layernorm 
language_model.encoder.layers.6.randomltd_layer.attention.query_key_value herelanguage_model.encoder.layers.6.randomltd_layer 

hereherelanguage_model.encoder.layers.6.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.6.randomltd_layer.attention.dense 
language_model.encoder.layers.6.randomltd_layer.attention.query_key_valuehere  

language_model.encoder.layers.5.randomltd_layer.mlp.dense_h_to_4hhere
 language_model.encoder.layers.6.randomltd_layer.mlplanguage_model.encoder.layers.5.randomltd_layer.mlpherehere
 herelanguage_model.encoder.layers.6.randomltd_layer.attention.scale_mask_softmax

herehere  herelanguage_model.encoder.layers.6.randomltd_layer.input_layernorm 
here  language_model.encoder.layers.6.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.6.randomltd_layer.mlp.dense_h_to_4h
 here
language_model.encoder.layers.5.randomltd_layer.mlp.dense_h_to_4hhere language_model.encoder.layers.6.randomltd_layer.attention.scale_mask_softmax
 language_model.encoder.layers.6.randomltd_layer.attention.dense
language_model.encoder.layers.6.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.6.randomltd_layer.mlp.dense_4h_to_hhere
here 

 here 
language_model.encoder.layers.5.randomltd_layer.mlp.dense_4h_to_hhereherelanguage_model.encoder.layers.5.randomltd_layer.mlp.dense_4h_to_h language_model.encoder.layers.6.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.6.randomltd_layer.attention.attention_dropout
 here 
language_model.encoder.layers.6.randomltd_layer.post_attention_layernorm

language_model.encoder.layers.7here language_model.encoder.layers.6.randomltd_layer.mlphere
herehere
 language_model.encoder.layers.6.randomltd_layer.attention
 herehere  language_model.encoder.layers.6
chec here, 6
language_model.encoder.layers.6here 
language_model.encoder.layers.7.randomltd_layer 
language_model.encoder.layers.6.randomltd_layer.mlp.dense_h_to_4hhere
 chec here, 5 language_model.encoder.layers.6.randomltd_layer.mlpherelanguage_model.encoder.layers.6.randomltd_layer.attention.attention_dropout
language_model.encoder.layers.7.randomltd_layer.input_layernorm

 language_model.encoder.layers.6.randomltd_layer.attention.dense
hereherehere
language_model.encoder.layers.6.randomltd_layer.mlp.dense_4h_to_h
 here here chec here, 5
herelanguage_model.encoder.layers.6.randomltd_layer language_model.encoder.layers.6.randomltd_layer.mlp.dense_h_to_4h language_model.encoder.layers.7.randomltd_layer.attention
 here
language_model.encoder.layers.6.randomltd_layer.attention.dense
language_model.encoder.layers.6.randomltd_layer.attention.query_key_valuehere
herelanguage_model.encoder.layers.6.randomltd_layer.post_attention_layernorm here

 language_model.encoder.layers.6.randomltd_layer.mlp.dense_4h_to_hhere
 herelanguage_model.encoder.layers.7.randomltd_layer.attention.query_key_value  
language_model.encoder.layers.6.randomltd_layerlanguage_model.encoder.layers.7here


 hereherelanguage_model.encoder.layers.7language_model.encoder.layers.7.randomltd_layer.attention.scale_mask_softmax chec here, 6 
 
language_model.encoder.layers.6.randomltd_layer.mlp
language_model.encoder.layers.6.randomltd_layer.input_layernormhereherelanguage_model.encoder.layers.6.randomltd_layer.input_layernorm
here
 herechec here, 6 
here herelanguage_model.encoder.layers.7.randomltd_layer.attention.attention_dropout 
language_model.encoder.layers.6.randomltd_layer.post_attention_layernormhere language_model.encoder.layers.7.randomltd_layer 
language_model.encoder.layers.6.randomltd_layer.attention.scale_mask_softmaxhere
 language_model.encoder.layers.6.randomltd_layer.mlp.dense_h_to_4h
language_model.encoder.layers.6.randomltd_layer.attentionherehere
 herelanguage_model.encoder.layers.6.randomltd_layer.attention
here
 language_model.encoder.layers.6.randomltd_layer.attention.query_key_value
herehere
  language_model.encoder.layers.6.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.6.randomltd_layer.attention.scale_mask_softmax here

language_model.encoder.layers.7.randomltd_layer.input_layernorm  hereherelanguage_model.encoder.layers.7.randomltd_layer.attention.dense
language_model.encoder.layers.6.randomltd_layer.attention.query_key_valuehere  
 herelanguage_model.encoder.layers.7.randomltd_layerhere
language_model.encoder.layers.7language_model.encoder.layers.6.randomltd_layer.attention.attention_dropout language_model.encoder.layers.6.randomltd_layer.attention.attention_dropout 
 here

language_model.encoder.layers.6.randomltd_layer.mlphere
language_model.encoder.layers.7.randomltd_layer.post_attention_layernormherelanguage_model.encoder.layers.7.randomltd_layer.attention 
 chec here, 6here
 language_model.encoder.layers.7.randomltd_layer.input_layernorm

here language_model.encoder.layers.7.randomltd_layer.attentionhere
language_model.encoder.layers.6.randomltd_layer.attention.scale_mask_softmaxhere  
herelanguage_model.encoder.layers.7.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.7.randomltd_layer.attention.query_key_value
herelanguage_model.encoder.layers.6.randomltd_layer.attention.dense 
here 
language_model.encoder.layers.6.randomltd_layer.mlp.dense_h_to_4h
here  language_model.encoder.layers.6.randomltd_layer.attention.attention_dropout
herehere herelanguage_model.encoder.layers.7.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.6.randomltd_layer.attention.dense
here  language_model.encoder.layers.7.randomltd_layer.attention.scale_mask_softmax 

here language_model.encoder.layers.7.randomltd_layerlanguage_model.encoder.layers.6.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.7.randomltd_layer.mlpherehere language_model.encoder.layers.6.randomltd_layer.mlp.dense_4h_to_h

here 
language_model.encoder.layers.7.randomltd_layer.attention.attention_dropout
here here language_model.encoder.layers.7.randomltd_layer.mlp.dense_h_to_4h 
language_model.encoder.layers.7.randomltd_layer.attention.attention_dropout language_model.encoder.layers.7.randomltd_layer.attention.densehere
language_model.encoder.layers.6.randomltd_layer.post_attention_layernorm language_model.encoder.layers.6.randomltd_layer.attention.dense
here
language_model.encoder.layers.7.randomltd_layer.mlp.dense_4h_to_h

 herehere
hereherelanguage_model.encoder.layers.7.randomltd_layer.attention.denseherehere here   
  language_model.encoder.layers.7.randomltd_layer.post_attention_layernorm language_model.encoder.layers.6.randomltd_layer.mlplanguage_model.encoder.layers.7.randomltd_layer.input_layernormlanguage_model.encoder.layers.8herelanguage_model.encoder.layers.6.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.7
language_model.encoder.layers.6.randomltd_layer.mlp


 

here
hereherelanguage_model.encoder.layers.7.randomltd_layer.post_attention_layernormherechec here, 7 language_model.encoder.layers.7.randomltd_layer.mlp
herehere  language_model.encoder.layers.7.randomltd_layer.mlp.dense_h_to_4h language_model.encoder.layers.6.randomltd_layer.mlp.dense_h_to_4h
language_model.encoder.layers.6.randomltd_layer.mlp.dense_h_to_4h
herechec here, 6
here 
 here language_model.encoder.layers.7.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.6.randomltd_layer.mlp.dense_4h_to_h
here
 language_model.encoder.layers.7.randomltd_layer.attention
here  language_model.encoder.layers.6.randomltd_layer.mlp.dense_4h_to_h
herehere
 language_model.encoder.layers.6.randomltd_layer.mlp
language_model.encoder.layers.7.randomltd_layerherehere  herelanguage_model.encoder.layers.8

here  language_model.encoder.layers.7.randomltd_layer.mlplanguage_model.encoder.layers.7 
here language_model.encoder.layers.7language_model.encoder.layers.7.randomltd_layer.attention.query_key_value

language_model.encoder.layers.8.randomltd_layer language_model.encoder.layers.6.randomltd_layer.mlp.dense_h_to_4h
chec here, 7
here
language_model.encoder.layers.7.randomltd_layer.input_layernorm

here language_model.encoder.layers.7.randomltd_layer.attentionhere
 chec here, 6herelanguage_model.encoder.layers.6.randomltd_layer.mlp.dense_4h_to_h 


language_model.encoder.layers.7.randomltd_layer.attention.query_key_valueherehere
  herehereherelanguage_model.encoder.layers.7language_model.encoder.layers.7.randomltd_layer    
chec here, 6
language_model.encoder.layers.7.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.7.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.8.randomltd_layerchec here, 6herelanguage_model.encoder.layers.7.randomltd_layer.attention.scale_mask_softmax

here

here
 
herehere here herelanguage_model.encoder.layers.8.randomltd_layer.input_layernormhere  language_model.encoder.layers.7.randomltd_layer.input_layernorm language_model.encoder.layers.7.randomltd_layer.attention.attention_dropout 
 language_model.encoder.layers.7.randomltd_layerlanguage_model.encoder.layers.7.randomltd_layer.mlp.dense_4h_to_h
language_model.encoder.layers.8.randomltd_layer.input_layernormhere
language_model.encoder.layers.7.randomltd_layerherelanguage_model.encoder.layers.7.randomltd_layer.attention.attention_dropout

here 
language_model.encoder.layers.8here 
 language_model.encoder.layers.7.randomltd_layer.attentionlanguage_model.encoder.layers.8.randomltd_layer.attention
here
chec here, 7here
 
here herelanguage_model.encoder.layers.7.randomltd_layer.attention.dense  language_model.encoder.layers.7.randomltd_layer.attention.query_key_value here

language_model.encoder.layers.8.randomltd_layer.attention.query_key_value
language_model.encoder.layers.8.randomltd_layer.attentionlanguage_model.encoder.layers.8.randomltd_layer herehere
herehere

language_model.encoder.layers.7.randomltd_layer.input_layernormhere  here  here
 language_model.encoder.layers.7.randomltd_layer.input_layernormlanguage_model.encoder.layers.7.randomltd_layer.post_attention_layernorm language_model.encoder.layers.7.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.7.randomltd_layer.attention.dense language_model.encoder.layers.8.randomltd_layer.attention.query_key_value
herehere  language_model.encoder.layers.8.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.7.randomltd_layer.attentionlanguage_model.encoder.layers.8.randomltd_layer.input_layernorm


here
herehere   
herelanguage_model.encoder.layers.8.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.7.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.8.randomltd_layer.attention 
language_model.encoder.layers.8.randomltd_layer.attention.scale_mask_softmaxhere

language_model.encoder.layers.7.randomltd_layer.attention
here 
language_model.encoder.layers.7.randomltd_layer.mlp
herehere
 herehere
here  herelanguage_model.encoder.layers.8.randomltd_layer.attention.dense  here language_model.encoder.layers.7.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.8.randomltd_layer.attention.query_key_value language_model.encoder.layers.7.randomltd_layer.attention.query_key_value

herehere language_model.encoder.layers.7.randomltd_layer.attention.attention_dropout language_model.encoder.layers.7.randomltd_layer.attention.scale_mask_softmax
language_model.encoder.layers.8.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.8.randomltd_layer.attention.attention_dropout
here
 
herehere   language_model.encoder.layers.7.randomltd_layer.attention.attention_dropoutherelanguage_model.encoder.layers.7.randomltd_layer.attention.denselanguage_model.encoder.layers.8.randomltd_layer.mlp
language_model.encoder.layers.7.randomltd_layer.post_attention_layernorm language_model.encoder.layers.7.randomltd_layer.mlp.dense_h_to_4h

here
language_model.encoder.layers.8.randomltd_layer.attention.dense

herehere
 
hereherehere  herelanguage_model.encoder.layers.7.randomltd_layer.attention.densehere   language_model.encoder.layers.7.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.8.randomltd_layer.mlp.dense_h_to_4h 
 language_model.encoder.layers.8.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.7.randomltd_layer.mlp

herelanguage_model.encoder.layers.7.randomltd_layer.attention.attention_dropout herelanguage_model.encoder.layers.8.randomltd_layer.mlp
 
language_model.encoder.layers.7.randomltd_layer.mlp.dense_4h_to_hherelanguage_model.encoder.layers.7.randomltd_layer.mlp.dense_h_to_4hhere
 

 herelanguage_model.encoder.layers.7.randomltd_layer.attention.denselanguage_model.encoder.layers.8.randomltd_layer.mlp.dense_h_to_4hhere
 
here
 language_model.encoder.layers.7.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.8.randomltd_layer.attention.scale_mask_softmaxherehere hereherelanguage_model.encoder.layers.8
 
 language_model.encoder.layers.7.randomltd_layer.mlp  
herelanguage_model.encoder.layers.7.randomltd_layer.post_attention_layernormherelanguage_model.encoder.layers.8.randomltd_layer.mlp.dense_4h_to_h
language_model.encoder.layers.8.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.7.randomltd_layer.post_attention_layernorm 
 herechec here, 7
here

language_model.encoder.layers.8herelanguage_model.encoder.layers.8.randomltd_layer.attention.attention_dropout 
here here
 
language_model.encoder.layers.7.randomltd_layer.mlphere language_model.encoder.layers.7.randomltd_layer.mlp.dense_h_to_4h language_model.encoder.layers.7.randomltd_layer.mlpherechec here, 7
 language_model.encoder.layers.9
herelanguage_model.encoder.layers.9 language_model.encoder.layers.7.randomltd_layer.mlp.dense_4h_to_h


here chec here, 8here language_model.encoder.layers.8
 language_model.encoder.layers.8.randomltd_layer.attention.denselanguage_model.encoder.layers.7.randomltd_layer.mlp.dense_h_to_4h



herehereherehere chec here, 7 language_model.encoder.layers.8.randomltd_layer  language_model.encoder.layers.8.randomltd_layer
language_model.encoder.layers.8.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.7.randomltd_layer.mlp.dense_4h_to_h

language_model.encoder.layers.7.randomltd_layer.mlp.dense_h_to_4hhere
here


 chec here, 8here hereherehereherelanguage_model.encoder.layers.8.randomltd_layer.input_layernorm
 language_model.encoder.layers.8.randomltd_layer    
herelanguage_model.encoder.layers.8.randomltd_layer.input_layernorm
language_model.encoder.layers.9.randomltd_layerlanguage_model.encoder.layers.8.randomltd_layer.mlplanguage_model.encoder.layers.8language_model.encoder.layers.7.randomltd_layer.mlp.dense_4h_to_hhere 
here



 language_model.encoder.layers.9.randomltd_layerhere herehereherelanguage_model.encoder.layers.8.randomltd_layer.attention
chec here, 7 language_model.encoder.layers.8.randomltd_layer.input_layernorm  language_model.encoder.layers.8.randomltd_layer.mlp.dense_h_to_4h 
language_model.encoder.layers.8here

 language_model.encoder.layers.8.randomltd_layer.mlp.dense_4h_to_hhereherechec here, 7
  
language_model.encoder.layers.8.randomltd_layer.attention.query_key_value
herelanguage_model.encoder.layers.9.randomltd_layer.input_layernormhere
 here
 language_model.encoder.layers.8.randomltd_layer.attentionherelanguage_model.encoder.layers.9 here
language_model.encoder.layers.8.randomltd_layer 
here
language_model.encoder.layers.9.randomltd_layer.input_layernormlanguage_model.encoder.layers.8.randomltd_layer 
language_model.encoder.layers.8.randomltd_layer.attention.scale_mask_softmaxhere 

language_model.encoder.layers.9.randomltd_layer.attentionchec here, 8here
 language_model.encoder.layers.8.randomltd_layer.attentionherehere

 herelanguage_model.encoder.layers.8.randomltd_layer.attention.query_key_value
 here hereherelanguage_model.encoder.layers.8.randomltd_layer.input_layernorm 
language_model.encoder.layers.9.randomltd_layer.attention language_model.encoder.layers.8.randomltd_layer.input_layernorm  
language_model.encoder.layers.8.randomltd_layer.attention.attention_dropouthere
language_model.encoder.layers.8.randomltd_layer.attention.query_key_value

here language_model.encoder.layers.8.randomltd_layer.attention.scale_mask_softmaxhere
language_model.encoder.layers.9.randomltd_layer.attention.query_key_value herelanguage_model.encoder.layers.8.randomltd_layer.attention 

language_model.encoder.layers.9.randomltd_layerlanguage_model.encoder.layers.8.randomltd_layer.attention.attention_dropoutherehere 
here
 language_model.encoder.layers.9.randomltd_layer.attention.scale_mask_softmaxhere 
language_model.encoder.layers.8.randomltd_layer.attention.query_key_valuehere
 language_model.encoder.layers.8.randomltd_layer.attention 
 hereherelanguage_model.encoder.layers.9.randomltd_layer.input_layernormhere
language_model.encoder.layers.8.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.8.randomltd_layer.attention.densehere  
 here

 language_model.encoder.layers.8.randomltd_layer.attention.denselanguage_model.encoder.layers.9.randomltd_layer.attention.attention_dropoutherelanguage_model.encoder.layers.9.randomltd_layer.attention.query_key_value herehere language_model.encoder.layers.8.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.8.randomltd_layer.attention.scale_mask_softmaxhere 
language_model.encoder.layers.8.randomltd_layer.mlp
here
 herehere
language_model.encoder.layers.8.randomltd_layer.attention.attention_dropout  here 
language_model.encoder.layers.8.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.8.randomltd_layer.mlp.dense_h_to_4h 

language_model.encoder.layers.9.randomltd_layer.attentionhere
language_model.encoder.layers.9.randomltd_layer.attention.denselanguage_model.encoder.layers.8.randomltd_layer.attention.query_key_valuehere
here  here
 
here language_model.encoder.layers.8.randomltd_layer.attention.denselanguage_model.encoder.layers.8.randomltd_layer.attention.attention_dropout herelanguage_model.encoder.layers.8.randomltd_layer.mlphere language_model.encoder.layers.9.randomltd_layer.attention.scale_mask_softmax

language_model.encoder.layers.8.randomltd_layer.mlp.dense_4h_to_h 
 herelanguage_model.encoder.layers.9.randomltd_layer.attention.query_key_value
here
herelanguage_model.encoder.layers.9.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.8.randomltd_layer.attention.scale_mask_softmax 
here here 
here
 language_model.encoder.layers.9.randomltd_layer.mlp
herehere language_model.encoder.layers.8.randomltd_layer.mlp.dense_h_to_4h language_model.encoder.layers.8.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.9.randomltd_layer.mlp.dense_h_to_4h


herehereherehere     language_model.encoder.layers.8.randomltd_layer.attention.denselanguage_model.encoder.layers.8.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.9.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.9.randomltd_layer.mlp.dense_4h_to_h
language_model.encoder.layers.8.randomltd_layer.post_attention_layernormhere
language_model.encoder.layers.9.randomltd_layer.attention.attention_dropout

 
 here
language_model.encoder.layers.8.randomltd_layer.attention.denseherehereherelanguage_model.encoder.layers.9language_model.encoder.layers.8.randomltd_layer.post_attention_layernormhere 
   

 language_model.encoder.layers.9herelanguage_model.encoder.layers.9.randomltd_layer.attention.denselanguage_model.encoder.layers.9.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.10
here chec here, 9language_model.encoder.layers.8.randomltd_layer.mlp
language_model.encoder.layers.8.randomltd_layer.mlp
here
 here
herelanguage_model.encoder.layers.10.randomltd_layer  
chec here, 8language_model.encoder.layers.8.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.8.randomltd_layer.mlp.dense_h_to_4hchec here, 8here 



 
language_model.encoder.layers.8.randomltd_layer.post_attention_layernormhereherehereherelanguage_model.encoder.layers.10.randomltd_layer.input_layernorm

 here   
hereherelanguage_model.encoder.layers.8.randomltd_layer.mlp.dense_4h_to_h language_model.encoder.layers.9.randomltd_layerlanguage_model.encoder.layers.8.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.9.randomltd_layer
herehere  language_model.encoder.layers.9.randomltd_layer.input_layernormlanguage_model.encoder.layers.10.randomltd_layer.attention
 here
 language_model.encoder.layers.9.randomltd_layer.attention.densehere language_model.encoder.layers.9.randomltd_layer.attention
 
language_model.encoder.layers.8.randomltd_layer.mlp
herelanguage_model.encoder.layers.10.randomltd_layer.attention.query_key_valuehere
 language_model.encoder.layers.9.randomltd_layer.post_attention_layernorm
 herehere
language_model.encoder.layers.9.randomltd_layer.post_attention_layernorm
herehere
language_model.encoder.layers.9.randomltd_layer.attention.query_key_value  
here  here
language_model.encoder.layers.9language_model.encoder.layers.8.randomltd_layer.mlp.dense_h_to_4hhere language_model.encoder.layers.10.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.9.randomltd_layer.input_layernorm here

 language_model.encoder.layers.9.randomltd_layer.mlp

herehere language_model.encoder.layers.9.randomltd_layer.attention
language_model.encoder.layers.9here language_model.encoder.layers.9.randomltd_layer.attention.query_key_value

 here language_model.encoder.layers.9.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.9.randomltd_layer.attention.scale_mask_softmaxherechec here, 8

language_model.encoder.layers.9.randomltd_layer.mlp
 herehere

language_model.encoder.layers.8.randomltd_layer.mlp.dense_4h_to_hhere  chec here, 8here
 herelanguage_model.encoder.layers.9.randomltd_layer.attention.attention_dropout language_model.encoder.layers.9.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.10.randomltd_layer.attention.attention_dropout 
herelanguage_model.encoder.layers.9.randomltd_layer 


language_model.encoder.layers.9.randomltd_layer.mlp.dense_h_to_4hhere 
language_model.encoder.layers.9.randomltd_layer.mlp.dense_h_to_4hherehereherehere
 language_model.encoder.layers.9
here language_model.encoder.layers.9.randomltd_layer.mlp.dense_4h_to_h 
language_model.encoder.layers.9.randomltd_layer.input_layernormhere
 here   language_model.encoder.layers.9.randomltd_layer.attention.denselanguage_model.encoder.layers.10language_model.encoder.layers.9.randomltd_layer.attentionlanguage_model.encoder.layers.9.randomltd_layer.attention.dense 



language_model.encoder.layers.10.randomltd_layer.attention.denseherehereherehere
chec here, 9 language_model.encoder.layers.9.randomltd_layer  language_model.encoder.layers.9.randomltd_layer.attention.query_key_value
 here
language_model.encoder.layers.9.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.9.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.9.randomltd_layer.mlp.dense_4h_to_h herechec here, 8
here
herehere
language_model.encoder.layers.10.randomltd_layer.post_attention_layernorm 
here   
herelanguage_model.encoder.layers.10.randomltd_layer
herehere language_model.encoder.layers.10.randomltd_layer.input_layernorm 
language_model.encoder.layers.9.randomltd_layerhere 
 language_model.encoder.layers.9.randomltd_layer.mlplanguage_model.encoder.layers.10.randomltd_layer.attentionlanguage_model.encoder.layers.9.randomltd_layer.input_layernormhere

 
herehere  language_model.encoder.layers.9.randomltd_layer.input_layernormlanguage_model.encoder.layers.10.randomltd_layer.attention.query_key_valueherelanguage_model.encoder.layers.9.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.9.randomltd_layer.mlp.dense_h_to_4h

 
language_model.encoder.layers.9.randomltd_layer.mlp
hereherelanguage_model.encoder.layers.9.randomltd_layer.attentionhere 
herehere 
 language_model.encoder.layers.9.randomltd_layer.attention here  language_model.encoder.layers.10.randomltd_layer.attention.scale_mask_softmaxherelanguage_model.encoder.layers.9.randomltd_layer.mlp.dense_4h_to_h
language_model.encoder.layers.10 language_model.encoder.layers.9.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.10.randomltd_layer.mlp
 
here
language_model.encoder.layers.9.randomltd_layer.mlp.dense_h_to_4h
here 
language_model.encoder.layers.9.randomltd_layer.attention.dense
hereherehere  language_model.encoder.layers.10.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.9.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.9.randomltd_layer.attention.query_key_value 


language_model.encoder.layers.10.randomltd_layer.attention.attention_dropouthereherehere
here    here language_model.encoder.layers.10.randomltd_layer.mlp.dense_4h_to_h
language_model.encoder.layers.9.randomltd_layer.mlplanguage_model.encoder.layers.10language_model.encoder.layers.9.randomltd_layer.attention.query_key_value language_model.encoder.layers.9.randomltd_layer.attention.scale_mask_softmax
chec here, 9

here
language_model.encoder.layers.10.randomltd_layer.attention.dense
herehere
here 
herechec here, 9   herelanguage_model.encoder.layers.9.randomltd_layer.mlp.dense_4h_to_hhere 
language_model.encoder.layers.11herelanguage_model.encoder.layers.9.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.9.randomltd_layer.mlp.dense_h_to_4h 
 language_model.encoder.layers.9.randomltd_layer.attention.scale_mask_softmax
 

language_model.encoder.layers.10.randomltd_layerherelanguage_model.encoder.layers.10.randomltd_layer.post_attention_layernorm
here language_model.encoder.layers.11.randomltd_layerlanguage_model.encoder.layers.10.randomltd_layer

hereherehere   language_model.encoder.layers.10.randomltd_layer.input_layernormherelanguage_model.encoder.layers.11.randomltd_layer.input_layernormlanguage_model.encoder.layers.9.randomltd_layer.attention.dense


 
herehere language_model.encoder.layers.9.randomltd_layer.mlp.dense_4h_to_h
here herelanguage_model.encoder.layers.10.randomltd_layer.mlp
 herelanguage_model.encoder.layers.9.randomltd_layer.attention.attention_dropout 
herelanguage_model.encoder.layers.10.randomltd_layer.mlp.dense_h_to_4hhere
  hereherelanguage_model.encoder.layers.9.randomltd_layer.attention.denselanguage_model.encoder.layers.10.randomltd_layer.input_layernorm language_model.encoder.layers.10
 
language_model.encoder.layers.10.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.9.randomltd_layer.post_attention_layernorm
 herehere
language_model.encoder.layers.10.randomltd_layer.attentionhere 
chec here, 9 
 

 language_model.encoder.layers.11.randomltd_layer.attentionherelanguage_model.encoder.layers.9.randomltd_layer.post_attention_layernormherelanguage_model.encoder.layers.10.randomltd_layer.attentionhereherelanguage_model.encoder.layers.11
 
 
  
herelanguage_model.encoder.layers.9.randomltd_layer.mlpherelanguage_model.encoder.layers.10.randomltd_layerherelanguage_model.encoder.layers.10language_model.encoder.layers.10.randomltd_layer.attention.query_key_valuehere 
here  language_model.encoder.layers.9.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.9.randomltd_layer.mlp


herehere  herelanguage_model.encoder.layers.9.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.9.randomltd_layer.mlp.dense_h_to_4h  

language_model.encoder.layers.10.randomltd_layer.input_layernormlanguage_model.encoder.layers.10.randomltd_layer.attention.query_key_value
herehere

 
 hereherechec here, 9language_model.encoder.layers.10  language_model.encoder.layers.9.randomltd_layer.mlp.dense_4h_to_hhere language_model.encoder.layers.11.randomltd_layer.attention.query_key_value

language_model.encoder.layers.11.randomltd_layerlanguage_model.encoder.layers.10.randomltd_layer.attention
 language_model.encoder.layers.10.randomltd_layer.attention.scale_mask_softmax
here

herelanguage_model.encoder.layers.10.randomltd_layer.attention.scale_mask_softmaxchec here, 9
here herehere 

here language_model.encoder.layers.10.randomltd_layer language_model.encoder.layers.11.randomltd_layer.input_layernorm 
herelanguage_model.encoder.layers.10.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.10 

language_model.encoder.layers.11.randomltd_layer.attentionhere
here herechec here, 9 language_model.encoder.layers.10.randomltd_layer.attention.scale_mask_softmaxhere 
language_model.encoder.layers.10.randomltd_layer.attention.attention_dropout
language_model.encoder.layers.11.randomltd_layer.attention.query_key_value here 
here
language_model.encoder.layers.10.randomltd_layerhere language_model.encoder.layers.11.randomltd_layer.attention.scale_mask_softmax language_model.encoder.layers.10.randomltd_layer.attention.attention_dropouthere
 language_model.encoder.layers.10.randomltd_layerlanguage_model.encoder.layers.10.randomltd_layer.attention.attention_dropout



 herelanguage_model.encoder.layers.11.randomltd_layer.attention.scale_mask_softmax
hereherehereherelanguage_model.encoder.layers.10.randomltd_layer.attention.dense 
here    
language_model.encoder.layers.10.randomltd_layer.input_layernormhere language_model.encoder.layers.11.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.10.randomltd_layer.attention.denselanguage_model.encoder.layers.10.randomltd_layer.input_layernormlanguage_model.encoder.layers.10.randomltd_layer.attention.densehere
 language_model.encoder.layers.11.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.10.randomltd_layer.input_layernorm

here
 herelanguage_model.encoder.layers.11.randomltd_layer.attention.dense here

language_model.encoder.layers.10.randomltd_layer.attention here
here
language_model.encoder.layers.11.randomltd_layer.attention.dense  herelanguage_model.encoder.layers.10.randomltd_layer.post_attention_layernormhere
language_model.encoder.layers.11.randomltd_layer.post_attention_layernorm 
 here
language_model.encoder.layers.10.randomltd_layer.attention.query_key_value
language_model.encoder.layers.10.randomltd_layer.attentionhere here 

here language_model.encoder.layers.11.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.10.randomltd_layer.mlphere language_model.encoder.layers.10.randomltd_layer.post_attention_layernormherehere 

here  language_model.encoder.layers.11.randomltd_layer.mlp
language_model.encoder.layers.10.randomltd_layer.attentionherelanguage_model.encoder.layers.11.randomltd_layer.mlp
 
herelanguage_model.encoder.layers.11.randomltd_layer.mlp.dense_h_to_4hhere
 
 language_model.encoder.layers.10.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.11.randomltd_layer.mlp.dense_h_to_4hhere here

here   language_model.encoder.layers.10.randomltd_layer.attention.scale_mask_softmaxhere language_model.encoder.layers.11.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.10.randomltd_layer.mlp
language_model.encoder.layers.10.randomltd_layer.post_attention_layernormherelanguage_model.encoder.layers.10.randomltd_layer.attention.query_key_value language_model.encoder.layers.10.randomltd_layer.attention.scale_mask_softmax
here
 

language_model.encoder.layers.11.randomltd_layer.mlp.dense_4h_to_h
here herelanguage_model.encoder.layers.10.randomltd_layer.attention.attention_dropoutherehere
here language_model.encoder.layers.10.randomltd_layer.mlp.dense_h_to_4h 
  here language_model.encoder.final_layernorm

10
here language_model.encoder.layers.10.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.10.randomltd_layer.mlp.dense_h_to_4h

hereherehere   language_model.encoder.layers.11language_model.encoder.layers.10.randomltd_layer.attention.denselanguage_model.encoder.layers.10.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.10.randomltd_layer.mlp



language_model.encoder.layers.10.randomltd_layer.attention.scale_mask_softmaxhereherehere here 
  language_model.encoder.layers.10.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.10.randomltd_layer.attention.attention_dropout language_model.encoder.final_layernormlanguage_model.encoder.layers.11.randomltd_layerherelanguage_model.encoder.layers.11

language_model.encoder.layers.10.randomltd_layer.mlp.dense_h_to_4h

 here
herehere
10language_model.encoder.layers.10.randomltd_layer.attention.attention_dropout here  language_model.encoder.layers.10.randomltd_layer.attention.dense
herehere  language_model.encoder.layers.10.randomltd_layer.post_attention_layernormlanguage_model.encoder.layers.10.randomltd_layer.mlp.dense_4h_to_h


herehere
  language_model.encoder.layers.10.randomltd_layer.mlplanguage_model.encoder.layers.11language_model.encoder.layers.11.randomltd_layer.input_layernormhere


  herehereherelanguage_model.encoder.layers.10.randomltd_layer.attention.dense language_model.encoder.layers.10.randomltd_layer.mlplanguage_model.encoder.layers.11.randomltd_layer  
language_model.encoder.layers.10.randomltd_layer.mlp.dense_h_to_4h

language_model.encoder.layers.11.randomltd_layerlanguage_model.encoder.layers.11.randomltd_layer.attentionhere
herehere

 here  hereherelanguage_model.encoder.layers.10.randomltd_layer.post_attention_layernorm language_model.encoder.layers.10.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.11.randomltd_layer.input_layernorm  
language_model.encoder.layers.10.randomltd_layer.mlp.dense_4h_to_h

language_model.encoder.layers.11.randomltd_layer.input_layernormlanguage_model.encoder.layers.11.randomltd_layer.attention.query_key_valuehere
herehere

 here  hereherelanguage_model.encoder.layers.10.randomltd_layer.mlp language_model.encoder.layers.10.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.11.randomltd_layer.attention  
language_model.encoder.layers.11

herelanguage_model.encoder.layers.11.randomltd_layer.attention
herelanguage_model.encoder.layers.11.randomltd_layer.attention.scale_mask_softmax 
language_model.encoder.layers.11.randomltd_layer.attention.query_key_valueherehere
 here 
language_model.encoder.layers.11.randomltd_layer.attention.attention_dropout language_model.encoder.layers.10.randomltd_layer.mlp.dense_h_to_4h
language_model.encoder.layers.11.randomltd_layer.attention.scale_mask_softmaxherehere
here
 here  here language_model.encoder.layers.11 language_model.encoder.layers.11.randomltd_layer.attention.query_key_valuelanguage_model.encoder.layers.11.randomltd_layer language_model.encoder.layers.11.randomltd_layer.attention.dense
language_model.encoder.layers.11.randomltd_layer.attention.attention_dropout

hereherelanguage_model.encoder.layers.10.randomltd_layer.mlp.dense_4h_to_h
here
  
here herelanguage_model.encoder.layers.11.randomltd_layer.attention.scale_mask_softmaxlanguage_model.encoder.layers.11.randomltd_layer.input_layernormhere language_model.encoder.layers.11.randomltd_layer 

 language_model.encoder.layers.11.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.11.randomltd_layer.attention.densehereherelanguage_model.encoder.layers.11
herehere language_model.encoder.layers.11.randomltd_layer.mlp 
language_model.encoder.layers.11.randomltd_layer.input_layernormhere

 language_model.encoder.layers.11.randomltd_layer.mlp.dense_h_to_4h
here herehere language_model.encoder.layers.11.randomltd_layer.attention.attention_dropout   language_model.encoder.layers.11.randomltd_layer.attention

language_model.encoder.layers.11.randomltd_layer.post_attention_layernorm
language_model.encoder.layers.11.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.11.randomltd_layer.attentionherehere
here

  here hereherelanguage_model.encoder.layers.11.randomltd_layer.attention.denselanguage_model.encoder.layers.11.randomltd_layer.attention.query_key_value language_model.encoder.layers.11.randomltd_layer  

hereherelanguage_model.encoder.layers.11.randomltd_layer.mlp
language_model.encoder.final_layernormlanguage_model.encoder.layers.11.randomltd_layer.attention.query_key_value  
here

language_model.encoder.layers.11.randomltd_layer.post_attention_layernormherelanguage_model.encoder.layers.11.randomltd_layer.attention.scale_mask_softmaxhere 10
here  language_model.encoder.layers.11.randomltd_layer.mlp
language_model.encoder.layers.11.randomltd_layer.attention.scale_mask_softmaxhere

 language_model.encoder.layers.11.randomltd_layer.mlp.dense_h_to_4hherehere 
  language_model.encoder.layers.11.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.11.randomltd_layer.input_layernormherelanguage_model.encoder.layers.11.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.11.randomltd_layer.attention.attention_dropout

 

here
herelanguage_model.encoder.layers.11.randomltd_layer.mlp.dense_4h_to_hhere here 
 language_model.encoder.layers.11.randomltd_layer.attention.dense language_model.encoder.layers.11.randomltd_layer.mlp.dense_4h_to_hherelanguage_model.encoder.layers.11.randomltd_layer.attention
language_model.encoder.layers.11.randomltd_layer.attention.dense
 
here
herelanguage_model.encoder.final_layernormhere here 
 language_model.encoder.layers.11.randomltd_layer.post_attention_layernorm language_model.encoder.final_layernorm10language_model.encoder.layers.11.randomltd_layer.attention.query_key_value

here language_model.encoder.layers.11.randomltd_layer.attention.scale_mask_softmaxherelanguage_model.encoder.layers.11.randomltd_layer.post_attention_layernorm
 
language_model.encoder.layers.11.randomltd_layer.mlphere
here
  
here10language_model.encoder.layers.11.randomltd_layer.attention.attention_dropoutlanguage_model.encoder.layers.11.randomltd_layer.mlp 


language_model.encoder.layers.11.randomltd_layer.mlp.dense_h_to_4hherehere
  herelanguage_model.encoder.layers.11.randomltd_layer.mlp.dense_h_to_4hlanguage_model.encoder.layers.11.randomltd_layer.attention.dense 

language_model.encoder.layers.11.randomltd_layer.mlp.dense_4h_to_hherehere
  herelanguage_model.encoder.layers.11.randomltd_layer.mlp.dense_4h_to_hlanguage_model.encoder.layers.11.randomltd_layer.post_attention_layernorm 

language_model.encoder.final_layernormherehere
  10language_model.encoder.final_layernormlanguage_model.encoder.layers.11.randomltd_layer.mlp


10here
 language_model.encoder.layers.11.randomltd_layer.mlp.dense_h_to_4h
here language_model.encoder.layers.11.randomltd_layer.mlp.dense_4h_to_h
here language_model.encoder.final_layernorm
10
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
[2022-10-31 10:49:51,423] [INFO] [config.py:978:print] DeepSpeedEngine configuration:
[2022-10-31 10:49:51,424] [INFO] [config.py:982:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-10-31 10:49:51,427] [INFO] [config.py:982:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-10-31 10:49:51,427] [INFO] [config.py:982:print]   amp_enabled .................. False
[2022-10-31 10:49:51,427] [INFO] [config.py:982:print]   amp_params ................... False
[2022-10-31 10:49:51,427] [INFO] [config.py:982:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...[2022-10-31 10:49:51,430] [INFO] [config.py:982:print]   bfloat16_enabled ............. False

Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...[2022-10-31 10:49:51,430] [INFO] [config.py:982:print]   checkpoint_tag_validation_enabled  True

Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...[2022-10-31 10:49:51,431] [INFO] [config.py:982:print]   checkpoint_tag_validation_fail  False

Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...[2022-10-31 10:49:51,431] [INFO] [config.py:982:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f46fc27fe80>

[2022-10-31 10:49:51,431] [INFO] [config.py:982:print]   communication_data_type ...... None
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
[2022-10-31 10:49:51,431] [INFO] [config.py:982:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-10-31 10:49:51,431] [INFO] [config.py:982:print]   curriculum_enabled_legacy .... False
[2022-10-31 10:49:51,431] [INFO] [config.py:982:print]   curriculum_params_legacy ..... {'curriculum_type': 'seqlen', 'min_difficulty': 72, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 221108, 'difficulty_step': 8}}
[2022-10-31 10:49:51,431] [INFO] [config.py:982:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}}
[2022-10-31 10:49:51,431] [INFO] [config.py:982:print]   data_efficiency_enabled ...... False
[2022-10-31 10:49:51,431] [INFO] [config.py:982:print]   dataloader_drop_last ......... False
[2022-10-31 10:49:51,431] [INFO] [config.py:982:print]   disable_allgather ............ False
[2022-10-31 10:49:51,434] [INFO] [config.py:982:print]   dump_state ................... False
[2022-10-31 10:49:51,434] [INFO] [config.py:982:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2022-10-31 10:49:51,434] [INFO] [config.py:982:print]   dynamic_train_config ......... {'random_ltd': {'enabled': True, 'total_layer_num': 12, 'randomltd_layer_num': 10, 'randomltd_layer_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'model_mask_name': 'attention_mask', 'model_type': 'decoder', 'hidden_state_order': 'batch_seq_dim', 'micro_batch_size': 4, 'randomltd_schedule': {'min_value': 256, 'max_value': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'require_steps': 10682, 'seq_per_step': 16, 'saving_layer_tokens': -1}}, 'layer_token_lr_schedule': {'enabled': False, 'warmup_type': 'linear', 'total_layer_tokens': 'by_iteration', 'warmup_layer_tokens': 'by_iteration', 'total_iterations': -1, 'warmup_iterations': -1}}}
[2022-10-31 10:49:51,434] [INFO] [config.py:982:print]   eigenvalue_enabled ........... False
[2022-10-31 10:49:51,434] [INFO] [config.py:982:print]   eigenvalue_gas_boundary_resolution  1
[2022-10-31 10:49:51,434] [INFO] [config.py:982:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-10-31 10:49:51,434] [INFO] [config.py:982:print]   eigenvalue_layer_num ......... 0
[2022-10-31 10:49:51,434] [INFO] [config.py:982:print]   eigenvalue_max_iter .......... 100
[2022-10-31 10:49:51,438] [INFO] [config.py:982:print]   eigenvalue_stability ......... 1e-06
[2022-10-31 10:49:51,438] [INFO] [config.py:982:print]   eigenvalue_tol ............... 0.01
[2022-10-31 10:49:51,438] [INFO] [config.py:982:print]   eigenvalue_verbose ........... False
[2022-10-31 10:49:51,438] [INFO] [config.py:982:print]   elasticity_enabled ........... False
[2022-10-31 10:49:51,438] [INFO] [config.py:982:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-10-31 10:49:51,438] [INFO] [config.py:982:print]   fp16_auto_cast ............... False
[2022-10-31 10:49:51,441] [INFO] [config.py:982:print]   fp16_enabled ................. True
[2022-10-31 10:49:51,441] [INFO] [config.py:982:print]   fp16_master_weights_and_gradients  False
[2022-10-31 10:49:51,441] [INFO] [config.py:982:print]   global_rank .................. 0
[2022-10-31 10:49:51,441] [INFO] [config.py:982:print]   gradient_accumulation_steps .. 8
[2022-10-31 10:49:51,441] [INFO] [config.py:982:print]   gradient_clipping ............ 1.0
[2022-10-31 10:49:51,442] [INFO] [config.py:982:print]   gradient_predivide_factor .... 1.0
[2022-10-31 10:49:51,442] [INFO] [config.py:982:print]   initial_dynamic_scale ........ 2048
[2022-10-31 10:49:51,442] [INFO] [config.py:982:print]   load_universal_checkpoint .... False
[2022-10-31 10:49:51,442] [INFO] [config.py:982:print]   loss_scale ................... 0
[2022-10-31 10:49:51,447] [INFO] [config.py:982:print]   memory_breakdown ............. False
[2022-10-31 10:49:51,447] [INFO] [config.py:982:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f46fc27ffa0>
[2022-10-31 10:49:51,447] [INFO] [config.py:982:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-10-31 10:49:51,447] [INFO] [config.py:982:print]   optimizer_legacy_fusion ...... False
[2022-10-31 10:49:51,447] [INFO] [config.py:982:print]   optimizer_name ............... None
[2022-10-31 10:49:51,447] [INFO] [config.py:982:print]   optimizer_params ............. None
[2022-10-31 10:49:51,447] [INFO] [config.py:982:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-10-31 10:49:51,447] [INFO] [config.py:982:print]   pld_enabled .................. False
[2022-10-31 10:49:51,450] [INFO] [config.py:982:print]   pld_params ................... False
[2022-10-31 10:49:51,450] [INFO] [config.py:982:print]   prescale_gradients ........... True
[2022-10-31 10:49:51,450] [INFO] [config.py:982:print]   scheduler_name ............... None
[2022-10-31 10:49:51,451] [INFO] [config.py:982:print]   scheduler_params ............. None
[2022-10-31 10:49:51,451] [INFO] [config.py:982:print]   sparse_attention ............. None
[2022-10-31 10:49:51,451] [INFO] [config.py:982:print]   sparse_gradients_enabled ..... False
[2022-10-31 10:49:51,451] [INFO] [config.py:982:print]   steps_per_print .............. 10
[2022-10-31 10:49:51,451] [INFO] [config.py:982:print]   train_batch_size ............. 256
[2022-10-31 10:49:51,451] [INFO] [config.py:982:print]   train_micro_batch_size_per_gpu  4
[2022-10-31 10:49:51,451] [INFO] [config.py:982:print]   wall_clock_breakdown ......... False
[2022-10-31 10:49:51,451] [INFO] [config.py:982:print]   world_size ................... 8
[2022-10-31 10:49:51,454] [INFO] [config.py:982:print]   zero_allow_untested_optimizer  False
[2022-10-31 10:49:51,454] [INFO] [config.py:982:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-10-31 10:49:51,454] [INFO] [config.py:982:print]   zero_enabled ................. False
[2022-10-31 10:49:51,454] [INFO] [config.py:982:print]   zero_optimization_stage ...... 0
[2022-10-31 10:49:51,454] [INFO] [config.py:967:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 4, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 72, 
        "max_difficulty": 2.048000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 2.211080e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false, 
    "dynamic_train": {
        "random_ltd": {
            "enabled": true, 
            "total_layer_num": 12, 
            "randomltd_layer_num": 10, 
            "randomltd_layer_id": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 
            "model_mask_name": "attention_mask", 
            "model_type": "decoder", 
            "hidden_state_order": "batch_seq_dim", 
            "micro_batch_size": 4, 
            "randomltd_schedule": {
                "min_value": 256, 
                "max_value": 2.048000e+03, 
                "schedule_type": "fixed_linear", 
                "schedule_config": {
                    "require_steps": 1.068200e+04, 
                    "seq_per_step": 16, 
                    "saving_layer_tokens": -1
                }
            }, 
            "layer_token_lr_schedule": {
                "enabled": false, 
                "warmup_type": "linear", 
                "total_layer_tokens": "by_iteration", 
                "warmup_layer_tokens": "by_iteration", 
                "total_iterations": -1, 
                "warmup_iterations": -1
            }
        }
    }
}
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.4551568031311035 seconds
Time to load utils op: 0.40900516510009766 seconds
Loading extension module utils...
Time to load utils op: 0.508333683013916 seconds
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.5179007053375244 seconds
Time to load utils op: 0.5210311412811279 seconds
Time to load utils op: 0.5213665962219238 seconds
Time to load utils op: 0.5208220481872559 seconds
Time to load utils op: 0.5208823680877686 seconds
[2022-10-31 10:49:52,167] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:49:52,368] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:49:52,578] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:49:52,794] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:49:52,987] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:49:53,199] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
WARNING: could not find the metadata file /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4 
    will not load any checkpoints and will start from random
[2022-10-31 10:49:53,413] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:49:53,625] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
time (ms) | load-checkpoint: 1675.24
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-10-31 10:49:53 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      43945312
    validation: 4395520
    test:       2560
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.007063 seconds
    number of documents: 210604984
 > dataset split:
    train:
     document indices in [0, 206392884) total of 206392884 documents
    validation:
     document indices in [206392884, 210604984) total of 4212100 documents
    test:
     document indices in [210604984, 210604984) total of 0 documents
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19348:21470 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19348:21470 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19340:21468 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19340:21468 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19342:21469 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19342:21469 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19350:21463 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19350:21463 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19339:21471 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19339:21471 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19338:21479 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19338:21479 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19344:21483 [4] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19346:21478 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19344:21483 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19346:21478 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000DQ:19344:21483 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000DQ:19348:21470 [6] NCCL INFO Connected all rings
azwus2f200000DQ:19348:21470 [6] NCCL INFO Connected all trees
azwus2f200000DQ:19348:21470 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19340:21468 [2] NCCL INFO Connected all rings
azwus2f200000DQ:19340:21468 [2] NCCL INFO Connected all trees
azwus2f200000DQ:19340:21468 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19340:21468 [2] NCCL INFO comm 0x7e565c002fb0 rank 0 nranks 1 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000DQ:19348:21470 [6] NCCL INFO comm 0x7e5f9c002fb0 rank 0 nranks 1 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000DQ:19342:21469 [3] NCCL INFO Connected all rings
azwus2f200000DQ:19342:21469 [3] NCCL INFO Connected all trees
azwus2f200000DQ:19342:21469 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19350:21463 [7] NCCL INFO Connected all rings
azwus2f200000DQ:19350:21463 [7] NCCL INFO Connected all trees
azwus2f200000DQ:19350:21463 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19342:21469 [3] NCCL INFO comm 0x7f2250002fb0 rank 0 nranks 1 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000DQ:19339:21471 [1] NCCL INFO Connected all rings
azwus2f200000DQ:19339:21471 [1] NCCL INFO Connected all trees
azwus2f200000DQ:19339:21471 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19350:21463 [7] NCCL INFO comm 0x7e4578002fb0 rank 0 nranks 1 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000DQ:19339:21471 [1] NCCL INFO comm 0x7e628c002fb0 rank 0 nranks 1 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000DQ:19338:21479 [0] NCCL INFO Connected all rings
azwus2f200000DQ:19338:21479 [0] NCCL INFO Connected all trees
azwus2f200000DQ:19338:21479 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19338:21479 [0] NCCL INFO comm 0x7e8f30002fb0 rank 0 nranks 1 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000DQ:19344:21483 [4] NCCL INFO Connected all rings
azwus2f200000DQ:19344:21483 [4] NCCL INFO Connected all trees
azwus2f200000DQ:19344:21483 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19346:21478 [5] NCCL INFO Connected all rings
azwus2f200000DQ:19344:21483 [4] NCCL INFO comm 0x7e6364002fb0 rank 0 nranks 1 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000DQ:19346:21478 [5] NCCL INFO Connected all trees
 > loading doc-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_train_indexmap_43945312ns_2048sl_1234s_doc_idx.npyazwus2f200000DQ:19346:21478 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer

azwus2f200000DQ:19346:21478 [5] NCCL INFO comm 0x7eaea0002fb0 rank 0 nranks 1 cudaDev 5 busId c00000 - Init COMPLETE
 > loading sample-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_train_indexmap_43945312ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_train_indexmap_43945312ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.012 seconds
    total number of samples: 179130331
    total number of epochs: 1
 > loading doc-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_valid_indexmap_4395520ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_valid_indexmap_4395520ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_valid_indexmap_4395520ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.069 seconds
    total number of samples: 7303184
    total number of epochs: 2
> finished creating GPT datasets ...
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19348:21502 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19348:21502 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19342:21499 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19342:21499 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19338:21515 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19338:21515 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19344:21506 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19344:21506 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19346:21505 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19346:21505 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19340:21494 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19340:21494 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19350:21500 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19350:21500 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19339:21501 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19339:21501 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000DQ:19348:21502 [6] NCCL INFO Connected all rings
azwus2f200000DQ:19348:21502 [6] NCCL INFO Connected all trees
azwus2f200000DQ:19348:21502 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19348:21502 [6] NCCL INFO comm 0x7e5fd0002fb0 rank 0 nranks 1 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000DQ:19342:21499 [3] NCCL INFO Connected all rings
azwus2f200000DQ:19338:21515 [0] NCCL INFO Connected all rings
azwus2f200000DQ:19342:21499 [3] NCCL INFO Connected all trees
azwus2f200000DQ:19342:21499 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19344:21506 [4] NCCL INFO Connected all rings
azwus2f200000DQ:19338:21515 [0] NCCL INFO Connected all trees
azwus2f200000DQ:19346:21505 [5] NCCL INFO Connected all rings
azwus2f200000DQ:19344:21506 [4] NCCL INFO Connected all trees
azwus2f200000DQ:19338:21515 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19344:21506 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19346:21505 [5] NCCL INFO Connected all trees
azwus2f200000DQ:19346:21505 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19340:21494 [2] NCCL INFO Connected all rings
azwus2f200000DQ:19342:21499 [3] NCCL INFO comm 0x7f2288002fb0 rank 0 nranks 1 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000DQ:19340:21494 [2] NCCL INFO Connected all trees
azwus2f200000DQ:19344:21506 [4] NCCL INFO comm 0x7e639c002fb0 rank 0 nranks 1 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000DQ:19340:21494 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19346:21505 [5] NCCL INFO comm 0x7eaed8002fb0 rank 0 nranks 1 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000DQ:19338:21515 [0] NCCL INFO comm 0x7e8f60002fb0 rank 0 nranks 1 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000DQ:19350:21500 [7] NCCL INFO Connected all rings
azwus2f200000DQ:19350:21500 [7] NCCL INFO Connected all trees
azwus2f200000DQ:19350:21500 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19340:21494 [2] NCCL INFO comm 0x7e5694002fb0 rank 0 nranks 1 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000DQ:19350:21500 [7] NCCL INFO comm 0x7e45b0002fb0 rank 0 nranks 1 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000DQ:19339:21501 [1] NCCL INFO Connected all rings
azwus2f200000DQ:19339:21501 [1] NCCL INFO Connected all trees
azwus2f200000DQ:19339:21501 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19339:21501 [1] NCCL INFO comm 0x7e62c4002fb0 rank 0 nranks 1 cudaDev 1 busId 200000 - Init COMPLETE
[after dataloaders are built] datetime: 2022-10-31 10:50:00 time (ms) | model-and-optimizer-setup: 18149.17 | train/valid/test-data-iterators-setup: 6671.13

done with setup ...
training ...
[before the start of training step] datetime: 2022-10-31 10:50:00 
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19348:21609 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19348:21609 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19339:21608 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19339:21608 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19340:21602 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19340:21602 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19350:21601 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19350:21601 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19338:21610 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19338:21610 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19346:21604 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19346:21604 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19342:21605 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19342:21605 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:19344:21606 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:19344:21606 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000DQ:19348:21609 [6] NCCL INFO Connected all rings
azwus2f200000DQ:19348:21609 [6] NCCL INFO Connected all trees
azwus2f200000DQ:19348:21609 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19348:21609 [6] NCCL INFO comm 0x7e5cd8002fb0 rank 0 nranks 1 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000DQ:19339:21608 [1] NCCL INFO Connected all rings
azwus2f200000DQ:19340:21602 [2] NCCL INFO Connected all rings
azwus2f200000DQ:19339:21608 [1] NCCL INFO Connected all trees
azwus2f200000DQ:19339:21608 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19340:21602 [2] NCCL INFO Connected all trees
azwus2f200000DQ:19340:21602 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19340:21602 [2] NCCL INFO comm 0x7e53a8002fb0 rank 0 nranks 1 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000DQ:19339:21608 [1] NCCL INFO comm 0x7e5fd0002fb0 rank 0 nranks 1 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000DQ:19350:21601 [7] NCCL INFO Connected all rings
azwus2f200000DQ:19350:21601 [7] NCCL INFO Connected all trees
azwus2f200000DQ:19350:21601 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19338:21610 [0] NCCL INFO Connected all rings
azwus2f200000DQ:19350:21601 [7] NCCL INFO comm 0x7e42bc002fb0 rank 0 nranks 1 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000DQ:19338:21610 [0] NCCL INFO Connected all trees
azwus2f200000DQ:19346:21604 [5] NCCL INFO Connected all rings
azwus2f200000DQ:19338:21610 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19342:21605 [3] NCCL INFO Connected all rings
azwus2f200000DQ:19342:21605 [3] NCCL INFO Connected all trees
azwus2f200000DQ:19342:21605 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19346:21604 [5] NCCL INFO Connected all trees
azwus2f200000DQ:19346:21604 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19342:21605 [3] NCCL INFO comm 0x7f1f94002fb0 rank 0 nranks 1 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000DQ:19346:21604 [5] NCCL INFO comm 0x7eabe4002fb0 rank 0 nranks 1 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000DQ:19338:21610 [0] NCCL INFO comm 0x7e8c74002fb0 rank 0 nranks 1 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000DQ:19344:21606 [4] NCCL INFO Connected all rings
azwus2f200000DQ:19344:21606 [4] NCCL INFO Connected all trees
azwus2f200000DQ:19344:21606 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:19344:21606 [4] NCCL INFO comm 0x7e60a8002fb0 rank 0 nranks 1 cudaDev 4 busId b00000 - Init COMPLETE
[2022-10-31 10:50:08,433] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[9.338879999999999e-08, 9.338879999999999e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:50:08,436] [INFO] [timer.py:198:stop] 0/10, RunningAvgSamplesPerSec=430.5819683931381, CurrSamplesPerSec=436.67212597400487, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
[Rank 0] (after 10 iterations) memory (MB) | allocated: 1678.34912109375 | max allocated: 7173.8876953125 | reserved: 8774.0 | max reserved: 8774.0
 iteration       10/  171661 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 810.1 | learning rate: 9.339E-08 | global batch size:   256 |reserve_length:   256 | lm loss: 1.078335E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 293.83 | backward-compute: 210.99 | backward-embedding-all-reduce: 0.01 | optimizer: 287.55 | batch-generator: 10.25
[2022-10-31 10:50:13,266] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[1.9715413333333332e-07, 1.9715413333333332e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:50:13,268] [INFO] [timer.py:198:stop] 0/20, RunningAvgSamplesPerSec=429.29791696467674, CurrSamplesPerSec=432.6198991761323, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       20/  171661 | consumed samples:         5120 | consumed tokens:     10485760 | elapsed time per iteration (ms): 483.2 | learning rate: 1.972E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.074999E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.87 | backward-compute: 191.42 | backward-embedding-all-reduce: 0.01 | optimizer: 13.79 | batch-generator: 9.21
[2022-10-31 10:50:18,139] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[3.0091946666666664e-07, 3.0091946666666664e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:50:18,142] [INFO] [timer.py:198:stop] 0/30, RunningAvgSamplesPerSec=426.8912294731414, CurrSamplesPerSec=418.3414050300312, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       30/  171661 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 487.4 | learning rate: 3.009E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.071889E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 264.79 | backward-compute: 191.66 | backward-embedding-all-reduce: 0.01 | optimizer: 13.75 | batch-generator: 9.13
[2022-10-31 10:50:22,960] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[4.0468479999999994e-07, 4.0468479999999994e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:50:22,963] [INFO] [timer.py:198:stop] 0/40, RunningAvgSamplesPerSec=426.988446647475, CurrSamplesPerSec=430.3960852597588, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       40/  171661 | consumed samples:        10240 | consumed tokens:     20971520 | elapsed time per iteration (ms): 482.0 | learning rate: 4.047E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.066174E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.71 | backward-compute: 191.43 | backward-embedding-all-reduce: 0.01 | optimizer: 13.77 | batch-generator: 8.96
[2022-10-31 10:50:27,817] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[5.084501333333333e-07, 5.084501333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:50:27,819] [INFO] [timer.py:198:stop] 0/50, RunningAvgSamplesPerSec=426.41017377083233, CurrSamplesPerSec=410.96578901439415, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       50/  171661 | consumed samples:        12800 | consumed tokens:     26214400 | elapsed time per iteration (ms): 485.5 | learning rate: 5.085E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.056701E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 263.57 | backward-compute: 191.43 | backward-embedding-all-reduce: 0.01 | optimizer: 13.78 | batch-generator: 8.91
[2022-10-31 10:50:32,643] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[6.122154666666666e-07, 6.122154666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:50:32,646] [INFO] [timer.py:198:stop] 0/60, RunningAvgSamplesPerSec=426.53363281058876, CurrSamplesPerSec=430.470626345043, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       60/  171661 | consumed samples:        15360 | consumed tokens:     31457280 | elapsed time per iteration (ms): 482.8 | learning rate: 6.122E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.047988E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.72 | backward-compute: 191.72 | backward-embedding-all-reduce: 0.01 | optimizer: 13.79 | batch-generator: 9.08
[2022-10-31 10:50:37,497] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[7.159807999999999e-07, 7.159807999999999e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:50:37,499] [INFO] [timer.py:198:stop] 0/70, RunningAvgSamplesPerSec=426.08589695653717, CurrSamplesPerSec=428.97920908216327, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       70/  171661 | consumed samples:        17920 | consumed tokens:     36700160 | elapsed time per iteration (ms): 485.3 | learning rate: 7.160E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.033031E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 261.04 | backward-compute: 192.18 | backward-embedding-all-reduce: 0.01 | optimizer: 13.79 | batch-generator: 10.00
[2022-10-31 10:50:42,345] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[8.197461333333333e-07, 8.197461333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:50:42,347] [INFO] [timer.py:198:stop] 0/80, RunningAvgSamplesPerSec=425.8746547415601, CurrSamplesPerSec=428.0024873162814, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       80/  171661 | consumed samples:        20480 | consumed tokens:     41943040 | elapsed time per iteration (ms): 484.8 | learning rate: 8.197E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.022072E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.67 | backward-compute: 192.38 | backward-embedding-all-reduce: 0.01 | optimizer: 13.81 | batch-generator: 9.46
[2022-10-31 10:50:47,169] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[9.235114666666666e-07, 9.235114666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:50:47,171] [INFO] [timer.py:198:stop] 0/90, RunningAvgSamplesPerSec=425.9092572075177, CurrSamplesPerSec=435.5950604462475, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       90/  171661 | consumed samples:        23040 | consumed tokens:     47185920 | elapsed time per iteration (ms): 482.4 | learning rate: 9.235E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.005317E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.14 | backward-compute: 192.23 | backward-embedding-all-reduce: 0.01 | optimizer: 13.83 | batch-generator: 10.24
[2022-10-31 10:50:51,973] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[1.0284032e-06, 1.0284032e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:50:51,976] [INFO] [timer.py:198:stop] 0/100, RunningAvgSamplesPerSec=426.5981051720118, CurrSamplesPerSec=434.25758232656256, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      100/  171661 | consumed samples:        25600 | consumed tokens:     52428800 | elapsed time per iteration (ms): 480.4 | learning rate: 1.028E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.932045E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 255.19 | backward-compute: 193.28 | backward-embedding-all-reduce: 0.01 | optimizer: 13.86 | batch-generator: 10.35
-----------------------------------------------------------------------------------------------
 validation loss at iteration 100 | lm loss value: 9.661073E+00 | lm loss PPL: 1.569461E+04 | 
-----------------------------------------------------------------------------------------------
[2022-10-31 10:50:59,983] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[1.1359231999999999e-06, 1.1359231999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:50:59,986] [INFO] [timer.py:198:stop] 0/110, RunningAvgSamplesPerSec=426.77393717115547, CurrSamplesPerSec=429.7057384711924, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      110/  171661 | consumed samples:        28160 | consumed tokens:     57671680 | elapsed time per iteration (ms): 801.0 | learning rate: 1.136E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.845078E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 573.86 | backward-compute: 194.81 | backward-embedding-all-reduce: 0.01 | optimizer: 13.86 | batch-generator: 17.56
[2022-10-31 10:51:04,791] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[1.2434431999999998e-06, 1.2434431999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:51:04,793] [INFO] [timer.py:198:stop] 0/120, RunningAvgSamplesPerSec=426.93587847202826, CurrSamplesPerSec=426.0352781719089, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      120/  171661 | consumed samples:        30720 | consumed tokens:     62914560 | elapsed time per iteration (ms): 480.8 | learning rate: 1.243E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.703465E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 254.39 | backward-compute: 194.37 | backward-embedding-all-reduce: 0.01 | optimizer: 13.85 | batch-generator: 10.34
[2022-10-31 10:51:09,579] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[1.3509632e-06, 1.3509632e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:51:09,582] [INFO] [timer.py:198:stop] 0/130, RunningAvgSamplesPerSec=427.31427535291937, CurrSamplesPerSec=434.3742309646559, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      130/  171661 | consumed samples:        33280 | consumed tokens:     68157440 | elapsed time per iteration (ms): 478.9 | learning rate: 1.351E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.552262E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 252.71 | backward-compute: 194.21 | backward-embedding-all-reduce: 0.01 | optimizer: 13.81 | batch-generator: 10.42
[2022-10-31 10:51:14,378] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[1.4584832e-06, 1.4584832e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:51:14,381] [INFO] [timer.py:198:stop] 0/140, RunningAvgSamplesPerSec=427.5188119577002, CurrSamplesPerSec=430.06096959210487, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      140/  171661 | consumed samples:        35840 | consumed tokens:     73400320 | elapsed time per iteration (ms): 479.9 | learning rate: 1.458E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.416933E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 254.74 | backward-compute: 193.64 | backward-embedding-all-reduce: 0.01 | optimizer: 13.82 | batch-generator: 10.45
[2022-10-31 10:51:19,186] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[1.5660031999999999e-06, 1.5660031999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:51:19,189] [INFO] [timer.py:198:stop] 0/150, RunningAvgSamplesPerSec=427.81679249437235, CurrSamplesPerSec=430.45405942816825, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      150/  171661 | consumed samples:        38400 | consumed tokens:     78643200 | elapsed time per iteration (ms): 480.7 | learning rate: 1.566E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.399093E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 254.16 | backward-compute: 194.35 | backward-embedding-all-reduce: 0.01 | optimizer: 13.91 | batch-generator: 10.37
[2022-10-31 10:51:23,976] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[1.6735231999999998e-06, 1.6735231999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:51:23,979] [INFO] [timer.py:198:stop] 0/160, RunningAvgSamplesPerSec=428.10146131727475, CurrSamplesPerSec=431.95437722465743, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      160/  171661 | consumed samples:        40960 | consumed tokens:     83886080 | elapsed time per iteration (ms): 479.0 | learning rate: 1.674E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.284677E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 253.83 | backward-compute: 194.18 | backward-embedding-all-reduce: 0.01 | optimizer: 13.83 | batch-generator: 10.36
[2022-10-31 10:51:28,775] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[1.7810431999999998e-06, 1.7810431999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:51:28,778] [INFO] [timer.py:198:stop] 0/170, RunningAvgSamplesPerSec=428.3309433112609, CurrSamplesPerSec=431.5904612455946, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      170/  171661 | consumed samples:        43520 | consumed tokens:     89128960 | elapsed time per iteration (ms): 480.0 | learning rate: 1.781E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.230325E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 254.80 | backward-compute: 194.24 | backward-embedding-all-reduce: 0.01 | optimizer: 13.85 | batch-generator: 10.34
[2022-10-31 10:51:33,554] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[1.8885631999999997e-06, 1.8885631999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:51:33,557] [INFO] [timer.py:198:stop] 0/180, RunningAvgSamplesPerSec=428.527244968704, CurrSamplesPerSec=432.8375428910503, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      180/  171661 | consumed samples:        46080 | consumed tokens:     94371840 | elapsed time per iteration (ms): 477.8 | learning rate: 1.889E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.298767E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 252.93 | backward-compute: 194.09 | backward-embedding-all-reduce: 0.01 | optimizer: 13.75 | batch-generator: 9.45
[2022-10-31 10:51:38,361] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[1.9960832e-06, 1.9960832e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:51:38,364] [INFO] [timer.py:198:stop] 0/190, RunningAvgSamplesPerSec=428.7630375881734, CurrSamplesPerSec=436.60394322946655, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      190/  171661 | consumed samples:        48640 | consumed tokens:     99614720 | elapsed time per iteration (ms): 480.7 | learning rate: 1.996E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.108112E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 255.69 | backward-compute: 194.00 | backward-embedding-all-reduce: 0.01 | optimizer: 13.83 | batch-generator: 9.52
[2022-10-31 10:51:43,176] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[2.106606933333333e-06, 2.106606933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:51:43,178] [INFO] [timer.py:198:stop] 0/200, RunningAvgSamplesPerSec=428.940974609253, CurrSamplesPerSec=432.50558444990105, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      200/  171661 | consumed samples:        51200 | consumed tokens:    104857600 | elapsed time per iteration (ms): 481.4 | learning rate: 2.107E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 9.106137E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 256.39 | backward-compute: 194.31 | backward-embedding-all-reduce: 0.01 | optimizer: 13.76 | batch-generator: 9.20
-----------------------------------------------------------------------------------------------
 validation loss at iteration 200 | lm loss value: 9.076966E+00 | lm loss PPL: 8.751377E+03 | 
-----------------------------------------------------------------------------------------------
[2022-10-31 10:51:51,210] [INFO] [logging.py:68:log_dist] [Rank 0] step=210, skipped=0, lr=[2.2178815999999997e-06, 2.2178815999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:51:51,213] [INFO] [timer.py:198:stop] 0/210, RunningAvgSamplesPerSec=428.9695787839304, CurrSamplesPerSec=430.5507177570693, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      210/  171661 | consumed samples:        53760 | consumed tokens:    110100480 | elapsed time per iteration (ms): 803.4 | learning rate: 2.218E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 9.045603E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 577.30 | backward-compute: 194.84 | backward-embedding-all-reduce: 0.01 | optimizer: 13.87 | batch-generator: 17.09
[2022-10-31 10:51:56,030] [INFO] [logging.py:68:log_dist] [Rank 0] step=220, skipped=0, lr=[2.3291562666666666e-06, 2.3291562666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:51:56,033] [INFO] [timer.py:198:stop] 0/220, RunningAvgSamplesPerSec=429.05689668349663, CurrSamplesPerSec=424.4441464802985, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      220/  171661 | consumed samples:        56320 | consumed tokens:    115343360 | elapsed time per iteration (ms): 482.0 | learning rate: 2.329E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 9.090004E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 256.59 | backward-compute: 194.90 | backward-embedding-all-reduce: 0.01 | optimizer: 13.84 | batch-generator: 9.51
[2022-10-31 10:52:00,862] [INFO] [logging.py:68:log_dist] [Rank 0] step=230, skipped=0, lr=[2.440430933333333e-06, 2.440430933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:00,865] [INFO] [timer.py:198:stop] 0/230, RunningAvgSamplesPerSec=429.03797509988794, CurrSamplesPerSec=416.944369198653, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      230/  171661 | consumed samples:        58880 | consumed tokens:    120586240 | elapsed time per iteration (ms): 483.2 | learning rate: 2.440E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.952828E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 257.26 | backward-compute: 194.92 | backward-embedding-all-reduce: 0.01 | optimizer: 13.82 | batch-generator: 9.79
[2022-10-31 10:52:05,696] [INFO] [logging.py:68:log_dist] [Rank 0] step=240, skipped=0, lr=[2.5517056e-06, 2.5517056e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:05,698] [INFO] [timer.py:198:stop] 0/240, RunningAvgSamplesPerSec=428.9392498728006, CurrSamplesPerSec=419.1212980426935, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      240/  171661 | consumed samples:        61440 | consumed tokens:    125829120 | elapsed time per iteration (ms): 483.1 | learning rate: 2.552E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.933978E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 257.18 | backward-compute: 195.08 | backward-embedding-all-reduce: 0.01 | optimizer: 13.84 | batch-generator: 9.86
[2022-10-31 10:52:10,527] [INFO] [logging.py:68:log_dist] [Rank 0] step=250, skipped=0, lr=[2.6629802666666665e-06, 2.6629802666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:10,530] [INFO] [timer.py:198:stop] 0/250, RunningAvgSamplesPerSec=428.90611757469094, CurrSamplesPerSec=429.1040136067471, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      250/  171661 | consumed samples:        64000 | consumed tokens:    131072000 | elapsed time per iteration (ms): 483.4 | learning rate: 2.663E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.940360E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.60 | backward-compute: 194.45 | backward-embedding-all-reduce: 0.01 | optimizer: 13.78 | batch-generator: 9.39
[2022-10-31 10:52:15,370] [INFO] [logging.py:68:log_dist] [Rank 0] step=260, skipped=0, lr=[2.774254933333333e-06, 2.774254933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:15,373] [INFO] [timer.py:198:stop] 0/260, RunningAvgSamplesPerSec=428.82769988146, CurrSamplesPerSec=426.42103992324166, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      260/  171661 | consumed samples:        66560 | consumed tokens:    136314880 | elapsed time per iteration (ms): 484.3 | learning rate: 2.774E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.893758E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.95 | backward-compute: 194.69 | backward-embedding-all-reduce: 0.01 | optimizer: 13.93 | batch-generator: 9.27
[2022-10-31 10:52:20,229] [INFO] [logging.py:68:log_dist] [Rank 0] step=270, skipped=0, lr=[2.8855295999999995e-06, 2.8855295999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:20,232] [INFO] [timer.py:198:stop] 0/270, RunningAvgSamplesPerSec=428.7142020461089, CurrSamplesPerSec=427.1620327935635, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      270/  171661 | consumed samples:        69120 | consumed tokens:    141557760 | elapsed time per iteration (ms): 485.9 | learning rate: 2.886E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.830926E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.59 | backward-compute: 194.60 | backward-embedding-all-reduce: 0.01 | optimizer: 13.79 | batch-generator: 9.46
[2022-10-31 10:52:25,089] [INFO] [logging.py:68:log_dist] [Rank 0] step=280, skipped=0, lr=[2.9968042666666665e-06, 2.9968042666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:25,092] [INFO] [timer.py:198:stop] 0/280, RunningAvgSamplesPerSec=428.6333497870976, CurrSamplesPerSec=419.783280320019, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      280/  171661 | consumed samples:        71680 | consumed tokens:    146800640 | elapsed time per iteration (ms): 486.0 | learning rate: 2.997E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.764866E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.99 | backward-compute: 194.35 | backward-embedding-all-reduce: 0.01 | optimizer: 13.78 | batch-generator: 9.32
[2022-10-31 10:52:29,937] [INFO] [logging.py:68:log_dist] [Rank 0] step=290, skipped=0, lr=[3.1088298666666665e-06, 3.1088298666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:29,940] [INFO] [timer.py:198:stop] 0/290, RunningAvgSamplesPerSec=428.6128394401416, CurrSamplesPerSec=428.80104278818044, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      290/  171661 | consumed samples:        74240 | consumed tokens:    152043520 | elapsed time per iteration (ms): 484.7 | learning rate: 3.109E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.761116E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.56 | backward-compute: 194.96 | backward-embedding-all-reduce: 0.01 | optimizer: 13.85 | batch-generator: 9.40
[2022-10-31 10:52:34,772] [INFO] [logging.py:68:log_dist] [Rank 0] step=300, skipped=0, lr=[3.2238592e-06, 3.2238592e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:34,775] [INFO] [timer.py:198:stop] 0/300, RunningAvgSamplesPerSec=428.5748782680162, CurrSamplesPerSec=426.5796076113108, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      300/  171661 | consumed samples:        76800 | consumed tokens:    157286400 | elapsed time per iteration (ms): 483.5 | learning rate: 3.224E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.732607E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.09 | backward-compute: 195.84 | backward-embedding-all-reduce: 0.01 | optimizer: 13.77 | batch-generator: 8.90
-----------------------------------------------------------------------------------------------
 validation loss at iteration 300 | lm loss value: 8.673430E+00 | lm loss PPL: 5.845518E+03 | 
-----------------------------------------------------------------------------------------------
[2022-10-31 10:52:42,801] [INFO] [logging.py:68:log_dist] [Rank 0] step=310, skipped=0, lr=[3.338888533333333e-06, 3.338888533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:42,804] [INFO] [timer.py:198:stop] 0/310, RunningAvgSamplesPerSec=428.5112694339953, CurrSamplesPerSec=429.0395450607832, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      310/  171661 | consumed samples:        79360 | consumed tokens:    162529280 | elapsed time per iteration (ms): 802.9 | learning rate: 3.339E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.726015E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 578.12 | backward-compute: 195.84 | backward-embedding-all-reduce: 0.01 | optimizer: 13.86 | batch-generator: 15.70
[2022-10-31 10:52:47,631] [INFO] [logging.py:68:log_dist] [Rank 0] step=320, skipped=0, lr=[3.453917866666666e-06, 3.453917866666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:47,634] [INFO] [timer.py:198:stop] 0/320, RunningAvgSamplesPerSec=428.4364490875562, CurrSamplesPerSec=426.9663561866952, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      320/  171661 | consumed samples:        81920 | consumed tokens:    167772160 | elapsed time per iteration (ms): 483.0 | learning rate: 3.454E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.620916E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.09 | backward-compute: 195.75 | backward-embedding-all-reduce: 0.01 | optimizer: 13.79 | batch-generator: 8.71
[2022-10-31 10:52:52,466] [INFO] [logging.py:68:log_dist] [Rank 0] step=330, skipped=0, lr=[3.5689471999999995e-06, 3.5689471999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:52,469] [INFO] [timer.py:198:stop] 0/330, RunningAvgSamplesPerSec=428.3825296268203, CurrSamplesPerSec=427.7828603483006, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      330/  171661 | consumed samples:        84480 | consumed tokens:    173015040 | elapsed time per iteration (ms): 483.5 | learning rate: 3.569E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.631646E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.79 | backward-compute: 195.92 | backward-embedding-all-reduce: 0.01 | optimizer: 13.77 | batch-generator: 8.77
[2022-10-31 10:52:57,315] [INFO] [logging.py:68:log_dist] [Rank 0] step=340, skipped=0, lr=[3.6839765333333326e-06, 3.6839765333333326e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:52:57,317] [INFO] [timer.py:198:stop] 0/340, RunningAvgSamplesPerSec=428.3836782858939, CurrSamplesPerSec=427.40415883832753, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      340/  171661 | consumed samples:        87040 | consumed tokens:    178257920 | elapsed time per iteration (ms): 484.8 | learning rate: 3.684E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.513618E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.58 | backward-compute: 195.79 | backward-embedding-all-reduce: 0.01 | optimizer: 13.76 | batch-generator: 8.79
[2022-10-31 10:53:02,148] [INFO] [logging.py:68:log_dist] [Rank 0] step=350, skipped=0, lr=[3.7990058666666665e-06, 3.7990058666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:02,151] [INFO] [timer.py:198:stop] 0/350, RunningAvgSamplesPerSec=428.3622491110096, CurrSamplesPerSec=429.5970834787006, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      350/  171661 | consumed samples:        89600 | consumed tokens:    183500800 | elapsed time per iteration (ms): 483.3 | learning rate: 3.799E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.531570E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.05 | backward-compute: 195.89 | backward-embedding-all-reduce: 0.01 | optimizer: 13.78 | batch-generator: 8.45
[2022-10-31 10:53:06,972] [INFO] [logging.py:68:log_dist] [Rank 0] step=360, skipped=0, lr=[3.9140352e-06, 3.9140352e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:06,975] [INFO] [timer.py:198:stop] 0/360, RunningAvgSamplesPerSec=428.36419401979384, CurrSamplesPerSec=426.81292989680884, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      360/  171661 | consumed samples:        92160 | consumed tokens:    188743680 | elapsed time per iteration (ms): 482.4 | learning rate: 3.914E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.525821E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.68 | backward-compute: 195.72 | backward-embedding-all-reduce: 0.01 | optimizer: 13.74 | batch-generator: 8.72
[2022-10-31 10:53:11,801] [INFO] [logging.py:68:log_dist] [Rank 0] step=370, skipped=0, lr=[4.029064533333333e-06, 4.029064533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:11,804] [INFO] [timer.py:198:stop] 0/370, RunningAvgSamplesPerSec=428.3579381589785, CurrSamplesPerSec=427.57571741678987, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      370/  171661 | consumed samples:        94720 | consumed tokens:    193986560 | elapsed time per iteration (ms): 482.9 | learning rate: 4.029E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.477269E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.62 | backward-compute: 195.87 | backward-embedding-all-reduce: 0.01 | optimizer: 13.78 | batch-generator: 8.50
[2022-10-31 10:53:16,648] [INFO] [logging.py:68:log_dist] [Rank 0] step=380, skipped=0, lr=[4.144093866666667e-06, 4.144093866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:16,651] [INFO] [timer.py:198:stop] 0/380, RunningAvgSamplesPerSec=428.34667181956195, CurrSamplesPerSec=428.96275676038465, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      380/  171661 | consumed samples:        97280 | consumed tokens:    199229440 | elapsed time per iteration (ms): 484.7 | learning rate: 4.144E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.413667E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.55 | backward-compute: 195.79 | backward-embedding-all-reduce: 0.01 | optimizer: 13.73 | batch-generator: 8.44
[2022-10-31 10:53:21,490] [INFO] [logging.py:68:log_dist] [Rank 0] step=390, skipped=0, lr=[4.261751466666666e-06, 4.261751466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:21,493] [INFO] [timer.py:198:stop] 0/390, RunningAvgSamplesPerSec=428.2779582625599, CurrSamplesPerSec=416.1906161722343, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      390/  171661 | consumed samples:        99840 | consumed tokens:    204472320 | elapsed time per iteration (ms): 484.2 | learning rate: 4.262E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.362445E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.36 | backward-compute: 196.78 | backward-embedding-all-reduce: 0.01 | optimizer: 13.78 | batch-generator: 8.53
[2022-10-31 10:53:26,346] [INFO] [logging.py:68:log_dist] [Rank 0] step=400, skipped=0, lr=[4.380535466666666e-06, 4.380535466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:26,349] [INFO] [timer.py:198:stop] 0/400, RunningAvgSamplesPerSec=428.2320514648795, CurrSamplesPerSec=426.5972335231896, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      400/  171661 | consumed samples:       102400 | consumed tokens:    209715200 | elapsed time per iteration (ms): 485.6 | learning rate: 4.381E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.318017E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.17 | backward-compute: 197.10 | backward-embedding-all-reduce: 0.01 | optimizer: 13.75 | batch-generator: 8.57
-----------------------------------------------------------------------------------------------
 validation loss at iteration 400 | lm loss value: 8.323271E+00 | lm loss PPL: 4.118609E+03 | 
-----------------------------------------------------------------------------------------------
[2022-10-31 10:53:34,371] [INFO] [logging.py:68:log_dist] [Rank 0] step=410, skipped=0, lr=[4.499319466666666e-06, 4.499319466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:34,373] [INFO] [timer.py:198:stop] 0/410, RunningAvgSamplesPerSec=428.17699407324017, CurrSamplesPerSec=427.7733164626353, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      410/  171661 | consumed samples:       104960 | consumed tokens:    214958080 | elapsed time per iteration (ms): 802.5 | learning rate: 4.499E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.280815E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 576.37 | backward-compute: 197.06 | backward-embedding-all-reduce: 0.01 | optimizer: 13.74 | batch-generator: 15.79
[2022-10-31 10:53:39,207] [INFO] [logging.py:68:log_dist] [Rank 0] step=420, skipped=0, lr=[4.618103466666667e-06, 4.618103466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:39,210] [INFO] [timer.py:198:stop] 0/420, RunningAvgSamplesPerSec=428.1461411637168, CurrSamplesPerSec=425.35883881599796, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      420/  171661 | consumed samples:       107520 | consumed tokens:    220200960 | elapsed time per iteration (ms): 483.6 | learning rate: 4.618E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.264466E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.32 | backward-compute: 197.21 | backward-embedding-all-reduce: 0.01 | optimizer: 13.76 | batch-generator: 8.59
[2022-10-31 10:53:44,044] [INFO] [logging.py:68:log_dist] [Rank 0] step=430, skipped=0, lr=[4.736887466666666e-06, 4.736887466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:44,047] [INFO] [timer.py:198:stop] 0/430, RunningAvgSamplesPerSec=428.04905648966826, CurrSamplesPerSec=426.32351838793494, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      430/  171661 | consumed samples:       110080 | consumed tokens:    225443840 | elapsed time per iteration (ms): 483.7 | learning rate: 4.737E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.212595E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.50 | backward-compute: 197.17 | backward-embedding-all-reduce: 0.01 | optimizer: 13.77 | batch-generator: 8.52
[2022-10-31 10:53:48,900] [INFO] [logging.py:68:log_dist] [Rank 0] step=440, skipped=0, lr=[4.855671466666666e-06, 4.855671466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:48,903] [INFO] [timer.py:198:stop] 0/440, RunningAvgSamplesPerSec=427.99416131261694, CurrSamplesPerSec=425.09748079079225, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      440/  171661 | consumed samples:       112640 | consumed tokens:    230686720 | elapsed time per iteration (ms): 485.6 | learning rate: 4.856E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.200500E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.16 | backward-compute: 197.00 | backward-embedding-all-reduce: 0.01 | optimizer: 13.75 | batch-generator: 8.54
[2022-10-31 10:53:53,755] [INFO] [logging.py:68:log_dist] [Rank 0] step=450, skipped=0, lr=[4.974455466666666e-06, 4.974455466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:53,758] [INFO] [timer.py:198:stop] 0/450, RunningAvgSamplesPerSec=427.9436791938376, CurrSamplesPerSec=430.2719074684952, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      450/  171661 | consumed samples:       115200 | consumed tokens:    235929600 | elapsed time per iteration (ms): 485.5 | learning rate: 4.974E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.067671E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.13 | backward-compute: 197.26 | backward-embedding-all-reduce: 0.01 | optimizer: 13.75 | batch-generator: 8.73
[2022-10-31 10:53:58,579] [INFO] [logging.py:68:log_dist] [Rank 0] step=460, skipped=0, lr=[5.093239466666666e-06, 5.093239466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:53:58,582] [INFO] [timer.py:198:stop] 0/460, RunningAvgSamplesPerSec=427.94062536200244, CurrSamplesPerSec=429.59433343255586, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      460/  171661 | consumed samples:       117760 | consumed tokens:    241172480 | elapsed time per iteration (ms): 482.4 | learning rate: 5.093E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.110712E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 257.18 | backward-compute: 197.06 | backward-embedding-all-reduce: 0.01 | optimizer: 13.77 | batch-generator: 8.61
[2022-10-31 10:54:03,415] [INFO] [logging.py:68:log_dist] [Rank 0] step=470, skipped=0, lr=[5.212023466666666e-06, 5.212023466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:03,418] [INFO] [timer.py:198:stop] 0/470, RunningAvgSamplesPerSec=427.9525655489865, CurrSamplesPerSec=429.4005438781713, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      470/  171661 | consumed samples:       120320 | consumed tokens:    246415360 | elapsed time per iteration (ms): 483.6 | learning rate: 5.212E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.101424E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 257.96 | backward-compute: 197.27 | backward-embedding-all-reduce: 0.01 | optimizer: 13.78 | batch-generator: 8.45
[2022-10-31 10:54:08,253] [INFO] [logging.py:68:log_dist] [Rank 0] step=480, skipped=0, lr=[5.331558399999999e-06, 5.331558399999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:08,256] [INFO] [timer.py:198:stop] 0/480, RunningAvgSamplesPerSec=427.94014191833077, CurrSamplesPerSec=429.3387329462758, MemAllocated=1.64GB, MaxMemAllocated=7.19GB
 iteration      480/  171661 | consumed samples:       122880 | consumed tokens:    251658240 | elapsed time per iteration (ms): 483.9 | learning rate: 5.332E-06 | global batch size:   256 |reserve_length:   336 | lm loss: 8.016349E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 257.09 | backward-compute: 198.05 | backward-embedding-all-reduce: 0.01 | optimizer: 13.81 | batch-generator: 8.76
[2022-10-31 10:54:13,094] [INFO] [logging.py:68:log_dist] [Rank 0] step=490, skipped=0, lr=[5.454097066666667e-06, 5.454097066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:13,097] [INFO] [timer.py:198:stop] 0/490, RunningAvgSamplesPerSec=427.91753431332023, CurrSamplesPerSec=427.31026841855595, MemAllocated=1.64GB, MaxMemAllocated=7.19GB
 iteration      490/  171661 | consumed samples:       125440 | consumed tokens:    256901120 | elapsed time per iteration (ms): 484.0 | learning rate: 5.454E-06 | global batch size:   256 |reserve_length:   336 | lm loss: 7.929800E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 256.41 | backward-compute: 199.62 | backward-embedding-all-reduce: 0.01 | optimizer: 13.74 | batch-generator: 8.54
[2022-10-31 10:54:17,943] [INFO] [logging.py:68:log_dist] [Rank 0] step=500, skipped=0, lr=[5.576635733333333e-06, 5.576635733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:17,946] [INFO] [timer.py:198:stop] 0/500, RunningAvgSamplesPerSec=427.8883697357905, CurrSamplesPerSec=428.03934125090893, MemAllocated=1.64GB, MaxMemAllocated=7.19GB
 iteration      500/  171661 | consumed samples:       128000 | consumed tokens:    262144000 | elapsed time per iteration (ms): 485.0 | learning rate: 5.577E-06 | global batch size:   256 |reserve_length:   336 | lm loss: 7.935809E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 257.04 | backward-compute: 199.57 | backward-embedding-all-reduce: 0.01 | optimizer: 13.77 | batch-generator: 8.54
-----------------------------------------------------------------------------------------------
 validation loss at iteration 500 | lm loss value: 7.883712E+00 | lm loss PPL: 2.653706E+03 | 
-----------------------------------------------------------------------------------------------
[2022-10-31 10:54:21,617] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-31 10:54:21,616] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-31 10:54:21,617] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-31 10:54:21,617] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-31 10:54:21,617] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-31 10:54:21,617] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-31 10:54:21,621] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-31 10:54:21,617] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-31 10:54:21,621] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-31 10:54:21,617] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-31 10:54:21,617] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-31 10:54:21,624] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-31 10:54:21,617] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations
[2022-10-31 10:54:21,621] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-31 10:54:21,624] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-31 10:54:21,624] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2022-10-31 10:54:25,994] [INFO] [logging.py:68:log_dist] [Rank 0] step=510, skipped=0, lr=[5.6991744e-06, 5.6991744e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:25,997] [INFO] [timer.py:198:stop] 0/510, RunningAvgSamplesPerSec=427.8614659861189, CurrSamplesPerSec=424.0766902374137, MemAllocated=1.64GB, MaxMemAllocated=7.19GB
 iteration      510/  171661 | consumed samples:       130560 | consumed tokens:    267386880 | elapsed time per iteration (ms): 805.0 | learning rate: 5.699E-06 | global batch size:   256 |reserve_length:   336 | lm loss: 7.912798E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 575.61 | backward-compute: 199.57 | backward-embedding-all-reduce: 0.01 | optimizer: 14.20 | batch-generator: 15.50
[2022-10-31 10:54:30,841] [INFO] [logging.py:68:log_dist] [Rank 0] step=520, skipped=0, lr=[5.821713066666666e-06, 5.821713066666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:30,843] [INFO] [timer.py:198:stop] 0/520, RunningAvgSamplesPerSec=427.8508801029292, CurrSamplesPerSec=428.18545446424866, MemAllocated=1.64GB, MaxMemAllocated=7.19GB
 iteration      520/  171661 | consumed samples:       133120 | consumed tokens:    272629760 | elapsed time per iteration (ms): 484.7 | learning rate: 5.822E-06 | global batch size:   256 |reserve_length:   336 | lm loss: 7.910426E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 256.82 | backward-compute: 199.62 | backward-embedding-all-reduce: 0.01 | optimizer: 13.75 | batch-generator: 8.54
[2022-10-31 10:54:35,696] [INFO] [logging.py:68:log_dist] [Rank 0] step=530, skipped=0, lr=[5.944251733333333e-06, 5.944251733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:35,699] [INFO] [timer.py:198:stop] 0/530, RunningAvgSamplesPerSec=427.76782943762504, CurrSamplesPerSec=415.74971579733176, MemAllocated=1.64GB, MaxMemAllocated=7.19GB
 iteration      530/  171661 | consumed samples:       135680 | consumed tokens:    277872640 | elapsed time per iteration (ms): 485.6 | learning rate: 5.944E-06 | global batch size:   256 |reserve_length:   336 | lm loss: 7.855273E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 257.61 | backward-compute: 199.95 | backward-embedding-all-reduce: 0.01 | optimizer: 13.73 | batch-generator: 8.52
[2022-10-31 10:54:40,545] [INFO] [logging.py:68:log_dist] [Rank 0] step=540, skipped=0, lr=[6.0667904e-06, 6.0667904e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:40,547] [INFO] [timer.py:198:stop] 0/540, RunningAvgSamplesPerSec=427.7310355323216, CurrSamplesPerSec=426.450847069888, MemAllocated=1.64GB, MaxMemAllocated=7.19GB
 iteration      540/  171661 | consumed samples:       138240 | consumed tokens:    283115520 | elapsed time per iteration (ms): 484.8 | learning rate: 6.067E-06 | global batch size:   256 |reserve_length:   336 | lm loss: 7.807861E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 257.28 | backward-compute: 199.56 | backward-embedding-all-reduce: 0.01 | optimizer: 13.74 | batch-generator: 8.55
[2022-10-31 10:54:45,388] [INFO] [logging.py:68:log_dist] [Rank 0] step=550, skipped=0, lr=[6.189329066666666e-06, 6.189329066666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:45,391] [INFO] [timer.py:198:stop] 0/550, RunningAvgSamplesPerSec=427.7211411296007, CurrSamplesPerSec=427.9479005582994, MemAllocated=1.64GB, MaxMemAllocated=7.19GB
 iteration      550/  171661 | consumed samples:       140800 | consumed tokens:    288358400 | elapsed time per iteration (ms): 484.4 | learning rate: 6.189E-06 | global batch size:   256 |reserve_length:   336 | lm loss: 7.724760E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 256.80 | backward-compute: 199.61 | backward-embedding-all-reduce: 0.01 | optimizer: 13.75 | batch-generator: 8.54
[2022-10-31 10:54:50,250] [INFO] [logging.py:68:log_dist] [Rank 0] step=560, skipped=0, lr=[6.311867733333332e-06, 6.311867733333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:50,253] [INFO] [timer.py:198:stop] 0/560, RunningAvgSamplesPerSec=427.6820113632935, CurrSamplesPerSec=426.71929902649634, MemAllocated=1.64GB, MaxMemAllocated=7.19GB
 iteration      560/  171661 | consumed samples:       143360 | consumed tokens:    293601280 | elapsed time per iteration (ms): 486.2 | learning rate: 6.312E-06 | global batch size:   256 |reserve_length:   336 | lm loss: 7.635006E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.55 | backward-compute: 199.59 | backward-embedding-all-reduce: 0.01 | optimizer: 13.76 | batch-generator: 8.53
[2022-10-31 10:54:55,121] [INFO] [logging.py:68:log_dist] [Rank 0] step=570, skipped=0, lr=[6.434406399999999e-06, 6.434406399999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:55,124] [INFO] [timer.py:198:stop] 0/570, RunningAvgSamplesPerSec=427.64068230603374, CurrSamplesPerSec=427.7324189184452, MemAllocated=1.64GB, MaxMemAllocated=7.19GB
 iteration      570/  171661 | consumed samples:       145920 | consumed tokens:    298844160 | elapsed time per iteration (ms): 487.0 | learning rate: 6.434E-06 | global batch size:   256 |reserve_length:   336 | lm loss: 7.654922E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.03 | backward-compute: 199.57 | backward-embedding-all-reduce: 0.01 | optimizer: 13.75 | batch-generator: 8.49
[2022-10-31 10:54:59,994] [INFO] [logging.py:68:log_dist] [Rank 0] step=580, skipped=0, lr=[6.559197866666666e-06, 6.559197866666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:54:59,997] [INFO] [timer.py:198:stop] 0/580, RunningAvgSamplesPerSec=427.5568111434164, CurrSamplesPerSec=426.5538492830265, MemAllocated=1.64GB, MaxMemAllocated=7.24GB
 iteration      580/  171661 | consumed samples:       148480 | consumed tokens:    304087040 | elapsed time per iteration (ms): 487.3 | learning rate: 6.559E-06 | global batch size:   256 |reserve_length:   352 | lm loss: 7.583697E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.87 | backward-compute: 200.12 | backward-embedding-all-reduce: 0.01 | optimizer: 13.77 | batch-generator: 8.51
[2022-10-31 10:55:04,859] [INFO] [logging.py:68:log_dist] [Rank 0] step=590, skipped=0, lr=[6.685491199999999e-06, 6.685491199999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:55:04,862] [INFO] [timer.py:198:stop] 0/590, RunningAvgSamplesPerSec=427.48644290426836, CurrSamplesPerSec=427.4980905271674, MemAllocated=1.64GB, MaxMemAllocated=7.24GB
 iteration      590/  171661 | consumed samples:       151040 | consumed tokens:    309329920 | elapsed time per iteration (ms): 486.5 | learning rate: 6.685E-06 | global batch size:   256 |reserve_length:   352 | lm loss: 7.551937E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 257.82 | backward-compute: 200.69 | backward-embedding-all-reduce: 0.01 | optimizer: 13.78 | batch-generator: 8.45
[2022-10-31 10:55:09,713] [INFO] [logging.py:68:log_dist] [Rank 0] step=600, skipped=0, lr=[6.811784533333332e-06, 6.811784533333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:55:09,716] [INFO] [timer.py:198:stop] 0/600, RunningAvgSamplesPerSec=427.4629848112017, CurrSamplesPerSec=422.9488053747109, MemAllocated=1.64GB, MaxMemAllocated=7.24GB
 iteration      600/  171661 | consumed samples:       153600 | consumed tokens:    314572800 | elapsed time per iteration (ms): 485.4 | learning rate: 6.812E-06 | global batch size:   256 |reserve_length:   352 | lm loss: 7.516389E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 256.97 | backward-compute: 200.38 | backward-embedding-all-reduce: 0.01 | optimizer: 13.78 | batch-generator: 8.49
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
[2022-10-31 10:55:10,500] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 19338
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain
    iteration = train(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 946, in train
    iteration = train(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 946, in train
        pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,    
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain
pretrain(train_valid_test_datasets_provider, model_provider, forward_step,    
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain
iteration = train(forward_step_func,

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 946, in train
        iteration = train(forward_step_func,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain

evaluate_and_print_results(prefix, forward_step_func,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 946, in train
evaluate_and_print_results(prefix, forward_step_func,

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1066, in evaluate_and_print_results
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1066, in evaluate_and_print_results
iteration = train(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 946, in train
    iteration = train(forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 946, in train
iteration = train(forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 946, in train
total_loss_dict = evaluate(forward_step_func, data_iterator, model, verbose)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1028, in evaluate
evaluate_and_print_results(prefix, forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1066, in evaluate_and_print_results
evaluate_and_print_results(prefix, forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1066, in evaluate_and_print_results
evaluate_and_print_results(prefix, forward_step_func,    
total_loss_dict = evaluate(forward_step_func, data_iterator, model, verbose)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1066, in evaluate_and_print_results
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1028, in evaluate
    evaluate_and_print_results(prefix, forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1066, in evaluate_and_print_results
    evaluate_and_print_results(prefix, forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1066, in evaluate_and_print_results
    loss_dicts = forward_backward_func(
    total_loss_dict = evaluate(forward_step_func, data_iterator, model, verbose)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1028, in evaluate
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
total_loss_dict = evaluate(forward_step_func, data_iterator, model, verbose)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1028, in evaluate
loss_dicts = forward_backward_func(
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
total_loss_dict = evaluate(forward_step_func, data_iterator, model, verbose)    
total_loss_dict = evaluate(forward_step_func, data_iterator, model, verbose)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1028, in evaluate
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1028, in evaluate
    total_loss_dict = evaluate(forward_step_func, data_iterator, model, verbose)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 1028, in evaluate
    loss_dicts = forward_backward_func(
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
loss_dicts = forward_backward_func(
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
loss_dicts = forward_backward_func(    
loss_dicts = forward_backward_func(
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
loss_dicts = forward_backward_func(    
output_tensor = forward_step(forward_step_func, data_iterator, model,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
        output_tensor = forward_step(forward_step_func, data_iterator, model,output_tensor = forward_step(forward_step_func, data_iterator, model,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining


    output_tensor = forward_step(forward_step_func, data_iterator, model,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
    output_tensor = forward_step(forward_step_func, data_iterator, model,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
output_tensor = forward_step(forward_step_func, data_iterator, model,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
output_tensor = forward_step(forward_step_func, data_iterator, model,        
    output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)


      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)    
    output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
output_tensor, *other_losses = model(tokens, position_ids, attention_mask,

      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
    output_tensor, *other_losses = model(tokens, position_ids, attention_mask,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
output_tensor, *other_losses = model(tokens, position_ids, attention_mask,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
output_tensor, *other_losses = model(tokens, position_ids, attention_mask,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
output_tensor, *other_losses = model(tokens, position_ids, attention_mask,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    output_tensor, *other_losses = model(tokens, position_ids, attention_mask,    output_tensor, *other_losses = model(tokens, position_ids, attention_mask,
return forward_call(*input, **kwargs)

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn

      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
return forward_call(*input, **kwargs)    
return forward_call(*input, **kwargs)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    
return forward_call(*input, **kwargs)            
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
return func(*args, **kwargs)return func(*args, **kwargs)
return func(*args, **kwargs)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward
    return func(*args, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)    
return func(*args, **kwargs)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward
    return func(*args, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward
loss = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    loss = self.module(*inputs, **kwargs)
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
loss = self.module(*inputs, **kwargs)
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
loss = self.module(*inputs, **kwargs)
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
loss = self.module(*inputs, **kwargs)
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
loss = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
loss = self.module(*inputs, **kwargs)
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
    lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    lm_output, *moe_losses = self.language_model(
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
lm_output, *moe_losses = self.language_model(
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
lm_output, *moe_losses = self.language_model(    
lm_output, *moe_losses = self.language_model(  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    encoder_output, *moe_losses = self.encoder(encoder_input,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
encoder_output, *moe_losses = self.encoder(encoder_input,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 813, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 813, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 813, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 813, in forward
return forward_call(*input, **kwargs)    
return forward_call(*input, **kwargs)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 813, in forward
    
return forward_call(*input, **kwargs)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 813, in forward
    
hidden_states, _ = layer(hidden_states,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 813, in forward

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    hidden_states, _ = layer(hidden_states,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
hidden_states, _ = layer(hidden_states,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
hidden_states, _ = layer(hidden_states,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
hidden_states, _ = layer(hidden_states,    
hidden_states, _ = layer(hidden_states,  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
hidden_states, _ = layer(hidden_states,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/dynamic_train/basic_layer.py", line 80, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/dynamic_train/basic_layer.py", line 80, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/dynamic_train/basic_layer.py", line 80, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/dynamic_train/basic_layer.py", line 80, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/dynamic_train/basic_layer.py", line 80, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/dynamic_train/basic_layer.py", line 80, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/dynamic_train/basic_layer.py", line 80, in forward
return self.randomltd_layer(hidden_states, **kwargs)
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
return self.randomltd_layer(hidden_states, **kwargs)        
return self.randomltd_layer(hidden_states, **kwargs)return self.randomltd_layer(hidden_states, **kwargs)  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    

return self.randomltd_layer(hidden_states, **kwargs)  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

return self.randomltd_layer(hidden_states, **kwargs)      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

return self.randomltd_layer(hidden_states, **kwargs)  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 484, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 484, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 484, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 484, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 484, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 484, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 484, in forward
    self.attention(layernorm_output,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    self.attention(layernorm_output,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    self.attention(layernorm_output,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    self.attention(layernorm_output,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    self.attention(layernorm_output,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
self.attention(layernorm_output,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
self.attention(layernorm_output,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 317, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 224, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 339, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 317, in forward
    attention_probs = self.scale_mask_softmax(attention_scores,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    value_layer) = mpu.split_tensor_along_last_dim(mixed_x_layer, 3)
    return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 317, in forward
KeyboardInterruptreturn forward_call(*input, **kwargs)
    
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 286, in forward
attention_probs = self.scale_mask_softmax(attention_scores,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
value_layer = value_layer.view(value_layer.size(0),
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 347, in forward
KeyboardInterrupt
        return forward_call(*input, **kwargs)attention_probs = self.scale_mask_softmax(attention_scores,    

matmul_result = torch.baddbmm(  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/fused_softmax.py", line 140, in forward
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

    KeyboardInterruptcontext_layer = torch.bmm(attention_probs, value_layer.transpose(0, 1))

KeyboardInterrupt    return forward_call(*input, **kwargs)

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/fused_softmax.py", line 144, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/fused_softmax.py", line 143, in forward
    if self.attn_mask_type == AttnMaskType.causal:
    probs = ScaledUpperTriangMaskedSoftmax.apply(input, scale)
KeyboardInterrupt
KeyboardInterrupt
    input = input.view(-1, query_seq_len, key_seq_len)
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/bin/deepspeed", line 6, in <module>
[2022-10-31 10:55:10,600] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 19338
    main()
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/launcher/runner.py", line 519, in main
    result.wait()
  File "/opt/conda/lib/python3.8/subprocess.py", line 1083, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.8/subprocess.py", line 1808, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.8/subprocess.py", line 1766, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/launcher/runner.py", line 511, in sigkill_handler
    result_kill = subprocess.Popen(kill_cmd, env=env)
NameError: free variable 'kill_cmd' referenced before assignment in enclosing scope
[2022-10-31 10:55:11,419] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 19339
[2022-10-31 10:55:12,529] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 19340
[2022-10-31 10:55:13,183] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 19342
[2022-10-31 10:55:13,557] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 19344
[2022-10-31 10:55:14,051] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 19346
[2022-10-31 10:55:14,052] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 19348
[2022-10-31 10:55:14,057] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 19350
[2022-10-31 10:55:14,192] [INFO] [launch.py:295:sigkill_handler] Main process received SIGTERM, exiting
