Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.251936674118042 seconds
Warning: Permanently added '[192.168.0.193]:41375' (ECDSA) to the list of known hosts.
[2022-10-31 10:59:02,323] [INFO] [runner.py:415:main] Using IP address of 192.168.0.193 for node worker-0
[2022-10-31 10:59:02,323] [INFO] [runner.py:504:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ3b3JrZXItMCI6IFswLCAxLCAyLCAzLCA0LCA1LCA2LCA3XX0= --master_addr=192.168.0.193 --master_port=29500 /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py --override-lr-scheduler --adam-beta1 0.9 --adam-beta2 0.95 --tensor-model-parallel-size 1 --init-method-std 0.02 --lr-decay-tokens 2600000000 --lr-warmup-tokens 3000000000 --micro-batch-size 4 --exit-duration-in-mins 30000000 --global-batch-size 256 --num-layers 12 --hidden-size 768 --num-attention-heads 12 --seq-length 2048 --max-position-embeddings 2048 --train-tokens 30000000000 --train-samples 43945312 --lr 3.0e-4 --min-lr 3.0e-5 --lr-decay-style cosine --split 98,2,0 --log-interval 10 --eval-interval 100 --eval-iters 10 --save-interval 10000 --weight-decay 0.1 --clip-grad 1.0 --hysteresis 2 --num-workers 0 --fp16 --increse-length-token-interval 0.05 --initial-sequence-length 256 --load /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4 --save /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4 --tensorboard-queue-size 1 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir tensorboard/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4_azwus2f200000DQ_2022.10.31-10.58.55 --log-optimizer-states-to-tensorboard --vocab-file /blob//data/the_pile_public_merged_nopreprocessing/gpt2-vocab.json --merge-file /blob//data/the_pile_public_merged_nopreprocessing/gpt2-merges.txt --data-path /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document --data-impl mmap --deepspeed --deepspeed_config ds_config_gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4.json --zero-stage 0 --pipeline-model-parallel-size 1 --no-pipeline-parallel
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.2450881004333496 seconds
[2022-10-31 10:59:04,770] [INFO] [launch.py:129:main] 0 NCCL_IB_PCI_RELAXED_ORDERING=1
[2022-10-31 10:59:04,771] [INFO] [launch.py:129:main] 0 NCCL_VERSION=2.9.8
[2022-10-31 10:59:04,774] [INFO] [launch.py:129:main] 0 NCCL_SOCKET_IFNAME=eth0
[2022-10-31 10:59:04,774] [INFO] [launch.py:129:main] 0 NCCL_NET_GDR_LEVEL=5
[2022-10-31 10:59:04,774] [INFO] [launch.py:129:main] 0 NCCL_DEBUG=INFO
[2022-10-31 10:59:04,774] [INFO] [launch.py:129:main] 0 NCCL_TREE_THRESHOLD=0
[2022-10-31 10:59:04,774] [INFO] [launch.py:129:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2022-10-31 10:59:04,774] [INFO] [launch.py:129:main] 0 NCCL_IB_TIMEOUT=20
[2022-10-31 10:59:04,778] [INFO] [launch.py:129:main] 0 NCCL_TOPO_FILE=/opt/msft/topo.xml
[2022-10-31 10:59:04,778] [INFO] [launch.py:136:main] WORLD INFO DICT: {'worker-0': [0, 1, 2, 3, 4, 5, 6, 7]}
[2022-10-31 10:59:04,778] [INFO] [launch.py:142:main] nnodes=1, num_local_procs=8, node_rank=0
[2022-10-31 10:59:04,778] [INFO] [launch.py:155:main] global_rank_mapping=defaultdict(<class 'list'>, {'worker-0': [0, 1, 2, 3, 4, 5, 6, 7]})
[2022-10-31 10:59:04,781] [INFO] [launch.py:156:main] dist_world_size=8
[2022-10-31 10:59:04,781] [INFO] [launch.py:158:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.33209681510925293 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.4412064552307129 seconds
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...
Building extension module token_dropping...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
ninja: no work to do.
Loading extension module token_dropping...
Time to load token_dropping op: 0.4883148670196533 seconds
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
using world size: 8, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
Detected CUDA files, patching ldflags
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/token_dropping/build.ninja...

  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_token_layers ........................... 0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning ............................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 8
  data_path ....................................... ['/blob/data/the_pile_public_merged_nopreprocessing/pile_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ ds_config_gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 2048
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 3072
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 768
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  increse_length_token_interval ................... 0.05
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  initial_sequence_length ......................... 256
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 64
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4
  load_teacher .................................... None
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. True
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0003
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 2600000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 3000000000
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /blob//data/the_pile_public_merged_nopreprocessing/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0Building extension module token_dropping...

  min_lr .......................................... 3e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)  moe_eval_capacity_factor ........................ 1.0

  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 12
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 12
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... True
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4
  save_interval ................................... 10000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 2048
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. tensorboard/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4_azwus2f200000DQ_2022.10.31-10.58.55
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_iters ..................................... None
  train_samples ................................... 43945312
  train_tokens .................................... 30000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /blob//data/the_pile_public_merged_nopreprocessing/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 8
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
ninja: no work to do.
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
[2022-10-31 10:59:08,631] [INFO] [comm.py:633:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading extension module token_dropping...
Time to load token_dropping op: 0.4899141788482666 seconds
Loading extension module token_dropping...
Time to load token_dropping op: 0.4398488998413086 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
Loading extension module token_dropping...
Time to load token_dropping op: 0.4225282669067383 seconds
Loading extension module token_dropping...
Time to load token_dropping op: 0.4058351516723633 seconds
Loading extension module token_dropping...
Time to load token_dropping op: 0.39885544776916504 seconds
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m sparse_attn.......  ............[92m[OKAY][0m 
[93m[NO][0m ....... stochastic_transformer[92m[OKAY][0m 
. transformer[93m[NO][0m  ...................  [93m[NO][0m[92m[OKAY][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m

sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. transformer_inference[93m[NO][0m  .........  [93m[NO][0m[92m[OKAY][0m 
....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0mtoken_dropping  ................  [92m[OKAY][0m[93m[NO][0m
 .......-------------------------------------------------- 
[92m[OKAY][0m
--------------------------------------------------
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
token_dropping ......... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
DeepSpeed general environment info:torch install path ............... 
transformer_inference['/opt/conda/lib/python3.8/site-packages/torch']torch install path
  torch version.................   ....................[93m[NO][0m  ['/opt/conda/lib/python3.8/site-packages/torch']1.11.0+cu113.......

 torch versiontorch cuda version[92m[OKAY][0m  
.................... ...............1.11.0+cu113
 torch cuda version11.3token_dropping 
 ...............torch hip version.........   11.3................[93m[NO][0m
  torch hip versionNone....... 
 ................nvcc version[92m[OKAY][0m  
None.....................
 --------------------------------------------------nvcc version11.3
 
.....................deepspeed install path  11.3...........
 deepspeed install path ...........['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed'] 
deepspeed info['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed'] 
deepspeed info ......................................  0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train

deepspeed wheel compiled w.deepspeed wheel compiled w.  ............  torch 1.11, cuda 11.3torch 1.11, cuda 11.3

DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed']
deepspeed info ................... 0.7.3+d505437d, d505437d, xiaoxia/token-drop-dynamic-train
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.3
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
**** Git info for Megatron: git_hash=ceadaa0 git_branch=xiaoxia/token-drop-dynamic-train ****
> setting tensorboard ...
2022-10-31 10:59:09.345682: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> compiling dataset index builder ...
make: Entering directory '/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/data'
>>> done with dataset index builder. Compilation time: 0.136 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
azwus2f200000DQ:22474:22474 [0] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:22474:22474 [0] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:22474:22474 [0] NCCL INFO P2P plugin IBext
azwus2f200000DQ:22474:22474 [0] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:22474:22474 [0] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:22474:22474 [0] NCCL INFO Using network IBext
NCCL version 2.10.3+cuda11.3azwus2f200000DQ:22476:22476 [2] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>

azwus2f200000DQ:22482:22482 [5] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:22478:22478 [3] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:22475:22475 [1] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:22480:22480 [4] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:22484:22484 [6] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:22476:22476 [2] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:22476:22476 [2] NCCL INFO P2P plugin IBext
azwus2f200000DQ:22476:22476 [2] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:22475:22475 [1] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:22480:22480 [4] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:22475:22475 [1] NCCL INFO P2P plugin IBext
azwus2f200000DQ:22484:22484 [6] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:22475:22475 [1] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:22480:22480 [4] NCCL INFO P2P plugin IBext
azwus2f200000DQ:22478:22478 [3] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:22480:22480 [4] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:22484:22484 [6] NCCL INFO P2P plugin IBext
azwus2f200000DQ:22482:22482 [5] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:22478:22478 [3] NCCL INFO P2P plugin IBext
azwus2f200000DQ:22484:22484 [6] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:22482:22482 [5] NCCL INFO P2P plugin IBext
azwus2f200000DQ:22478:22478 [3] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:22482:22482 [5] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:22480:22480 [4] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:22480:22480 [4] NCCL INFO Using network IBext
azwus2f200000DQ:22476:22476 [2] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:22476:22476 [2] NCCL INFO Using network IBext
azwus2f200000DQ:22478:22478 [3] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:22475:22475 [1] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:22475:22475 [1] NCCL INFO Using network IBext
azwus2f200000DQ:22478:22478 [3] NCCL INFO Using network IBext
azwus2f200000DQ:22484:22484 [6] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:22482:22482 [5] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:22484:22484 [6] NCCL INFO Using network IBext
azwus2f200000DQ:22482:22482 [5] NCCL INFO Using network IBext
azwus2f200000DQ:22486:22486 [7] NCCL INFO Bootstrap : Using eth0:192.168.0.193<0>
azwus2f200000DQ:22486:22486 [7] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins.install/lib/libnccl-net.so
azwus2f200000DQ:22486:22486 [7] NCCL INFO P2P plugin IBext
azwus2f200000DQ:22486:22486 [7] NCCL INFO NCCL_IB_PCI_RELAXED_ORDERING set by environment to 1.
azwus2f200000DQ:22486:22486 [7] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB/SHARP [1]mlx5_ib1:1/IB/SHARP [2]mlx5_ib2:1/IB/SHARP [3]mlx5_ib3:1/IB/SHARP [4]mlx5_ib4:1/IB/SHARP [5]mlx5_ib5:1/IB/SHARP [6]mlx5_ib6:1/IB/SHARP [7]mlx5_ib7:1/IB/SHARP ; OOB eth0:192.168.0.193<0>
azwus2f200000DQ:22486:22486 [7] NCCL INFO Using network IBext
azwus2f200000DQ:22478:23692 [3] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:22484:23694 [6] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:22480:23685 [4] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:22474:23640 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:22486:23703 [7] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:22475:23691 [1] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:22476:23686 [2] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:22482:23693 [5] NCCL INFO NCCL_IB_TIMEOUT set by environment to 20.
azwus2f200000DQ:22478:23692 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:22476:23686 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:22480:23685 [4] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:22475:23691 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:22486:23703 [7] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:22474:23640 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:22482:23693 [5] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:22484:23694 [6] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
azwus2f200000DQ:22482:23693 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
azwus2f200000DQ:22482:23693 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000DQ:22484:23694 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
azwus2f200000DQ:22486:23703 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22484:23694 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000DQ:22475:23691 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
azwus2f200000DQ:22486:23703 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22480:23685 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22475:23691 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000DQ:22476:23686 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22480:23685 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000DQ:22478:23692 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22476:23686 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22478:23692 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:23640 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
azwus2f200000DQ:22474:23640 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 00 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 01 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 00 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 00 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 00 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 02 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 01 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 00 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 01 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 00 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 01 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 03 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 02 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 00 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 02 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 01 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 01 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 02 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 04 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 03 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 00 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 01 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 03 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 02 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 02 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 03 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 04 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 05 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 01 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 02 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 04 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 03 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 03 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 04 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 06 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 05 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 02 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 03 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 05 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 04 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 04 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 07 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 06 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 05 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 03 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 04 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 06 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 05 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 08 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 05 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 07 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 06 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 04 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 05 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 07 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 06 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 09 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 06 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 08 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 07 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 06 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 05 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 08 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 07 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 10 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 07 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 09 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 08 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 07 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 06 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 09 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 08 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 11 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 08 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 09 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 10 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 08 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 07 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 10 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 09 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 12 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 09 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 10 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 11 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 09 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 08 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 11 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 10 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 13 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 10 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 11 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 12 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 09 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 10 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 12 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 11 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 14 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 11 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 12 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 13 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 11 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 10 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 13 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 12 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 15 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 12 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 13 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 14 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 12 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 11 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 14 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 13 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 16 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 13 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 14 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 13 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 15 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 12 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 15 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 14 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 17 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 14 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 15 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 14 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 16 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 13 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 16 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 15 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 18 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 15 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 16 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 15 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 17 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 14 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 17 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 16 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 19 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 16 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 17 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 16 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 18 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 15 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 18 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 17 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 17 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 20 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 18 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 17 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 19 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 16 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 19 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 18 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 18 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 21 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 19 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 18 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 20 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 17 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 20 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 19 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 19 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 22 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 20 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 19 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 21 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 18 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 21 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 20 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 20 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 23 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 21 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 20 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 22 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 19 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 22 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 21 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 21 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 22 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 21 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 23 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 20 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 23 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 22 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 22 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 23 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 22 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 21 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 23 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 23 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22474:23640 [0] NCCL INFO Channel 23 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 22 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 23 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Connected all rings
azwus2f200000DQ:22484:23694 [6] NCCL INFO Connected all rings
azwus2f200000DQ:22475:23691 [1] NCCL INFO Connected all rings
azwus2f200000DQ:22486:23703 [7] NCCL INFO Connected all rings
azwus2f200000DQ:22474:23640 [0] NCCL INFO Connected all rings
azwus2f200000DQ:22476:23686 [2] NCCL INFO Connected all rings
azwus2f200000DQ:22478:23692 [3] NCCL INFO Connected all rings
azwus2f200000DQ:22480:23685 [4] NCCL INFO Connected all rings
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 00 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 01 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 02 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 03 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 04 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 05 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 06 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 07 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 08 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 09 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 10 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 11 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 12 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 13 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 14 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 15 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 00 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 16 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 01 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 17 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 00 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 02 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 18 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 01 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 03 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 19 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 00 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 02 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 20 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 04 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 00 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 00 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 00 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 03 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 21 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 01 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 05 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 01 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 01 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 01 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 22 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 04 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 02 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 06 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 02 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 02 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 02 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Channel 23 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 05 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 03 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 07 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 03 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 03 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 03 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 06 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 08 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 04 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 04 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 04 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 04 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 07 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 05 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 09 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 05 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 05 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 05 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 08 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 06 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 10 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 06 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 06 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 06 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 07 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 09 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 11 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 07 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 07 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 07 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 08 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 10 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 12 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 08 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 08 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 08 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 09 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 11 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 13 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 09 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 09 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 10 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 09 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 12 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 14 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 10 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 10 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 11 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 10 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 13 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 15 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 11 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 11 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 12 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 11 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 14 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 16 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 12 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 12 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 13 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 12 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 15 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 17 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 13 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 13 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 13 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 14 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 16 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 18 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 14 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 14 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 14 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 15 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 17 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 19 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 15 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 15 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 15 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 16 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 18 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 20 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 16 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 16 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 16 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 17 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 19 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 21 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 17 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 17 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 17 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 18 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 20 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 22 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 18 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 18 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 18 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 21 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 19 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22482:23693 [5] NCCL INFO Channel 23 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 19 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 19 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 19 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 22 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 20 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 20 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 20 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 20 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:23694 [6] NCCL INFO Channel 23 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 21 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 21 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 21 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 21 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 22 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 22 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 22 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 22 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:23691 [1] NCCL INFO Channel 23 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22476:23686 [2] NCCL INFO Channel 23 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:23692 [3] NCCL INFO Channel 23 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:23685 [4] NCCL INFO Channel 23 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:23703 [7] NCCL INFO Connected all trees
azwus2f200000DQ:22486:23703 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22474:23640 [0] NCCL INFO Connected all trees
azwus2f200000DQ:22474:23640 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22486:23703 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22474:23640 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22484:23694 [6] NCCL INFO Connected all trees
azwus2f200000DQ:22484:23694 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22475:23691 [1] NCCL INFO Connected all trees
azwus2f200000DQ:22475:23691 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22476:23686 [2] NCCL INFO Connected all trees
azwus2f200000DQ:22476:23686 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22484:23694 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22482:23693 [5] NCCL INFO Connected all trees
azwus2f200000DQ:22480:23685 [4] NCCL INFO Connected all trees
azwus2f200000DQ:22478:23692 [3] NCCL INFO Connected all trees
azwus2f200000DQ:22480:23685 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22482:23693 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22478:23692 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22475:23691 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22476:23686 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22480:23685 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22482:23693 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22478:23692 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22482:23693 [5] NCCL INFO comm 0x7f8f54002fb0 rank 5 nranks 8 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000DQ:22478:23692 [3] NCCL INFO comm 0x7fcf98002fb0 rank 3 nranks 8 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000DQ:22476:23686 [2] NCCL INFO comm 0x7fddc8002fb0 rank 2 nranks 8 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000DQ:22474:23640 [0] NCCL INFO comm 0x7f873c002fb0 rank 0 nranks 8 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000DQ:22480:23685 [4] NCCL INFO comm 0x7fcf30002fb0 rank 4 nranks 8 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000DQ:22486:23703 [7] NCCL INFO comm 0x7fa9d8002fb0 rank 7 nranks 8 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000DQ:22484:23694 [6] NCCL INFO comm 0x7f42b8002fb0 rank 6 nranks 8 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000DQ:22475:23691 [1] NCCL INFO comm 0x7f6c8c002fb0 rank 1 nranks 8 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000DQ:22474:22474 [0] NCCL INFO Launch mode Parallel
>>> done with compiling and loading fused kernels. Compilation time: 14.663 seconds
time to initialize megatron (seconds): 61.528
[after megatron is initialized] datetime: 2022-10-31 10:59:25 
building GPT model ...
[2022-10-31 10:59:25,762] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2022-10-31 10:59:25,763] [INFO] [utils.py:828:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2022-10-31 10:59:25,763] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 56.16 GB, percent = 3.2%
[2022-10-31 10:59:25,859] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2022-10-31 10:59:25,860] [INFO] [utils.py:828:see_memory_usage] MA 0.24 GB         Max_MA 0.24 GB         CA 0.25 GB         Max_CA 0 GB 
[2022-10-31 10:59:25,865] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 56.2 GB, percent = 3.2%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 125262336
setting training iterations to 171661
> learning rate decay style: cosine
DeepSpeed is enabled.
[2022-10-31 10:59:25,876] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.3+d505437d, git-hash=d505437d, git-branch=xiaoxia/token-drop-dynamic-train
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22475:24567 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22476:24561 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22475:24567 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
azwus2f200000DQ:22476:24561 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000DQ:22474:24560 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
azwus2f200000DQ:22474:24560 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000DQ:22478:24563 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
azwus2f200000DQ:22478:24563 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000DQ:22482:24565 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
azwus2f200000DQ:22480:24564 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
azwus2f200000DQ:22484:24562 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
azwus2f200000DQ:22480:24564 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000DQ:22486:24566 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
azwus2f200000DQ:22484:24562 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000DQ:22486:24566 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000DQ:22482:24565 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 00 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 00 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 01 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 00 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 00 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 00 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 00 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 00 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 01 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 02 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 00 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 01 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 01 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 01 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 01 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 01 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 02 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 03 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 01 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 02 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 02 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 02 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 02 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 02 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 03 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 02 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 04 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 03 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 03 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 03 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 03 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 03 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 04 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 03 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 05 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 04 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 04 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 04 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 04 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 05 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 04 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 06 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 04 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 05 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 05 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 05 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 05 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 06 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 05 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 07 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 05 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 06 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 06 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 06 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 06 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 07 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 06 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 08 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 06 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 07 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 07 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 07 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 07 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 08 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 07 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 07 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 09 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 08 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 08 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 08 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 09 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 08 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 08 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 10 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 08 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 09 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 10 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 09 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 09 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 09 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 09 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 11 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 09 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 10 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 11 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 10 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 10 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 10 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 10 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 12 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 10 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 11 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 12 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 11 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 11 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 11 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 11 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 13 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 11 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 12 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 13 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 12 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 12 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 12 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 12 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 12 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 14 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 13 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 14 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 13 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 13 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 13 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 13 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 15 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 13 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 15 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 14 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 14 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 14 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 14 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 14 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 16 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 14 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 16 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 15 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 15 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 15 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 15 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 15 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 17 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 15 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 17 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 16 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 16 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 16 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 16 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 16 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 18 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 16 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 18 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 17 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 17 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 17 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 17 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 17 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 17 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 19 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 19 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 18 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 18 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 18 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 18 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 18 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 20 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 18 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 20 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 19 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 19 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 19 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 19 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 19 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 21 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 19 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 21 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 20 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 20 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 20 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 20 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 20 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 22 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 20 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 22 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 21 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 21 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 21 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 21 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 21 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 23 : 3[400000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 21 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 22 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22474:24560 [0] NCCL INFO Channel 23 : 0[100000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 22 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 22 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 22 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 22 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 22 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 23 : 1[200000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 23 : 6[d00000] -> 7[e00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 23 : 2[300000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 23 : 4[b00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 23 : 5[c00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 23 : 7[e00000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Connected all rings
azwus2f200000DQ:22476:24561 [2] NCCL INFO Connected all rings
azwus2f200000DQ:22475:24567 [1] NCCL INFO Connected all rings
azwus2f200000DQ:22474:24560 [0] NCCL INFO Connected all rings
azwus2f200000DQ:22480:24564 [4] NCCL INFO Connected all rings
azwus2f200000DQ:22478:24563 [3] NCCL INFO Connected all rings
azwus2f200000DQ:22482:24565 [5] NCCL INFO Connected all rings
azwus2f200000DQ:22484:24562 [6] NCCL INFO Connected all rings
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 00 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 01 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 02 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 03 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 04 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 05 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 06 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 07 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 08 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 09 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 10 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 11 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 12 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 13 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 14 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 15 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 16 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 17 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 18 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 19 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 20 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 21 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 22 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 00 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 00 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 00 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 00 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 00 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 00 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Channel 23 : 7[e00000] -> 6[d00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 01 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 01 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 01 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 01 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 01 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 01 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 02 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 02 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 02 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 02 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 02 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 02 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 03 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 03 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 03 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 03 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 03 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 03 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 04 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 04 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 04 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 04 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 04 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 04 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 05 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 05 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 05 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 05 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 05 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 05 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 06 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 06 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 06 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 06 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 06 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 06 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 07 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 07 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 07 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 07 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 07 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 07 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 08 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 08 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 08 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 08 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 08 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 08 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 09 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 09 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 09 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 09 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 09 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 09 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 10 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 10 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 10 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 10 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 10 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 10 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 11 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 11 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 11 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 11 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 11 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 11 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 12 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 12 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 12 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 12 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 12 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 12 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 13 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 13 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 13 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 13 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 13 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 14 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 13 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 14 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 14 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 14 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 14 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 15 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 14 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 15 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 15 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 15 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 15 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 16 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 15 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 16 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 16 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 16 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 16 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 17 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 16 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 17 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 17 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 17 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 17 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 18 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 17 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 18 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 18 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 19 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 18 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 18 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 19 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 18 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 19 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 20 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 19 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 19 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 20 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 19 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 20 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 21 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 20 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 20 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 21 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 20 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 21 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 22 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 21 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 21 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 22 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 21 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 22 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22476:24561 [2] NCCL INFO Channel 23 : 2[300000] -> 1[200000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 22 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 22 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 22 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22480:24564 [4] NCCL INFO Channel 23 : 4[b00000] -> 3[400000] via P2P/IPC/read
azwus2f200000DQ:22475:24567 [1] NCCL INFO Channel 23 : 1[200000] -> 0[100000] via P2P/IPC/read
azwus2f200000DQ:22484:24562 [6] NCCL INFO Channel 23 : 6[d00000] -> 5[c00000] via P2P/IPC/read
azwus2f200000DQ:22478:24563 [3] NCCL INFO Channel 23 : 3[400000] -> 2[300000] via P2P/IPC/read
azwus2f200000DQ:22482:24565 [5] NCCL INFO Channel 23 : 5[c00000] -> 4[b00000] via P2P/IPC/read
azwus2f200000DQ:22486:24566 [7] NCCL INFO Connected all trees
azwus2f200000DQ:22486:24566 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22474:24560 [0] NCCL INFO Connected all trees
azwus2f200000DQ:22474:24560 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22486:24566 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22474:24560 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22480:24564 [4] NCCL INFO Connected all trees
azwus2f200000DQ:22480:24564 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22475:24567 [1] NCCL INFO Connected all trees
azwus2f200000DQ:22478:24563 [3] NCCL INFO Connected all trees
azwus2f200000DQ:22476:24561 [2] NCCL INFO Connected all trees
azwus2f200000DQ:22482:24565 [5] NCCL INFO Connected all trees
azwus2f200000DQ:22484:24562 [6] NCCL INFO Connected all trees
azwus2f200000DQ:22478:24563 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22476:24561 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22484:24562 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22475:24567 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22482:24565 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
azwus2f200000DQ:22480:24564 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22478:24563 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22484:24562 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22482:24565 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22475:24567 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22476:24561 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22475:24567 [1] NCCL INFO comm 0x7f67e8002fb0 rank 1 nranks 8 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000DQ:22476:24561 [2] NCCL INFO comm 0x7fd874002fb0 rank 2 nranks 8 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000DQ:22478:24563 [3] NCCL INFO comm 0x7fca7c002fb0 rank 3 nranks 8 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000DQ:22480:24564 [4] NCCL INFO comm 0x7fca24002fb0 rank 4 nranks 8 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000DQ:22474:24560 [0] NCCL INFO comm 0x7f81f0002fb0 rank 0 nranks 8 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000DQ:22482:24565 [5] NCCL INFO comm 0x7f8a54002fb0 rank 5 nranks 8 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000DQ:22484:24562 [6] NCCL INFO comm 0x7f3dbc002fb0 rank 6 nranks 8 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000DQ:22486:24566 [7] NCCL INFO comm 0x7fa540002fb0 rank 7 nranks 8 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000DQ:22474:22474 [0] NCCL INFO Launch mode Parallel
[2022-10-31 10:59:30,884] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-10-31 10:59:30,885] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-10-31 10:59:30,888] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-10-31 10:59:30,891] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = {basic_optimizer.__class__.__name__}
[2022-10-31 10:59:30,905] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...

[2022-10-31 10:59:30,969] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2022-10-31 10:59:30,970] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2022-10-31 10:59:30,973] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f8933f14c10>
[2022-10-31 10:59:30,973] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:59:30,974] [INFO] [config.py:978:print] DeepSpeedEngine configuration:
[2022-10-31 10:59:30,977] [INFO] [config.py:982:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-10-31 10:59:30,980] [INFO] [config.py:982:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-10-31 10:59:30,980] [INFO] [config.py:982:print]   amp_enabled .................. False
[2022-10-31 10:59:30,980] [INFO] [config.py:982:print]   amp_params ................... False
[2022-10-31 10:59:30,980] [INFO] [config.py:982:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-10-31 10:59:30,984] [INFO] [config.py:982:print]   bfloat16_enabled ............. False
[2022-10-31 10:59:30,984] [INFO] [config.py:982:print]   checkpoint_tag_validation_enabled  True
[2022-10-31 10:59:30,984] [INFO] [config.py:982:print]   checkpoint_tag_validation_fail  False
[2022-10-31 10:59:30,984] [INFO] [config.py:982:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f893396ae50>
[2022-10-31 10:59:30,984] [INFO] [config.py:982:print]   communication_data_type ...... None
[2022-10-31 10:59:30,984] [INFO] [config.py:982:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-10-31 10:59:30,984] [INFO] [config.py:982:print]   curriculum_enabled_legacy .... False
[2022-10-31 10:59:30,984] [INFO] [config.py:982:print]   curriculum_params_legacy ..... {'curriculum_type': 'seqlen', 'min_difficulty': 72, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 221108, 'difficulty_step': 8}}
[2022-10-31 10:59:30,988] [INFO] [config.py:982:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}}
[2022-10-31 10:59:30,988] [INFO] [config.py:982:print]   data_efficiency_enabled ...... False
[2022-10-31 10:59:30,988] [INFO] [config.py:982:print]   dataloader_drop_last ......... False
[2022-10-31 10:59:30,988] [INFO] [config.py:982:print]   disable_allgather ............ False
[2022-10-31 10:59:30,988] [INFO] [config.py:982:print]   dump_state ................... False
[2022-10-31 10:59:30,988] [INFO] [config.py:982:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2022-10-31 10:59:30,988] [INFO] [config.py:982:print]   dynamic_train_config ......... {'random_ltd': {'enabled': False}}
[2022-10-31 10:59:30,988] [INFO] [config.py:982:print]   eigenvalue_enabled ........... False
[2022-10-31 10:59:30,988] [INFO] [config.py:982:print]   eigenvalue_gas_boundary_resolution  1
[2022-10-31 10:59:30,988] [INFO] [config.py:982:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-10-31 10:59:30,988] [INFO] [config.py:982:print]   eigenvalue_layer_num ......... 0
[2022-10-31 10:59:30,991] [INFO] [config.py:982:print]   eigenvalue_max_iter .......... 100
[2022-10-31 10:59:30,991] [INFO] [config.py:982:print]   eigenvalue_stability ......... 1e-06
[2022-10-31 10:59:30,991] [INFO] [config.py:982:print]   eigenvalue_tol ............... 0.01
[2022-10-31 10:59:30,991] [INFO] [config.py:982:print]   eigenvalue_verbose ........... False
[2022-10-31 10:59:30,991] [INFO] [config.py:982:print]   elasticity_enabled ........... False
[2022-10-31 10:59:30,991] [INFO] [config.py:982:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-10-31 10:59:30,991] [INFO] [config.py:982:print]   fp16_auto_cast ............... False
[2022-10-31 10:59:30,992] [INFO] [config.py:982:print]   fp16_enabled ................. True
[2022-10-31 10:59:30,992] [INFO] [config.py:982:print]   fp16_master_weights_and_gradients  False
[2022-10-31 10:59:30,992] [INFO] [config.py:982:print]   global_rank .................. 0
[2022-10-31 10:59:30,995] [INFO] [config.py:982:print]   gradient_accumulation_steps .. 8
[2022-10-31 10:59:30,995] [INFO] [config.py:982:print]   gradient_clipping ............ 1.0
[2022-10-31 10:59:30,995] [INFO] [config.py:982:print]   gradient_predivide_factor .... 1.0
[2022-10-31 10:59:30,995] [INFO] [config.py:982:print]   initial_dynamic_scale ........ 2048
[2022-10-31 10:59:30,995] [INFO] [config.py:982:print]   load_universal_checkpoint .... False
[2022-10-31 10:59:30,995] [INFO] [config.py:982:print]   loss_scale ................... 0
[2022-10-31 10:59:31,000] [INFO] [config.py:982:print]   memory_breakdown ............. False
[2022-10-31 10:59:31,000] [INFO] [config.py:982:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f893396af70>
[2022-10-31 10:59:31,000] [INFO] [config.py:982:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-10-31 10:59:31,000] [INFO] [config.py:982:print]   optimizer_legacy_fusion ...... False
[2022-10-31 10:59:31,000] [INFO] [config.py:982:print]   optimizer_name ............... None
[2022-10-31 10:59:31,000] [INFO] [config.py:982:print]   optimizer_params ............. None
[2022-10-31 10:59:31,000] [INFO] [config.py:982:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-10-31 10:59:31,003] [INFO] [config.py:982:print]   pld_enabled .................. False
[2022-10-31 10:59:31,003] [INFO] [config.py:982:print]   pld_params ................... False
[2022-10-31 10:59:31,003] [INFO] [config.py:982:print]   prescale_gradients ........... True
[2022-10-31 10:59:31,003] [INFO] [config.py:982:print]   scheduler_name ............... None
[2022-10-31 10:59:31,004] [INFO] [config.py:982:print]   scheduler_params ............. None
[2022-10-31 10:59:31,004] [INFO] [config.py:982:print]   sparse_attention ............. None
[2022-10-31 10:59:31,004] [INFO] [config.py:982:print]   sparse_gradients_enabled ..... False
[2022-10-31 10:59:31,004] [INFO] [config.py:982:print]   steps_per_print .............. 10
[2022-10-31 10:59:31,004] [INFO] [config.py:982:print]   train_batch_size ............. 256
[2022-10-31 10:59:31,004] [INFO] [config.py:982:print]   train_micro_batch_size_per_gpu  4
[2022-10-31 10:59:31,004] [INFO] [config.py:982:print]   wall_clock_breakdown ......... False
[2022-10-31 10:59:31,007] [INFO] [config.py:982:print]   world_size ................... 8
[2022-10-31 10:59:31,007] [INFO] [config.py:982:print]   zero_allow_untested_optimizer  False
[2022-10-31 10:59:31,007] [INFO] [config.py:982:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-10-31 10:59:31,010] [INFO] [config.py:982:print]   zero_enabled ................. False
[2022-10-31 10:59:31,010] [INFO] [config.py:982:print]   zero_optimization_stage ...... 0
[2022-10-31 10:59:31,011] [INFO] [config.py:967:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 4, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 72, 
        "max_difficulty": 2.048000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 2.211080e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false, 
    "dynamic_train": {
        "random_ltd": {
            "enabled": false, 
            "total_layer_num": 12, 
            "randomltd_layer_num": 10, 
            "randomltd_layer_id": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 
            "model_mask_name": "attention_mask", 
            "model_type": "decoder", 
            "hidden_state_order": "batch_seq_dim", 
            "micro_batch_size": 4, 
            "randomltd_schedule": {
                "min_value": 256, 
                "max_value": 2.048000e+03, 
                "schedule_type": "fixed_linear", 
                "schedule_config": {
                    "require_steps": 1.068200e+04, 
                    "seq_per_step": 16, 
                    "saving_layer_tokens": -1
                }
            }, 
            "layer_token_lr_schedule": {
                "enabled": false, 
                "warmup_type": "linear", 
                "total_layer_tokens": "by_iteration", 
                "warmup_layer_tokens": "by_iteration", 
                "total_iterations": -1, 
                "warmup_iterations": -1
            }
        }
    }
}
Using /home/xiaoxiawu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Emitting ninja build file /home/xiaoxiawu/.cache/torch_extensions/py38_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.4432332515716553 seconds
Loading extension module utils...
Time to load utils op: 0.40705156326293945 seconds
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.5095157623291016 seconds
Loading extension module utils...
Time to load utils op: 0.5133278369903564 seconds
Time to load utils op: 0.5116710662841797 seconds
Time to load utils op: 0.512000560760498 seconds
Time to load utils op: 0.5121805667877197 seconds
Time to load utils op: 0.5138938426971436 seconds
[2022-10-31 10:59:31,689] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:59:31,901] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:59:32,115] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:59:32,314] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:59:32,510] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:59:32,708] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2022-10-31 10:59:32,918] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
WARNING: could not find the metadata file /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4 
    will not load any checkpoints and will start from random
[2022-10-31 10:59:33,114] [WARNING] [engine.py:2645:load_checkpoint] Unable to find latest file at /blob/users/xiaoxiawu/project/tokendropping/checkpoint/gpt3-0.125B-tokendropping-graduallyIncrease-lr-3.0e-4/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
time (ms) | load-checkpoint: 1637.26
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-10-31 10:59:33 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      43945312
    validation: 4395520
    test:       2560
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.007924 seconds
    number of documents: 210604984
 > dataset split:
    train:
     document indices in [0, 206392884) total of 206392884 documents
    validation:
     document indices in [206392884, 210604984) total of 4212100 documents
    test:
     document indices in [210604984, 210604984) total of 0 documents
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
NCCL version 2.10.3+cuda11.3
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22482:24612 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22482:24612 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22475:24606 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22474:24615 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22480:24597 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22478:24609 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22484:24600 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000DQ:22478:24609 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000DQ:22486:24604 [7] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22486:24604 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22486:24604 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22476:24619 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22476:24619 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000DQ:22482:24612 [5] NCCL INFO Connected all rings
azwus2f200000DQ:22482:24612 [5] NCCL INFO Connected all trees
azwus2f200000DQ:22482:24612 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22475:24606 [1] NCCL INFO Connected all rings
azwus2f200000DQ:22482:24612 [5] NCCL INFO comm 0x7ed9b8002fb0 rank 0 nranks 1 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000DQ:22475:24606 [1] NCCL INFO Connected all trees
azwus2f200000DQ:22474:24615 [0] NCCL INFO Connected all rings
azwus2f200000DQ:22475:24606 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22474:24615 [0] NCCL INFO Connected all trees
azwus2f200000DQ:22474:24615 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22480:24597 [4] NCCL INFO Connected all rings
azwus2f200000DQ:22475:24606 [1] NCCL INFO comm 0x7eb740002fb0 rank 0 nranks 1 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000DQ:22480:24597 [4] NCCL INFO Connected all trees
azwus2f200000DQ:22480:24597 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22474:24615 [0] NCCL INFO comm 0x7ed1ac002fb0 rank 0 nranks 1 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000DQ:22478:24609 [3] NCCL INFO Connected all rings
azwus2f200000DQ:22484:24600 [6] NCCL INFO Connected all rings
azwus2f200000DQ:22478:24609 [3] NCCL INFO Connected all trees
azwus2f200000DQ:22486:24604 [7] NCCL INFO Connected all rings
azwus2f200000DQ:22484:24600 [6] NCCL INFO Connected all trees
azwus2f200000DQ:22486:24604 [7] NCCL INFO Connected all trees
azwus2f200000DQ:22486:24604 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22478:24609 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22484:24600 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22480:24597 [4] NCCL INFO comm 0x7f198c002fb0 rank 0 nranks 1 cudaDev 4 busId b00000 - Init COMPLETE
 > loading doc-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_train_indexmap_43945312ns_2048sl_1234s_doc_idx.npy
azwus2f200000DQ:22486:24604 [7] NCCL INFO comm 0x7ef4b8002fb0 rank 0 nranks 1 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000DQ:22478:24609 [3] NCCL INFO comm 0x7f1a14002fb0 rank 0 nranks 1 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000DQ:22484:24600 [6] NCCL INFO comm 0x7e8cf4002fb0 rank 0 nranks 1 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000DQ:22476:24619 [2] NCCL INFO Connected all rings
 > loading sample-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_train_indexmap_43945312ns_2048sl_1234s_sample_idx.npyazwus2f200000DQ:22476:24619 [2] NCCL INFO Connected all trees

azwus2f200000DQ:22476:24619 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22476:24619 [2] NCCL INFO comm 0x7f27f8002fb0 rank 0 nranks 1 cudaDev 2 busId 300000 - Init COMPLETE
 > loading shuffle-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_train_indexmap_43945312ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.014 seconds
    total number of samples: 179130331
    total number of epochs: 1
 > loading doc-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_valid_indexmap_4395520ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_valid_indexmap_4395520ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /blob/data/the_pile_public_merged_nopreprocessing/pile_text_document_valid_indexmap_4395520ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.106 seconds
    total number of samples: 7303184
    total number of epochs: 2
> finished creating GPT datasets ...
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22480:24636 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22480:24636 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22486:24641 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22486:24641 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22482:24635 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22482:24635 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22478:24642 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22478:24642 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22484:24637 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22484:24637 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22475:24633 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22475:24633 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22476:24638 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22476:24638 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22474:24651 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22474:24651 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000DQ:22480:24636 [4] NCCL INFO Connected all rings
azwus2f200000DQ:22480:24636 [4] NCCL INFO Connected all trees
azwus2f200000DQ:22480:24636 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22480:24636 [4] NCCL INFO comm 0x7f19bc002fb0 rank 0 nranks 1 cudaDev 4 busId b00000 - Init COMPLETE
azwus2f200000DQ:22486:24641 [7] NCCL INFO Connected all rings
azwus2f200000DQ:22486:24641 [7] NCCL INFO Connected all trees
azwus2f200000DQ:22486:24641 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22486:24641 [7] NCCL INFO comm 0x7ef4f0002fb0 rank 0 nranks 1 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000DQ:22482:24635 [5] NCCL INFO Connected all rings
azwus2f200000DQ:22482:24635 [5] NCCL INFO Connected all trees
azwus2f200000DQ:22482:24635 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22482:24635 [5] NCCL INFO comm 0x7ed9e4002fb0 rank 0 nranks 1 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000DQ:22478:24642 [3] NCCL INFO Connected all rings
azwus2f200000DQ:22484:24637 [6] NCCL INFO Connected all rings
azwus2f200000DQ:22484:24637 [6] NCCL INFO Connected all trees
azwus2f200000DQ:22478:24642 [3] NCCL INFO Connected all trees
azwus2f200000DQ:22484:24637 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22478:24642 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22484:24637 [6] NCCL INFO comm 0x7e8d24002fb0 rank 0 nranks 1 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000DQ:22478:24642 [3] NCCL INFO comm 0x7f1a40002fb0 rank 0 nranks 1 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000DQ:22475:24633 [1] NCCL INFO Connected all rings
azwus2f200000DQ:22475:24633 [1] NCCL INFO Connected all trees
azwus2f200000DQ:22475:24633 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22476:24638 [2] NCCL INFO Connected all rings
azwus2f200000DQ:22476:24638 [2] NCCL INFO Connected all trees
azwus2f200000DQ:22476:24638 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22475:24633 [1] NCCL INFO comm 0x7eb778002fb0 rank 0 nranks 1 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000DQ:22476:24638 [2] NCCL INFO comm 0x7f2830002fb0 rank 0 nranks 1 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000DQ:22474:24651 [0] NCCL INFO Connected all rings
azwus2f200000DQ:22474:24651 [0] NCCL INFO Connected all trees
azwus2f200000DQ:22474:24651 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22474:24651 [0] NCCL INFO comm 0x7ed1dc002fb0 rank 0 nranks 1 cudaDev 0 busId 100000 - Init COMPLETE
[after dataloaders are built] datetime: 2022-10-31 10:59:41 
time (ms) | model-and-optimizer-setup: 7586.05 | train/valid/test-data-iterators-setup: 8260.22done with setup ...

training ...
[before the start of training step] datetime: 2022-10-31 10:59:41 
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22484:24745 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22484:24745 [6] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22476:24743 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22476:24743 [2] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22478:24746 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22478:24746 [3] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22482:24740 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22482:24740 [5] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22486:24739 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22486:24739 [7] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22475:24744 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22475:24744 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22474:24737 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22474:24737 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 00/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 01/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 02/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 03/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 04/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 05/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 06/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 07/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 08/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 09/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 10/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 11/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 12/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 13/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 14/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 15/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 16/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 17/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 18/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 19/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 20/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 21/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 22/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 23/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 24/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 25/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 26/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 27/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 28/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 29/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 30/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Channel 31/32 :    0
azwus2f200000DQ:22480:24741 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
azwus2f200000DQ:22480:24741 [4] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
azwus2f200000DQ:22484:24745 [6] NCCL INFO Connected all rings
azwus2f200000DQ:22484:24745 [6] NCCL INFO Connected all trees
azwus2f200000DQ:22484:24745 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22476:24743 [2] NCCL INFO Connected all rings
azwus2f200000DQ:22476:24743 [2] NCCL INFO Connected all trees
azwus2f200000DQ:22476:24743 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22484:24745 [6] NCCL INFO comm 0x7e8a40002fb0 rank 0 nranks 1 cudaDev 6 busId d00000 - Init COMPLETE
azwus2f200000DQ:22476:24743 [2] NCCL INFO comm 0x7f2544002fb0 rank 0 nranks 1 cudaDev 2 busId 300000 - Init COMPLETE
azwus2f200000DQ:22478:24746 [3] NCCL INFO Connected all rings
azwus2f200000DQ:22478:24746 [3] NCCL INFO Connected all trees
azwus2f200000DQ:22478:24746 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22478:24746 [3] NCCL INFO comm 0x7f1750002fb0 rank 0 nranks 1 cudaDev 3 busId 400000 - Init COMPLETE
azwus2f200000DQ:22482:24740 [5] NCCL INFO Connected all rings
azwus2f200000DQ:22486:24739 [7] NCCL INFO Connected all rings
azwus2f200000DQ:22482:24740 [5] NCCL INFO Connected all trees
azwus2f200000DQ:22486:24739 [7] NCCL INFO Connected all trees
azwus2f200000DQ:22482:24740 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22486:24739 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22482:24740 [5] NCCL INFO comm 0x7ed6f4002fb0 rank 0 nranks 1 cudaDev 5 busId c00000 - Init COMPLETE
azwus2f200000DQ:22486:24739 [7] NCCL INFO comm 0x7ef204002fb0 rank 0 nranks 1 cudaDev 7 busId e00000 - Init COMPLETE
azwus2f200000DQ:22475:24744 [1] NCCL INFO Connected all rings
azwus2f200000DQ:22474:24737 [0] NCCL INFO Connected all rings
azwus2f200000DQ:22475:24744 [1] NCCL INFO Connected all trees
azwus2f200000DQ:22474:24737 [0] NCCL INFO Connected all trees
azwus2f200000DQ:22474:24737 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22475:24744 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22474:24737 [0] NCCL INFO comm 0x7ecef8002fb0 rank 0 nranks 1 cudaDev 0 busId 100000 - Init COMPLETE
azwus2f200000DQ:22475:24744 [1] NCCL INFO comm 0x7eb494002fb0 rank 0 nranks 1 cudaDev 1 busId 200000 - Init COMPLETE
azwus2f200000DQ:22480:24741 [4] NCCL INFO Connected all rings
azwus2f200000DQ:22480:24741 [4] NCCL INFO Connected all trees
azwus2f200000DQ:22480:24741 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
azwus2f200000DQ:22480:24741 [4] NCCL INFO comm 0x7f16d8002fb0 rank 0 nranks 1 cudaDev 4 busId b00000 - Init COMPLETE
[2022-10-31 10:59:51,434] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[9.338879999999999e-08, 9.338879999999999e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:59:51,436] [INFO] [timer.py:198:stop] 0/10, RunningAvgSamplesPerSec=428.0729589385703, CurrSamplesPerSec=431.9529870656501, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
[Rank 0] (after 10 iterations) memory (MB) | allocated: 1674.31005859375 | max allocated: 7176.13916015625 | reserved: 8774.0 | max reserved: 8774.0
 iteration       10/  171661 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 1001.5 | learning rate: 9.339E-08 | global batch size:   256 |reserve_length:   256 | lm loss: 1.078335E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 467.20 | backward-compute: 212.86 | backward-embedding-all-reduce: 0.01 | optimizer: 301.68 | batch-generator: 11.59
[2022-10-31 10:59:56,281] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[1.9715413333333332e-07, 1.9715413333333332e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 10:59:56,284] [INFO] [timer.py:198:stop] 0/20, RunningAvgSamplesPerSec=427.2631875502374, CurrSamplesPerSec=431.44894980809164, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       20/  171661 | consumed samples:         5120 | consumed tokens:     10485760 | elapsed time per iteration (ms): 484.7 | learning rate: 1.972E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.074999E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.12 | backward-compute: 191.56 | backward-embedding-all-reduce: 0.01 | optimizer: 14.15 | batch-generator: 10.29
[2022-10-31 11:00:01,125] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[3.0091946666666664e-07, 3.0091946666666664e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:01,128] [INFO] [timer.py:198:stop] 0/30, RunningAvgSamplesPerSec=425.96347213212823, CurrSamplesPerSec=427.7951316842129, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       30/  171661 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 484.5 | learning rate: 3.009E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.071889E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.71 | backward-compute: 192.07 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 9.61
[2022-10-31 11:00:05,972] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[4.0468479999999994e-07, 4.0468479999999994e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:05,975] [INFO] [timer.py:198:stop] 0/40, RunningAvgSamplesPerSec=426.4146226617011, CurrSamplesPerSec=420.7320397479703, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       40/  171661 | consumed samples:        10240 | consumed tokens:     20971520 | elapsed time per iteration (ms): 484.6 | learning rate: 4.047E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.066174E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.06 | backward-compute: 191.45 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 9.85
[2022-10-31 11:00:10,809] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[5.084501333333333e-07, 5.084501333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:10,812] [INFO] [timer.py:198:stop] 0/50, RunningAvgSamplesPerSec=426.8371075826525, CurrSamplesPerSec=430.96961134372833, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       50/  171661 | consumed samples:        12800 | consumed tokens:     26214400 | elapsed time per iteration (ms): 483.7 | learning rate: 5.085E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.056701E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.42 | backward-compute: 191.46 | backward-embedding-all-reduce: 0.01 | optimizer: 14.12 | batch-generator: 9.73
[2022-10-31 11:00:15,660] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[6.122154666666666e-07, 6.122154666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:15,662] [INFO] [timer.py:198:stop] 0/60, RunningAvgSamplesPerSec=426.71740437305095, CurrSamplesPerSec=430.54519325461365, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       60/  171661 | consumed samples:        15360 | consumed tokens:     31457280 | elapsed time per iteration (ms): 484.8 | learning rate: 6.122E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.047988E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.67 | backward-compute: 191.90 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 10.15
[2022-10-31 11:00:20,451] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[7.159807999999999e-07, 7.159807999999999e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:20,453] [INFO] [timer.py:198:stop] 0/70, RunningAvgSamplesPerSec=426.9929634251937, CurrSamplesPerSec=425.43029665247695, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       70/  171661 | consumed samples:        17920 | consumed tokens:     36700160 | elapsed time per iteration (ms): 479.1 | learning rate: 7.160E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.033031E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 257.18 | backward-compute: 191.67 | backward-embedding-all-reduce: 0.01 | optimizer: 13.93 | batch-generator: 9.60
[2022-10-31 11:00:25,265] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[8.197461333333333e-07, 8.197461333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:25,268] [INFO] [timer.py:198:stop] 0/80, RunningAvgSamplesPerSec=427.1588258201022, CurrSamplesPerSec=429.94938030758783, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       80/  171661 | consumed samples:        20480 | consumed tokens:     41943040 | elapsed time per iteration (ms): 481.6 | learning rate: 8.197E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.022072E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.18 | backward-compute: 191.60 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 9.72
[2022-10-31 11:00:30,095] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[9.235114666666666e-07, 9.235114666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:30,098] [INFO] [timer.py:198:stop] 0/90, RunningAvgSamplesPerSec=427.138258524034, CurrSamplesPerSec=428.3344013684466, MemAllocated=1.64GB, MaxMemAllocated=7.01GB
 iteration       90/  171661 | consumed samples:        23040 | consumed tokens:     47185920 | elapsed time per iteration (ms): 483.0 | learning rate: 9.235E-07 | global batch size:   256 |reserve_length:   256 | lm loss: 1.005317E+01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.25 | backward-compute: 191.70 | backward-embedding-all-reduce: 0.01 | optimizer: 14.24 | batch-generator: 9.85
[2022-10-31 11:00:34,940] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[1.0284032e-06, 1.0284032e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:34,942] [INFO] [timer.py:198:stop] 0/100, RunningAvgSamplesPerSec=427.1142479716977, CurrSamplesPerSec=420.40916508746926, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      100/  171661 | consumed samples:        25600 | consumed tokens:     52428800 | elapsed time per iteration (ms): 484.4 | learning rate: 1.028E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.932045E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.37 | backward-compute: 192.59 | backward-embedding-all-reduce: 0.01 | optimizer: 13.93 | batch-generator: 9.42
-----------------------------------------------------------------------------------------------
 validation loss at iteration 100 | lm loss value: 9.661073E+00 | lm loss PPL: 1.569461E+04 | 
-----------------------------------------------------------------------------------------------
[2022-10-31 11:00:42,968] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[1.1359231999999999e-06, 1.1359231999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:42,970] [INFO] [timer.py:198:stop] 0/110, RunningAvgSamplesPerSec=427.12121428828414, CurrSamplesPerSec=425.7622834593216, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      110/  171661 | consumed samples:        28160 | consumed tokens:     57671680 | elapsed time per iteration (ms): 802.8 | learning rate: 1.136E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.845078E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 576.01 | backward-compute: 194.33 | backward-embedding-all-reduce: 0.01 | optimizer: 14.04 | batch-generator: 17.20
[2022-10-31 11:00:47,829] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[1.2434431999999998e-06, 1.2434431999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:47,832] [INFO] [timer.py:198:stop] 0/120, RunningAvgSamplesPerSec=427.05880577711406, CurrSamplesPerSec=426.98401407397745, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      120/  171661 | consumed samples:        30720 | consumed tokens:     62914560 | elapsed time per iteration (ms): 486.1 | learning rate: 1.243E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.703465E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.30 | backward-compute: 194.13 | backward-embedding-all-reduce: 0.01 | optimizer: 14.10 | batch-generator: 9.86
[2022-10-31 11:00:52,681] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[1.3509632e-06, 1.3509632e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:52,683] [INFO] [timer.py:198:stop] 0/130, RunningAvgSamplesPerSec=427.0626858221566, CurrSamplesPerSec=427.27626033031544, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      130/  171661 | consumed samples:        33280 | consumed tokens:     68157440 | elapsed time per iteration (ms): 485.2 | learning rate: 1.351E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.552262E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.13 | backward-compute: 194.18 | backward-embedding-all-reduce: 0.01 | optimizer: 13.90 | batch-generator: 10.11
[2022-10-31 11:00:57,532] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[1.4584832e-06, 1.4584832e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:00:57,535] [INFO] [timer.py:198:stop] 0/140, RunningAvgSamplesPerSec=427.0266293387032, CurrSamplesPerSec=427.93425604433094, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      140/  171661 | consumed samples:        35840 | consumed tokens:     73400320 | elapsed time per iteration (ms): 485.1 | learning rate: 1.458E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.416933E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.49 | backward-compute: 194.26 | backward-embedding-all-reduce: 0.01 | optimizer: 13.86 | batch-generator: 9.89
[2022-10-31 11:01:02,391] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[1.5660031999999999e-06, 1.5660031999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:02,394] [INFO] [timer.py:198:stop] 0/150, RunningAvgSamplesPerSec=427.01087999171096, CurrSamplesPerSec=427.73787147250505, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      150/  171661 | consumed samples:        38400 | consumed tokens:     78643200 | elapsed time per iteration (ms): 485.9 | learning rate: 1.566E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.399093E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.03 | backward-compute: 194.10 | backward-embedding-all-reduce: 0.01 | optimizer: 13.92 | batch-generator: 9.90
[2022-10-31 11:01:07,239] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[1.6735231999999998e-06, 1.6735231999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:07,241] [INFO] [timer.py:198:stop] 0/160, RunningAvgSamplesPerSec=426.94601789869785, CurrSamplesPerSec=427.7324189184452, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      160/  171661 | consumed samples:        40960 | consumed tokens:     83886080 | elapsed time per iteration (ms): 484.7 | learning rate: 1.674E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.284677E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.08 | backward-compute: 194.03 | backward-embedding-all-reduce: 0.01 | optimizer: 13.83 | batch-generator: 9.60
[2022-10-31 11:01:12,090] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[1.7810431999999998e-06, 1.7810431999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:12,092] [INFO] [timer.py:198:stop] 0/170, RunningAvgSamplesPerSec=426.8973152562065, CurrSamplesPerSec=424.4146191840426, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      170/  171661 | consumed samples:        43520 | consumed tokens:     89128960 | elapsed time per iteration (ms): 485.4 | learning rate: 1.781E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.230325E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 257.95 | backward-compute: 194.23 | backward-embedding-all-reduce: 0.01 | optimizer: 13.90 | batch-generator: 9.84
[2022-10-31 11:01:16,957] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[1.8885631999999997e-06, 1.8885631999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:16,960] [INFO] [timer.py:198:stop] 0/180, RunningAvgSamplesPerSec=426.9027800498647, CurrSamplesPerSec=427.2476977192061, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      180/  171661 | consumed samples:        46080 | consumed tokens:     94371840 | elapsed time per iteration (ms): 486.5 | learning rate: 1.889E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.298767E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.24 | backward-compute: 194.16 | backward-embedding-all-reduce: 0.01 | optimizer: 13.87 | batch-generator: 9.78
[2022-10-31 11:01:21,788] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[1.9960832e-06, 1.9960832e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:21,790] [INFO] [timer.py:198:stop] 0/190, RunningAvgSamplesPerSec=426.95400953825884, CurrSamplesPerSec=427.73787147250505, MemAllocated=1.64GB, MaxMemAllocated=7.04GB
 iteration      190/  171661 | consumed samples:        48640 | consumed tokens:     99614720 | elapsed time per iteration (ms): 483.0 | learning rate: 1.996E-06 | global batch size:   256 |reserve_length:   272 | lm loss: 9.108112E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 256.50 | backward-compute: 194.10 | backward-embedding-all-reduce: 0.01 | optimizer: 13.91 | batch-generator: 9.76
[2022-10-31 11:01:26,672] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[2.106606933333333e-06, 2.106606933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:26,675] [INFO] [timer.py:198:stop] 0/200, RunningAvgSamplesPerSec=426.8351545854803, CurrSamplesPerSec=424.90638098494344, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      200/  171661 | consumed samples:        51200 | consumed tokens:    104857600 | elapsed time per iteration (ms): 488.4 | learning rate: 2.107E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 9.106137E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.87 | backward-compute: 194.71 | backward-embedding-all-reduce: 0.01 | optimizer: 13.84 | batch-generator: 9.57
-----------------------------------------------------------------------------------------------
 validation loss at iteration 200 | lm loss value: 9.076966E+00 | lm loss PPL: 8.751377E+03 | 
-----------------------------------------------------------------------------------------------
[2022-10-31 11:01:34,735] [INFO] [logging.py:68:log_dist] [Rank 0] step=210, skipped=0, lr=[2.2178815999999997e-06, 2.2178815999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:34,738] [INFO] [timer.py:198:stop] 0/210, RunningAvgSamplesPerSec=426.70594144878913, CurrSamplesPerSec=424.64960910186767, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      210/  171661 | consumed samples:        53760 | consumed tokens:    110100480 | elapsed time per iteration (ms): 806.4 | learning rate: 2.218E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 9.045603E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 577.61 | backward-compute: 195.09 | backward-embedding-all-reduce: 0.01 | optimizer: 13.88 | batch-generator: 17.13
[2022-10-31 11:01:39,675] [INFO] [logging.py:68:log_dist] [Rank 0] step=220, skipped=0, lr=[2.3291562666666666e-06, 2.3291562666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:39,677] [INFO] [timer.py:198:stop] 0/220, RunningAvgSamplesPerSec=426.2272646021089, CurrSamplesPerSec=414.59774503444226, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      220/  171661 | consumed samples:        56320 | consumed tokens:    115343360 | elapsed time per iteration (ms): 493.9 | learning rate: 2.329E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 9.090004E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 265.83 | backward-compute: 195.16 | backward-embedding-all-reduce: 0.01 | optimizer: 13.84 | batch-generator: 9.69
[2022-10-31 11:01:44,562] [INFO] [logging.py:68:log_dist] [Rank 0] step=230, skipped=0, lr=[2.440430933333333e-06, 2.440430933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:44,565] [INFO] [timer.py:198:stop] 0/230, RunningAvgSamplesPerSec=426.04603180368787, CurrSamplesPerSec=424.4911301927669, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      230/  171661 | consumed samples:        58880 | consumed tokens:    120586240 | elapsed time per iteration (ms): 488.8 | learning rate: 2.440E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.952828E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.80 | backward-compute: 195.45 | backward-embedding-all-reduce: 0.01 | optimizer: 13.84 | batch-generator: 9.89
[2022-10-31 11:01:49,463] [INFO] [logging.py:68:log_dist] [Rank 0] step=240, skipped=0, lr=[2.5517056e-06, 2.5517056e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:49,465] [INFO] [timer.py:198:stop] 0/240, RunningAvgSamplesPerSec=425.8410012631195, CurrSamplesPerSec=422.04443773622876, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      240/  171661 | consumed samples:        61440 | consumed tokens:    125829120 | elapsed time per iteration (ms): 490.2 | learning rate: 2.552E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.933978E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 262.38 | backward-compute: 194.99 | backward-embedding-all-reduce: 0.01 | optimizer: 13.88 | batch-generator: 9.90
[2022-10-31 11:01:54,347] [INFO] [logging.py:68:log_dist] [Rank 0] step=250, skipped=0, lr=[2.6629802666666665e-06, 2.6629802666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:54,349] [INFO] [timer.py:198:stop] 0/250, RunningAvgSamplesPerSec=425.6729778964878, CurrSamplesPerSec=423.71522016390753, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      250/  171661 | consumed samples:        64000 | consumed tokens:    131072000 | elapsed time per iteration (ms): 488.3 | learning rate: 2.663E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.940360E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.74 | backward-compute: 195.13 | backward-embedding-all-reduce: 0.01 | optimizer: 13.85 | batch-generator: 9.81
[2022-10-31 11:01:59,259] [INFO] [logging.py:68:log_dist] [Rank 0] step=260, skipped=0, lr=[2.774254933333333e-06, 2.774254933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:01:59,261] [INFO] [timer.py:198:stop] 0/260, RunningAvgSamplesPerSec=425.3868254376409, CurrSamplesPerSec=416.17383970431376, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      260/  171661 | consumed samples:        66560 | consumed tokens:    136314880 | elapsed time per iteration (ms): 491.1 | learning rate: 2.774E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.893758E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 263.62 | backward-compute: 195.07 | backward-embedding-all-reduce: 0.01 | optimizer: 13.93 | batch-generator: 9.55
[2022-10-31 11:02:04,186] [INFO] [logging.py:68:log_dist] [Rank 0] step=270, skipped=0, lr=[2.8855295999999995e-06, 2.8855295999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:02:04,189] [INFO] [timer.py:198:stop] 0/270, RunningAvgSamplesPerSec=425.1526744046535, CurrSamplesPerSec=419.0951239008793, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      270/  171661 | consumed samples:        69120 | consumed tokens:    141557760 | elapsed time per iteration (ms): 492.8 | learning rate: 2.886E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.830926E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 264.88 | backward-compute: 195.00 | backward-embedding-all-reduce: 0.01 | optimizer: 13.86 | batch-generator: 9.65
[2022-10-31 11:02:09,083] [INFO] [logging.py:68:log_dist] [Rank 0] step=280, skipped=0, lr=[2.9968042666666665e-06, 2.9968042666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:02:09,085] [INFO] [timer.py:198:stop] 0/280, RunningAvgSamplesPerSec=425.05748548816405, CurrSamplesPerSec=424.8472018232464, MemAllocated=1.64GB, MaxMemAllocated=7.08GB
 iteration      280/  171661 | consumed samples:        71680 | consumed tokens:    146800640 | elapsed time per iteration (ms): 489.7 | learning rate: 2.997E-06 | global batch size:   256 |reserve_length:   288 | lm loss: 8.764866E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 261.57 | backward-compute: 195.28 | backward-embedding-all-reduce: 0.01 | optimizer: 13.85 | batch-generator: 9.80
[2022-10-31 11:02:13,962] [INFO] [logging.py:68:log_dist] [Rank 0] step=290, skipped=0, lr=[3.1088298666666665e-06, 3.1088298666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:02:13,964] [INFO] [timer.py:198:stop] 0/290, RunningAvgSamplesPerSec=425.0178583517448, CurrSamplesPerSec=424.4213295724409, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      290/  171661 | consumed samples:        74240 | consumed tokens:    152043520 | elapsed time per iteration (ms): 487.9 | learning rate: 3.109E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.761116E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.07 | backward-compute: 195.33 | backward-embedding-all-reduce: 0.01 | optimizer: 13.85 | batch-generator: 9.59
[2022-10-31 11:02:18,839] [INFO] [logging.py:68:log_dist] [Rank 0] step=300, skipped=0, lr=[3.2238592e-06, 3.2238592e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:02:18,842] [INFO] [timer.py:198:stop] 0/300, RunningAvgSamplesPerSec=424.95676305947154, CurrSamplesPerSec=424.9615400446436, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      300/  171661 | consumed samples:        76800 | consumed tokens:    157286400 | elapsed time per iteration (ms): 487.8 | learning rate: 3.224E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.732607E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.76 | backward-compute: 196.36 | backward-embedding-all-reduce: 0.01 | optimizer: 13.88 | batch-generator: 9.44
-----------------------------------------------------------------------------------------------
 validation loss at iteration 300 | lm loss value: 8.673430E+00 | lm loss PPL: 5.845518E+03 | 
-----------------------------------------------------------------------------------------------
[2022-10-31 11:02:26,921] [INFO] [logging.py:68:log_dist] [Rank 0] step=310, skipped=0, lr=[3.338888533333333e-06, 3.338888533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:02:26,923] [INFO] [timer.py:198:stop] 0/310, RunningAvgSamplesPerSec=424.7999035371962, CurrSamplesPerSec=423.2275446269499, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      310/  171661 | consumed samples:        79360 | consumed tokens:    162529280 | elapsed time per iteration (ms): 808.1 | learning rate: 3.339E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.726015E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 577.97 | backward-compute: 196.58 | backward-embedding-all-reduce: 0.01 | optimizer: 13.96 | batch-generator: 16.92
[2022-10-31 11:02:31,807] [INFO] [logging.py:68:log_dist] [Rank 0] step=320, skipped=0, lr=[3.453917866666666e-06, 3.453917866666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:02:31,809] [INFO] [timer.py:198:stop] 0/320, RunningAvgSamplesPerSec=424.7771276450007, CurrSamplesPerSec=424.9776868689107, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      320/  171661 | consumed samples:        81920 | consumed tokens:    167772160 | elapsed time per iteration (ms): 488.6 | learning rate: 3.454E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.620916E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.35 | backward-compute: 196.37 | backward-embedding-all-reduce: 0.01 | optimizer: 13.97 | batch-generator: 9.92
[2022-10-31 11:02:36,687] [INFO] [logging.py:68:log_dist] [Rank 0] step=330, skipped=0, lr=[3.5689471999999995e-06, 3.5689471999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:02:36,690] [INFO] [timer.py:198:stop] 0/330, RunningAvgSamplesPerSec=424.73144582024656, CurrSamplesPerSec=424.9346313973184, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      330/  171661 | consumed samples:        84480 | consumed tokens:    173015040 | elapsed time per iteration (ms): 488.0 | learning rate: 3.569E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.631646E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 259.14 | backward-compute: 196.50 | backward-embedding-all-reduce: 0.01 | optimizer: 13.96 | batch-generator: 9.68
[2022-10-31 11:02:41,566] [INFO] [logging.py:68:log_dist] [Rank 0] step=340, skipped=0, lr=[3.6839765333333326e-06, 3.6839765333333326e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:02:41,569] [INFO] [timer.py:198:stop] 0/340, RunningAvgSamplesPerSec=424.72117873985036, CurrSamplesPerSec=426.0190508838252, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      340/  171661 | consumed samples:        87040 | consumed tokens:    178257920 | elapsed time per iteration (ms): 487.9 | learning rate: 3.684E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.513618E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 258.69 | backward-compute: 196.41 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 9.64
[2022-10-31 11:02:46,461] [INFO] [logging.py:68:log_dist] [Rank 0] step=350, skipped=0, lr=[3.7990058666666665e-06, 3.7990058666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:02:46,464] [INFO] [timer.py:198:stop] 0/350, RunningAvgSamplesPerSec=424.6818681668336, CurrSamplesPerSec=423.78746487322786, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      350/  171661 | consumed samples:        89600 | consumed tokens:    183500800 | elapsed time per iteration (ms): 489.5 | learning rate: 3.799E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.531570E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.07 | backward-compute: 196.47 | backward-embedding-all-reduce: 0.01 | optimizer: 14.01 | batch-generator: 9.56
[2022-10-31 11:02:51,356] [INFO] [logging.py:68:log_dist] [Rank 0] step=360, skipped=0, lr=[3.9140352e-06, 3.9140352e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:02:51,358] [INFO] [timer.py:198:stop] 0/360, RunningAvgSamplesPerSec=424.6424411581904, CurrSamplesPerSec=424.35021025008695, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      360/  171661 | consumed samples:        92160 | consumed tokens:    188743680 | elapsed time per iteration (ms): 489.5 | learning rate: 3.914E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.525821E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.91 | backward-compute: 196.35 | backward-embedding-all-reduce: 0.01 | optimizer: 13.93 | batch-generator: 9.69
[2022-10-31 11:02:56,249] [INFO] [logging.py:68:log_dist] [Rank 0] step=370, skipped=0, lr=[4.029064533333333e-06, 4.029064533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:02:56,251] [INFO] [timer.py:198:stop] 0/370, RunningAvgSamplesPerSec=424.56849723578597, CurrSamplesPerSec=424.5032134001316, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      370/  171661 | consumed samples:        94720 | consumed tokens:    193986560 | elapsed time per iteration (ms): 489.3 | learning rate: 4.029E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.477269E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.49 | backward-compute: 196.52 | backward-embedding-all-reduce: 0.01 | optimizer: 13.95 | batch-generator: 9.44
[2022-10-31 11:03:01,148] [INFO] [logging.py:68:log_dist] [Rank 0] step=380, skipped=0, lr=[4.144093866666667e-06, 4.144093866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:03:01,151] [INFO] [timer.py:198:stop] 0/380, RunningAvgSamplesPerSec=424.4647285407974, CurrSamplesPerSec=423.5347161082869, MemAllocated=1.64GB, MaxMemAllocated=7.12GB
 iteration      380/  171661 | consumed samples:        97280 | consumed tokens:    199229440 | elapsed time per iteration (ms): 490.0 | learning rate: 4.144E-06 | global batch size:   256 |reserve_length:   304 | lm loss: 8.413667E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.59 | backward-compute: 196.71 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 9.43
[2022-10-31 11:03:06,072] [INFO] [logging.py:68:log_dist] [Rank 0] step=390, skipped=0, lr=[4.261751466666666e-06, 4.261751466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:03:06,075] [INFO] [timer.py:198:stop] 0/390, RunningAvgSamplesPerSec=424.3108254380558, CurrSamplesPerSec=416.50833372123697, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      390/  171661 | consumed samples:        99840 | consumed tokens:    204472320 | elapsed time per iteration (ms): 492.3 | learning rate: 4.262E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.362445E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 262.24 | backward-compute: 197.38 | backward-embedding-all-reduce: 0.01 | optimizer: 13.94 | batch-generator: 9.68
[2022-10-31 11:03:10,994] [INFO] [logging.py:68:log_dist] [Rank 0] step=400, skipped=0, lr=[4.380535466666666e-06, 4.380535466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:03:10,997] [INFO] [timer.py:198:stop] 0/400, RunningAvgSamplesPerSec=424.12996380513067, CurrSamplesPerSec=417.853004906478, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      400/  171661 | consumed samples:       102400 | consumed tokens:    209715200 | elapsed time per iteration (ms): 492.2 | learning rate: 4.381E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.318017E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 262.39 | backward-compute: 197.72 | backward-embedding-all-reduce: 0.01 | optimizer: 13.93 | batch-generator: 9.52
-----------------------------------------------------------------------------------------------
 validation loss at iteration 400 | lm loss value: 8.323271E+00 | lm loss PPL: 4.118609E+03 | 
-----------------------------------------------------------------------------------------------
[2022-10-31 11:03:19,089] [INFO] [logging.py:68:log_dist] [Rank 0] step=410, skipped=0, lr=[4.499319466666666e-06, 4.499319466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:03:19,091] [INFO] [timer.py:198:stop] 0/410, RunningAvgSamplesPerSec=424.07232568946847, CurrSamplesPerSec=422.8395438220654, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      410/  171661 | consumed samples:       104960 | consumed tokens:    214958080 | elapsed time per iteration (ms): 809.4 | learning rate: 4.499E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.280815E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 579.00 | backward-compute: 197.90 | backward-embedding-all-reduce: 0.01 | optimizer: 13.98 | batch-generator: 16.46
[2022-10-31 11:03:23,995] [INFO] [logging.py:68:log_dist] [Rank 0] step=420, skipped=0, lr=[4.618103466666667e-06, 4.618103466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:03:23,998] [INFO] [timer.py:198:stop] 0/420, RunningAvgSamplesPerSec=423.9941153076811, CurrSamplesPerSec=422.1599277830724, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      420/  171661 | consumed samples:       107520 | consumed tokens:    220200960 | elapsed time per iteration (ms): 490.7 | learning rate: 4.618E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.264466E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 261.23 | backward-compute: 197.67 | backward-embedding-all-reduce: 0.01 | optimizer: 13.92 | batch-generator: 9.05
[2022-10-31 11:03:28,902] [INFO] [logging.py:68:log_dist] [Rank 0] step=430, skipped=0, lr=[4.736887466666666e-06, 4.736887466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:03:28,904] [INFO] [timer.py:198:stop] 0/430, RunningAvgSamplesPerSec=423.92914374975953, CurrSamplesPerSec=424.4454887277488, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      430/  171661 | consumed samples:       110080 | consumed tokens:    225443840 | elapsed time per iteration (ms): 490.7 | learning rate: 4.737E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.212595E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 261.19 | backward-compute: 197.54 | backward-embedding-all-reduce: 0.01 | optimizer: 13.93 | batch-generator: 9.43
[2022-10-31 11:03:33,807] [INFO] [logging.py:68:log_dist] [Rank 0] step=440, skipped=0, lr=[4.855671466666666e-06, 4.855671466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:03:33,809] [INFO] [timer.py:198:stop] 0/440, RunningAvgSamplesPerSec=423.8969571133693, CurrSamplesPerSec=423.22087193426125, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      440/  171661 | consumed samples:       112640 | consumed tokens:    230686720 | elapsed time per iteration (ms): 490.5 | learning rate: 4.856E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.200500E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.22 | backward-compute: 197.55 | backward-embedding-all-reduce: 0.01 | optimizer: 13.92 | batch-generator: 9.49
[2022-10-31 11:03:38,703] [INFO] [logging.py:68:log_dist] [Rank 0] step=450, skipped=0, lr=[4.974455466666666e-06, 4.974455466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:03:38,706] [INFO] [timer.py:198:stop] 0/450, RunningAvgSamplesPerSec=423.8790424837186, CurrSamplesPerSec=422.45624283933677, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      450/  171661 | consumed samples:       115200 | consumed tokens:    235929600 | elapsed time per iteration (ms): 489.7 | learning rate: 4.974E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.067671E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 260.41 | backward-compute: 197.44 | backward-embedding-all-reduce: 0.01 | optimizer: 13.89 | batch-generator: 8.93
[2022-10-31 11:03:43,617] [INFO] [logging.py:68:log_dist] [Rank 0] step=460, skipped=0, lr=[5.093239466666666e-06, 5.093239466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:03:43,619] [INFO] [timer.py:198:stop] 0/460, RunningAvgSamplesPerSec=423.83440850870767, CurrSamplesPerSec=423.9561317059611, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      460/  171661 | consumed samples:       117760 | consumed tokens:    241172480 | elapsed time per iteration (ms): 491.3 | learning rate: 5.093E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.110712E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 262.06 | backward-compute: 197.49 | backward-embedding-all-reduce: 0.01 | optimizer: 13.92 | batch-generator: 8.99
[2022-10-31 11:03:48,520] [INFO] [logging.py:68:log_dist] [Rank 0] step=470, skipped=0, lr=[5.212023466666666e-06, 5.212023466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2022-10-31 11:03:48,522] [INFO] [timer.py:198:stop] 0/470, RunningAvgSamplesPerSec=423.7718829517655, CurrSamplesPerSec=421.77785738752243, MemAllocated=1.64GB, MaxMemAllocated=7.15GB
 iteration      470/  171661 | consumed samples:       120320 | consumed tokens:    246415360 | elapsed time per iteration (ms): 490.4 | learning rate: 5.212E-06 | global batch size:   256 |reserve_length:   320 | lm loss: 8.101424E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 261.21 | backward-compute: 197.59 | backward-embedding-all-reduce: 0.01 | optimizer: 13.99 | batch-generator: 8.98
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
[2022-10-31 11:03:50,563] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 22474
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 276, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain
        pretrain(train_valid_test_datasets_provider, model_provider, forward_step,pretrain(train_valid_test_datasets_provider, model_provider, forward_step,    

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain
pretrain(train_valid_test_datasets_provider, model_provider, forward_step,      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain
pretrain(train_valid_test_datasets_provider, model_provider, forward_step,    
pretrain(train_valid_test_datasets_provider, model_provider, forward_step,      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain

pretrain(train_valid_test_datasets_provider, model_provider, forward_step,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain
    
iteration = train(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 171, in pretrain
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 899, in train
    iteration = train(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 899, in train
    iteration = train(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 899, in train
    iteration = train(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 899, in train
    iteration = train(forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 899, in train
iteration = train(forward_step_func,    
iteration = train(forward_step_func,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 899, in train

      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 899, in train
train_step(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 509, in train_step
    train_step(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 509, in train_step
    train_step(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 509, in train_step
    train_step(forward_step_func,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 509, in train_step
    losses_reduced = forward_backward_func(
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
train_step(forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 509, in train_step
train_step(forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 509, in train_step
train_step(forward_step_func,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/training.py", line 509, in train_step
losses_reduced = forward_backward_func(    
losses_reduced = forward_backward_func(  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
    losses_reduced = forward_backward_func(
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
    losses_reduced = forward_backward_func(
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
    losses_reduced = forward_backward_func(
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
    losses_reduced = forward_backward_func(
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 147, in forward_backward_no_pipelining
    output_tensor = forward_step(forward_step_func, data_iterator, model,    
output_tensor = forward_step(forward_step_func, data_iterator, model,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
    
output_tensor = forward_step(forward_step_func, data_iterator, model,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
    
output_tensor = forward_step(forward_step_func, data_iterator, model,  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step

      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
output_tensor = forward_step(forward_step_func, data_iterator, model,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
output_tensor = forward_step(forward_step_func, data_iterator, model,
          File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)    output_tensor = forward_step(forward_step_func, data_iterator, model,
output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step

      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/schedules.py", line 61, in forward_step
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)    
output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
        output_tensor, *other_losses = model(tokens, position_ids, attention_mask,output_tensor, loss_func = forward_step_func(data_iterator, model, teacher_model)

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/examples/random_ltd/../../pretrain_gpt.py", line 208, in forward_step
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    output_tensor, *other_losses = model(tokens, position_ids, attention_mask,    
output_tensor, *other_losses = model(tokens, position_ids, attention_mask,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
output_tensor, *other_losses = model(tokens, position_ids, attention_mask,output_tensor, *other_losses = model(tokens, position_ids, attention_mask,    

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
output_tensor, *other_losses = model(tokens, position_ids, attention_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    output_tensor, *other_losses = model(tokens, position_ids, attention_mask,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return forward_call(*input, **kwargs)
    return func(*args, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward
    return func(*args, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward
return func(*args, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward
return func(*args, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
return func(*args, **kwargs)    
return func(*args, **kwargs)  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward

  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward
    return func(*args, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/engine.py", line 1740, in forward
loss = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    loss = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    loss = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    loss = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    loss = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    loss = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
loss = self.module(*inputs, **kwargs)
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
    lm_output, *moe_losses = self.language_model(
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/gpt_model.py", line 120, in forward
lm_output, *moe_losses = self.language_model(    
lm_output, *moe_losses = self.language_model(  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    lm_output, *moe_losses = self.language_model(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/language_model.py", line 390, in forward
    encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    encoder_output, *moe_losses = self.encoder(encoder_input,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    encoder_output, *moe_losses = self.encoder(encoder_input,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    encoder_output, *moe_losses = self.encoder(encoder_input,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
encoder_output, *moe_losses = self.encoder(encoder_input,    
encoder_output, *moe_losses = self.encoder(encoder_input,  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 847, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 847, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 815, in forward
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 811, in forward
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 815, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 815, in forward
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 815, in forward
hidden_states, _ = layer(hidden_states,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    hidden_states, _ = layer(hidden_states,
      File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
sampled_indices, part_attention_mask = gpt_sample_tokens(attention_mask,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/ops/token_dropping/dropping_utils.py", line 16, in gpt_sample_tokens
    return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/dynamic_train/basic_layer.py", line 80, in forward
    if self.training and index not in [0, 11] and self.reserved_length < 2048:    
sampled_indices, part_attention_mask = gpt_sample_tokens(attention_mask,
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/ops/token_dropping/dropping_utils.py", line 23, in gpt_sample_tokens
    KeyboardInterruptsampled_indices, part_attention_mask = gpt_sample_tokens(attention_mask,

      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/ops/token_dropping/dropping_utils.py", line 23, in gpt_sample_tokens
sampled_indices, part_attention_mask = gpt_sample_tokens(attention_mask,
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/ops/token_dropping/dropping_utils.py", line 23, in gpt_sample_tokens
return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/runtime/dynamic_train/basic_layer.py", line 80, in forward
def gpt_sample_tokens(attn_mask: torch.Tensor,
    KeyboardInterruptsampled_indices = torch.multinomial(prob_dist, reserved_length)

KeyboardInterrupt
    sampled_indices = torch.multinomial(prob_dist, reserved_length)
    sampled_indices = torch.multinomial(prob_dist, reserved_length)
KeyboardInterrupt
KeyboardInterrupt
    return self.randomltd_layer(hidden_states, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return self.randomltd_layer(hidden_states, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
      File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 545, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/Megatron-DeepSpeed-internal-dev/megatron/model/transformer.py", line 546, in forward
    moe_loss = torch.tensor(0.0, device=layernorm_output.device, dtype=layernorm_output.dtype)    
mlp_bias = torch.tensor(0.0, device=layernorm_output.device, dtype=layernorm_output.dtype)
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/bin/deepspeed", line 6, in <module>
[2022-10-31 11:03:50,663] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 22474
    main()
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/launcher/runner.py", line 519, in main
    result.wait()
  File "/opt/conda/lib/python3.8/subprocess.py", line 1083, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.8/subprocess.py", line 1808, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.8/subprocess.py", line 1766, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/home/xiaoxiawu/TokenDropping/deepspeed-token-drop/DeepSpeed-internal-connor/deepspeed/launcher/runner.py", line 511, in sigkill_handler
    result_kill = subprocess.Popen(kill_cmd, env=env)
NameError: free variable 'kill_cmd' referenced before assignment in enclosing scope
[2022-10-31 11:03:51,364] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 22475
[2022-10-31 11:03:52,388] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 22476
[2022-10-31 11:03:52,801] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 22478
[2022-10-31 11:03:53,576] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 22480
[2022-10-31 11:03:53,577] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 22482
[2022-10-31 11:03:53,954] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 22484
[2022-10-31 11:03:53,955] [INFO] [launch.py:286:sigkill_handler] Killing subprocess 22486
[2022-10-31 11:03:54,132] [INFO] [launch.py:295:sigkill_handler] Main process received SIGTERM, exiting
